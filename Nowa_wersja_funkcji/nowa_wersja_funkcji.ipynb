{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f037dd59-8d78-4bbc-b1f8-f878afe1f342",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Bio import AlignIO, Align\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daa9c022-9b97-48b7-b7f7-9d01e024710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#description of goal\n",
    "    #main goal that function is select introns and exon from alignment and descripte their class, start - ends nucleotide.\n",
    "#description step by step\n",
    "    # 1. iterate through files in input before mafft and linking with input after mafft, by the same name\n",
    "    # 2. checking if both files have .fasta extension\n",
    "    # 3. making alignment in file after mafft\n",
    "    # 4. extracting_strands_from_alignment function is used to determine which strand is genomic strand or transcript\n",
    "    # 5. Removing gaps from 5' and 3'. Then pre-exon (candidat to be exon) will be first\n",
    "    # 6. Creating list with nucleotides's positions of intron or exon\n",
    "    # 7. Creating dictionary like: start_exon:end_exon. If distance between two following indices is bigger than two (acceptable gap length) then there is intron between them.\n",
    "    # 8. Making alignment and determinig class of exon according to their homology.\n",
    "    # 9. Forming a table with exons\n",
    "    # 10. Spliting a data frame to 3 others, with different exon's class\n",
    "    # 11. Adding introns to tables\n",
    "    # 12. Saving table in tsv gff format.\n",
    "\n",
    "#ASSUMPTIONS\n",
    "#- both files, path_to_file_before_MAFFT and path_to_file_after_MAFFT have to have the same name\n",
    "#- they have to be .fasta\n",
    "#- acceptable_gap_length is int.\n",
    "\n",
    "min_length_aligned_sequence = 30 #Minimal lenght of sequence which could be anÂ exon\n",
    "# extreme_homology = 0.97 #percentage of homology of sequence, treshold #I assume two faulty aligned nucleotides per 100 (98%) and one more nt because sometimes latest nt can move from end of one sequence to beginning next sequence\n",
    "\n",
    "path_to_file_before_MAFFT = \"/home/norbert/mrdn/euglena/kod_i_pliki/surowe_pliki_plus_minus_500/raw_reads_9/merging_fastas\"\n",
    "path_to_file_after_MAFFT = \"/home/norbert/mrdn/euglena/kod_i_pliki/surowe_pliki_plus_minus_500/raw_reads_9/fastas_after_mafft_na_probe\"\n",
    "\n",
    "# path_to_file_before_MAFFT = '/home/norbert/mrdn/euglena/kod_i_pliki/pliki_ktore_nie_przeszly_divided_fastas'\n",
    "# path_to_file_after_MAFFT = '/home/norbert/mrdn/euglena/kod_i_pliki/pliki_ktore_nie_przeszly_fastas_after_mafft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f91688eb-6822-4df4-b7fc-59c484ad9f0c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_to_file_before_mafft' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 375\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m files_not_in_df\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 375\u001b[0m     cutting_scrap(\u001b[43mpath_to_file_before_mafft\u001b[49m, path_to_file_after_mafft, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0.97\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path_to_file_before_mafft' is not defined"
     ]
    }
   ],
   "source": [
    "def cutting_scrap(path_to_file_before_mafft, path_to_file_after_mafft, acceptable_gap_length, extreme_homology):\n",
    "    gff_data_frames = []  # pre-list for tables\n",
    "    invalid_nucleotides = []\n",
    "    gaps_signs = \"-\" * acceptable_gap_length  # maximum length of gaps in sequence in one exon's sequence\n",
    "    files_in_progress = 0\n",
    "    broken_files = []  # list of files which caused error\n",
    "    column_names = ['seqid', 'source', 'exon_type', 'start', 'end', 'length', 'homology', 'strand', 'phase',\n",
    "                    'attributes']\n",
    "    print(f\"\\n \\n \\n \\n Start running function: cutting_scrap\")\n",
    "\n",
    "    total_files = len([f for f in os.listdir(path_to_file_after_mafft) if f.endswith(\".fasta\")])\n",
    "\n",
    "    # linking two files from input and making alignment\n",
    "    for filename in os.listdir(path_to_file_after_mafft):\n",
    "        file = os.path.join(path_to_file_after_mafft, filename)\n",
    "        if not os.path.isfile(file):\n",
    "            continue\n",
    "        if file.endswith(\".fasta\"):\n",
    "            try:\n",
    "                files_in_progress += 1\n",
    "                count_files = percentage_of_advancement(path_to_file_before_mafft)\n",
    "                print(f\"\\n ################################################################################### \\n \"\n",
    "                      f\"running file: {filename} which is {files_in_progress} of {count_files}, what means \"\n",
    "                      f\"{round(files_in_progress / count_files * 100, 2)}% of advancement \\n \"\n",
    "                      f\"################################################################################### \\n\")\n",
    "                alignment = AlignIO.read(file, \"fasta\")\n",
    "                alignment_id = alignment[0].id\n",
    "                alignment_didnt_touched = AlignIO.read(file, \"fasta\")\n",
    "\n",
    "                file_before_mafft = os.path.join(path_to_file_before_mafft, filename)\n",
    "                if not os.path.isfile(file_before_mafft):\n",
    "                    continue\n",
    "\n",
    "                # extracting_strands_from_alignment function is used to determine which strand is genomic strand\n",
    "                # or transcript\n",
    "                seq1_dt, seq2_dt = extracting_strands_from_alignment(alignment_didnt_touched)\n",
    "                seq1, seq2 = extracting_strands_from_alignment(alignment)\n",
    "\n",
    "                # Removing gaps from 5' and 3'. Then pre-exon (candidat to be exon) will be first\n",
    "                seq1, seq2, count_deleted_gaps_left = cleaning_gaps_from_both_edges(seq1_dt, seq2_dt, alignment,\n",
    "                                                                                    filename)  # deleting gaps from 5'\n",
    "                # and 3'. It is important. The next function start counting from possible exon.\n",
    "\n",
    "                # Creating list with nucleotides's positions of intron or exon\n",
    "                temp_intron, temp_exon, invalid_nucleotides = indices_of_introns_and_exons_fun(seq1_dt, seq2_dt, seq1,\n",
    "                                                                                               seq2,\n",
    "                                                                                               count_deleted_gaps_left,\n",
    "                                                                                               alignment_id,\n",
    "                                                                                               invalid_nucleotides)\n",
    "\n",
    "                # Creating dictionary like: start_exon:end_exon. If distance between two following indices is bigger\n",
    "                # than two (acceptable gap length) then there is intron between them.\n",
    "                all_exon_range_dict = start_and_end_parameters_of_exons_dict_fun(temp_exon, gaps_signs)\n",
    "\n",
    "                # Making alignment and determinig class of exon according to their homology.\n",
    "                # Forming a table with exons\n",
    "                exon_rows_to_concete_table, one_alignment_df = making_exons_gff_table_fun(all_exon_range_dict,\n",
    "                                                                                          min_length_aligned_sequence,\n",
    "                                                                                          extreme_homology, seq1_dt,\n",
    "                                                                                          seq2_dt, alignment_id,\n",
    "                                                                                          column_names, filename[:-6])\n",
    "\n",
    "                if one_alignment_df.empty:\n",
    "                    print(\n",
    "                        f'File: {filename} is out of exons and introns. Processing failed. Broken_file list appended.')\n",
    "                    broken_files.append(filename)\n",
    "                else:\n",
    "                    gff_data_frames.append(one_alignment_df)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "                broken_files.append(filename)\n",
    "\n",
    "    # spliting a data frame to 3 others, with different exon's class\n",
    "    if not len(gff_data_frames) == 0:\n",
    "        all_exons_df = pd.concat(gff_data_frames, ignore_index=True)\n",
    "        tlh_exons_df = all_exons_df[all_exons_df['exon_type'] == 'tlh'].copy()\n",
    "        fine_exons_df = all_exons_df[all_exons_df['exon_type'] == 'fine'].copy()\n",
    "\n",
    "        all_exons_df, tlh_exons_df, fine_exons_df = adding_introns_to_gff_data_frame(all_exons_df, tlh_exons_df,\n",
    "                                                                      fine_exons_df, column_names)\n",
    "        files_in_df, unique_seqid_count = getting_files_from_df(fine_exons_df, 'attributes')\n",
    "\n",
    "        print(f'{((total_files - len(broken_files)) / total_files) * 100}% of files end up succesfully. \\n'\n",
    "              f' {len(broken_files)} had error. List of files with errors in broken_files file. \\n '\n",
    "              f'Invalid nucleotides (different than A T G C) are written with positions in invalid_nucleotides file. \\n'\n",
    "              f'Found {unique_seqid_count} unique files in data frame from {count_files} from input. It means {count_files - unique_seqid_count} files wasn\\'t '\n",
    "              f'written down to output. List of that files is in not_written_alignments file')\n",
    "        fine_exons_count = fine_exons_df['exon_type'].str.contains('fine', case=False).sum()\n",
    "\n",
    "    else:\n",
    "        print(\"No valid data frames to concatenate.\")\n",
    "        all_exons_df = pd.DataFrame(columns=column_names)\n",
    "        tlh_exons_df = pd.DataFrame(columns=column_names)\n",
    "        fine_exons_df = pd.DataFrame(columns=column_names)\n",
    "        fine_exons_count = 0\n",
    "\n",
    "        # Adding introns to tables\n",
    "\n",
    "    # Saving table in tsv gff format\n",
    "    save_to_gff_file(all_exons_df, 'all_exons_gff.tsv')\n",
    "    save_to_gff_file(tlh_exons_df, 'tlh_exons_gff.tsv')\n",
    "    save_to_gff_file(fine_exons_df, 'fine_exons_gff.tsv')\n",
    "\n",
    "    if broken_files:\n",
    "        with open('broken_files', 'w') as file:\n",
    "            file.write('Files which made error: \\n')\n",
    "            file.write(str(broken_files))\n",
    "    length_fine_exons_df = len(fine_exons_df)\n",
    "\n",
    "    if invalid_nucleotides:\n",
    "        with open('invalid_nucleotides', 'w') as file:\n",
    "            for alignment_id, nt1, nt2, pos in invalid_nucleotides:\n",
    "                file.write(f'{alignment_id}\\t{nt1}\\t{nt2}\\t{pos}\\n')\n",
    "\n",
    "    pliki_z_inputu = getting_files_in_directory(path_to_file_after_mafft)\n",
    "    print(f'pliki_z_inputu: {pliki_z_inputu}')\n",
    "    files_not_in_df = find_alignments_not_in_df(path_to_file_after_mafft, files_in_df)\n",
    "    print(f'pliki nieobecne w df: {files_not_in_df}')\n",
    "\n",
    "    if files_not_in_df:\n",
    "        with open('not_written_alignments', 'w') as file:\n",
    "            file.write('Files absent in data frame from input. Propably because lack of introns \\n')\n",
    "            file.write(str(files_not_in_df))\n",
    "\n",
    "    return fine_exons_count, length_fine_exons_df\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "#####################################               MINOR FUNCTIONS                #####################################\n",
    "########################################################################################################################\n",
    "\n",
    "def percentage_of_advancement(directory):\n",
    "    count = 0\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".fasta\"):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def cleaning_gaps_from_both_edges(sequence_transcipt, sequence_genome, alignment, filename):\n",
    "    sequence_transcipt_left_shorted = sequence_transcipt.lstrip(\"-\")\n",
    "    count_deleted_gaps_left = len(sequence_transcipt) - len(\n",
    "        sequence_transcipt_left_shorted)  # inform how many gaps were deleted from 5'\n",
    "\n",
    "    sequence_transcipt_right_and_left_shorted = sequence_transcipt_left_shorted.rstrip(\"-\")\n",
    "    count_deleted_gaps_right = len(sequence_transcipt_left_shorted) - len(\n",
    "        sequence_transcipt_right_and_left_shorted)  # inform how many gaps were deleted from 3'\n",
    "\n",
    "    if count_deleted_gaps_right:\n",
    "        sequence_genome_left_and_right_shorted = sequence_genome[count_deleted_gaps_left:-count_deleted_gaps_right]\n",
    "    else:\n",
    "        sequence_genome_left_and_right_shorted = sequence_genome[count_deleted_gaps_left:]\n",
    "\n",
    "    if len(sequence_genome_left_and_right_shorted) != len(sequence_transcipt_right_and_left_shorted):\n",
    "        raise ValueError(\n",
    "            f\" VALUE ERROR: Given sequences: {alignment[0].id} - len: {len(alignment[0])} and  {alignment[1].id} -\"\n",
    "            f\" len {len(alignment[1])} from {filename} must have the same length.\")\n",
    "    return sequence_transcipt_right_and_left_shorted, sequence_genome_left_and_right_shorted, count_deleted_gaps_left\n",
    "\n",
    "\n",
    "def indices_of_introns_and_exons_fun(seq1_dt, seq2_dt, seq1, seq2, count_deleted_gaps_left, alignment_id,\n",
    "                                     invalid_nucleotides):\n",
    "    # linking pre-exon's nucleotides (nt-nt pairs) and pre-intron's nucleotides (gap-nt pairs) to theirs indices \n",
    "    # NOTE: output is nucleotide or gap from seq1 (transcript sequence). Even if it is gap like this: \n",
    "    # seq1: aaatttggg, seq2: aaa---ggg, output will be 'ttt' instead '---'\n",
    "    # it shows that where were gap, seq2 genome or in seq1 transcript\n",
    "    temp_exon = []  # list containing indices of intron's positions in sequence.\n",
    "    temp_intron = []  # list containing indices of intron's positions in sequence.\n",
    "    index = 0\n",
    "    paired_nucleotides = zip(seq1, seq2)\n",
    "    valid_nucleotides = set('ATGCatgc')\n",
    "\n",
    "    if len(seq1_dt) != len(seq2_dt):\n",
    "        print(seq1)\n",
    "        print(seq2)\n",
    "        raise ValueError(\"The two sequences must be of the same length.\")\n",
    "\n",
    "    for nt1, nt2 in paired_nucleotides:\n",
    "        if \"-\" in (nt1, nt2):\n",
    "            temp_intron.append(index + count_deleted_gaps_left)\n",
    "        elif nt1 not in valid_nucleotides or nt2 not in valid_nucleotides:\n",
    "            if alignment_id not in invalid_nucleotides:\n",
    "                invalid_nucleotides.append(\n",
    "                    (alignment_id, nt1, nt2, index + count_deleted_gaps_left))  # Dodanie nukleotydÃ³w i pozycji\n",
    "        else:\n",
    "            temp_exon.append(index + count_deleted_gaps_left)\n",
    "        index += 1  # indeks do wskazywania pozycji w sekwencji\n",
    "    return temp_intron, temp_exon, invalid_nucleotides\n",
    "\n",
    "\n",
    "def start_and_end_parameters_of_exons_dict_fun(temp_exon, gaps_signs):\n",
    "    # linking indices into single strand of possible exons\n",
    "    start_exon = temp_exon[1]\n",
    "    all_exon_range_dict = {}  # that dictionary contains: key(index of start exon) and value(index of end exon)\n",
    "\n",
    "    for i in range(len(temp_exon)):\n",
    "        # print(start_exon)\n",
    "        if (temp_exon[i] - temp_exon[i - 1]) > 1 + len(\n",
    "                gaps_signs):  # if difference between two indices of exons's positions is bigger than given number (2),\n",
    "            # that smaller number is index of 3' nucleotide in exon\n",
    "            end_exon = temp_exon[\n",
    "                i - 1]  # temp_exon[i-1] because it is index in list temp_exon. +1 because python starts counting from 0\n",
    "            all_exon_range_dict[\n",
    "                start_exon] = end_exon + 1  # creating dictionary with all exons, even with theese too short and with\n",
    "            # too low homology\n",
    "            start_exon = temp_exon[i] + 1\n",
    "    all_exon_range_dict[start_exon] = temp_exon[-1]  # last pair\n",
    "\n",
    "    if all_exon_range_dict == {}:\n",
    "        raise ValueError(f\"ValueError - all_exon_range_dict seems to be empty. Its length = {len(all_exon_range_dict)}\")\n",
    "    # print(f\" \\n all_exon_range_dict: {all_exon_range_dict}\")\n",
    "    return all_exon_range_dict\n",
    "\n",
    "\n",
    "def start_and_end_parameters_of_introns_dict_fun(all_exon_range_dict):\n",
    "    # linking indices into single strand of possible introns\n",
    "    intron_range_dict = {}\n",
    "    keys_from_all_exon_range_dict = sorted(all_exon_range_dict.keys())  # list cointaining exons's start positions\n",
    "    # print(f\"keys_from_all_exon_range_dict: {keys_from_all_exon_range_dict}\\n\")\n",
    "\n",
    "    for i in range(len(keys_from_all_exon_range_dict) - 1):\n",
    "        start_intron = all_exon_range_dict[keys_from_all_exon_range_dict[i]] + 1\n",
    "        end_intron = keys_from_all_exon_range_dict[i + 1] - 1\n",
    "        intron_range_dict[start_intron] = end_intron\n",
    "    if intron_range_dict == {}:\n",
    "        raise ValueError(f\"ValueError - intron_range_dict seems to be empty. Its length = {len(intron_range_dict)}\")\n",
    "    # print(f\" \\n intron_range_dict: {intron_range_dict}\")\n",
    "    return intron_range_dict\n",
    "\n",
    "\n",
    "def making_exons_gff_table_fun(all_exon_range_dict, min_length_aligned_sequence, extreme_homology, seq1_dt, seq2_dt,\n",
    "                               alignment_id, column_names, filename):\n",
    "    exon_rows_to_concete_table = []\n",
    "\n",
    "    aligner = Align.PairwiseAligner()\n",
    "    aligner.mismatch_score = 0  # customized setting towards get pure percentage of homology, not alignment with gap\n",
    "    # penalty etc. The object of interest is simply that how many nt in query has equivalend in target seq.\n",
    "    aligner.open_gap_score = 0\n",
    "    aligner.extend_gap_score = 0\n",
    "\n",
    "    for key in all_exon_range_dict:\n",
    "        query = seq1_dt[key - 1: all_exon_range_dict[key]]\n",
    "        target = seq2_dt[key - 1: all_exon_range_dict[key]]\n",
    "        target_length = len(target)\n",
    "\n",
    "        if target_length > min_length_aligned_sequence:\n",
    "            score = aligner.score(query, target)\n",
    "            identity_level = (round(((score * 100) / len(target)), 2))\n",
    "            # print(f\"query: {query}, \\n target: {target}, percent of homology = {round(score / len(target), 2)\n",
    "            # * 100} %, score = {score}, len = {len(target)}\\n\")\n",
    "            if identity_level >= extreme_homology * 100:\n",
    "                exon_class = 'fine'  # print(f\"\\n query: {query} \\n target: {target} \\n with start: {key} and end: {all_exon_range_dict[key]}  # goes to fine_exons with homology: {identity_level} \\n\")  # if len(target) != len(query):  # print(f'target: {len(target)} query: {len(query)}')\n",
    "            else:\n",
    "                exon_class = 'tlh'  # print(f\"\\n query: {query} \\n target: {target} \\n with start: {key} and end: {all_exon_range_dict[key]}  # goes to tlh_exons with homology: {identity_level}\")  # if len(target) != len(query):  # print(f'target: {len(target)} query: {len(query)}')\n",
    "        else:\n",
    "            identity_level = 0\n",
    "            exon_class = \"TS\"  # print(f\"\\n query: {query} \\n target: {target} \\n with start: {key} and end: {all_exon_range_dict[key]}  # goes to TS_exons with homology: {identity_level}\")  # if len(target) != len(query):  # print(f'target: {len(target)} query: {len(query)}')\n",
    "        source = extending_source_data_frame(alignment_id)\n",
    "        strand = extending_strand_data_frame(alignment_id)\n",
    "        exon_rows_to_concete_table.append((alignment_id, source, exon_class, key, (all_exon_range_dict[key]),\n",
    "                                           ((all_exon_range_dict[key]) - key + 1), identity_level, strand, \".\",\n",
    "                                           filename))\n",
    "    df = pd.DataFrame(exon_rows_to_concete_table, columns=column_names)\n",
    "\n",
    "    return exon_rows_to_concete_table, df\n",
    "\n",
    "\n",
    "def extending_source_data_frame(alignment_id):\n",
    "    # print(f\"Alignment: {alignment[0].id}, funkcja: extending_source_data_frame, przeszlo\")\n",
    "    keywords = [\"TRINITY\", \"BACKBONE\", \"SCAFFOLD\"]\n",
    "    found = False\n",
    "    for key in keywords:\n",
    "        alignment_id = str(alignment_id).lower()\n",
    "        if alignment_id.find(key.lower()) != -1:\n",
    "            source = key\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        source = None  # none means unknown\n",
    "    return source\n",
    "\n",
    "\n",
    "def extending_strand_data_frame(alignment_id):\n",
    "    # print(f\"Alignment: {alignment[0].id}, funkcja: extending_strand_data_frame, przeszlo\")\n",
    "    if str(alignment_id).find(\"SL+\") != -1:\n",
    "        strand = \"+\"\n",
    "    elif str(alignment_id).find(\"SL-\") != -1:\n",
    "        strand = \"-\"\n",
    "    else:\n",
    "        strand = None\n",
    "    return strand\n",
    "\n",
    "\n",
    "def adding_introns_to_gff_data_frame(all_exons_df, tlh_exons_df, fine_exons_df, column_names):\n",
    "    data_frames_list = [all_exons_df, tlh_exons_df, fine_exons_df]\n",
    "\n",
    "    for i, df in enumerate(data_frames_list):\n",
    "        df.sort_values(by=['seqid', 'start'], inplace=True)\n",
    "\n",
    "        df['seqid_introns'] = df['seqid'].shift(\n",
    "            -1)  # creating new column with seqid of next sequention. In next steps, rows without matching seqid and\n",
    "        # seqid_introns, would not be consideresd\n",
    "        df['intron_start'] = df['start'].shift(-1)  # creating index of start position in introns\n",
    "\n",
    "        intron_df_temp = df[df['seqid'] == df['seqid_introns']].copy()\n",
    "        intron_df_temp['start'] = df['end'] + 1\n",
    "        intron_df_temp['end'] = df['intron_start'] - 1\n",
    "        intron_df_temp['exon_type'] = 'intron'\n",
    "        intron_df_temp['homology'] = 0\n",
    "        intron_df_temp['length'] = intron_df_temp['end'] - intron_df_temp['start'] + 1\n",
    "        intron_df_temp = intron_df_temp[column_names]\n",
    "\n",
    "        df = df[column_names]\n",
    "        df = pd.concat([df, intron_df_temp]).sort_values(by=['seqid', 'start']).reset_index(drop=True)\n",
    "\n",
    "        data_frames_list[i] = df\n",
    "\n",
    "    return data_frames_list[0], data_frames_list[1], data_frames_list[2]\n",
    "\n",
    "\n",
    "def save_to_gff_file(gff_final_data_frame, filename):\n",
    "    # print(f\"Alignment: {alignment[0].id}, funkcja: save_to_gff_file, przeszlo\")\n",
    "    if os.path.isfile(filename):\n",
    "        user_input = input(f\"GFF file  already exists. Do you want overwrite? y/n: \")\n",
    "        if user_input == \"n\":\n",
    "            base_name, ext = os.path.splitext(filename)\n",
    "            i = 1\n",
    "            while os.path.isfile(f\"{base_name}_{i}{ext}\"):\n",
    "                i += 1\n",
    "            filename = f\"{base_name}_{i}{ext}\"\n",
    "            gff_final_data_frame.to_csv(filename, sep=\"\\t\")\n",
    "\n",
    "        elif user_input == \"y\":\n",
    "            gff_final_data_frame.to_csv(filename, sep=\"\\t\")\n",
    "        else:\n",
    "            print(\"Clarify your answer. Nothing has done.\")\n",
    "\n",
    "    else:\n",
    "        gff_final_data_frame.to_csv(filename, sep=\"\\t\")\n",
    "\n",
    "\n",
    "def extracting_strands_from_alignment(\n",
    "        alignment):  # that function describe what strand is transcript strand and genomic strand - in our files,\n",
    "    # transcript strand has longer ID.\n",
    "    # print(f\"Alignment: {alignment[0].id}, funkcja: extracting_strands_from_alignment, przeszlo\")\n",
    "    seq1 = alignment[0].seq\n",
    "    seq2 = alignment[1].seq\n",
    "    # if len(alignment[0].id) >= len(alignment[1].id):\n",
    "    #     seq1 = alignment[0].seq\n",
    "    #     seq2 = alignment[1].seq\n",
    "    #     #seq1_is_transcript = True #obecnie nie uzywane\n",
    "    # else:\n",
    "    #     seq1 = alignment[1].seq\n",
    "    #     seq2 = alignment[0].seq    \n",
    "    # seq1_is_transcript = False #obecnie nie uzywane\n",
    "    # print(f\"\\n Just run function: extracting_strands_from_alignment\")\n",
    "    return seq1, seq2  # return seq1_is_transcript #obecnie nie uzywane\n",
    "\n",
    "\n",
    "def getting_files_in_directory(path_to_file_after_mafft):\n",
    "    with os.scandir(path_to_file_after_mafft) as entries:\n",
    "        return [entry.name.split('.')[0] for entry in entries if entry.is_file()]\n",
    "\n",
    "def getting_files_from_df(df, column_name):\n",
    "    return df[column_name].astype(str).tolist(), df[column_name].nunique()\n",
    "\n",
    "def find_alignments_not_in_df(path_to_file_after_mafft, files_in_df):\n",
    "    files_in_directory = set(getting_files_in_directory(path_to_file_after_mafft))\n",
    "    files_in_df = set(files_in_df)\n",
    "    files_not_in_df = files_in_directory - files_in_df\n",
    "    return files_not_in_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cutting_scrap(path_to_file_before_mafft, path_to_file_after_mafft, 2, 0.97)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0175598-095b-4300-8771-14b24766ed79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f936e-814a-40de-816a-cc9fa1d2c801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
