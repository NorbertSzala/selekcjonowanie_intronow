{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f037dd59-8d78-4bbc-b1f8-f878afe1f342",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Bio import AlignIO, Align\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daa9c022-9b97-48b7-b7f7-9d01e024710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#description of goal\n",
    "    #main goal that function is select introns and exon from alignment and descripte their class, start - ends nucleotide.\n",
    "#description step by step\n",
    "    # 1. iterate through files in input before mafft and linking with input after mafft, by the same name\n",
    "    # 2. checking if both files have .fasta extension\n",
    "    # 3. making alignment in file after mafft\n",
    "    # 4. extracting_strands_from_alignment function is used to determine which strand is genomic strand or transcript\n",
    "    # 5. Removing gaps from 5' and 3'. Then pre-exon (candidat to be exon) will be first\n",
    "    # 6. Creating list with nucleotides's positions of intron or exon\n",
    "    # 7. Creating dictionary like: start_exon:end_exon. If distance between two following indices is bigger than two (acceptable gap length) then there is intron between them.\n",
    "    # 8. Making alignment and determinig class of exon according to their homology.\n",
    "    # 9. Forming a table with exons\n",
    "    # 10. Spliting a data frame to 3 others, with different exon's class\n",
    "    # 11. Adding introns to tables\n",
    "    # 12. Saving table in tsv gff format.\n",
    "\n",
    "#ASSUMPTIONS\n",
    "#- both files, path_to_file_before_MAFFT and path_to_file_after_MAFFT have to have the same name\n",
    "#- they have to be .fasta\n",
    "#- acceptable_gap_length is int.\n",
    "\n",
    "min_length_aligned_sequence = 30 #Minimal lenght of sequence which could be an exon\n",
    "# extreme_homology = 0.97 #percentage of homology of sequence, treshold #I assume two faulty aligned nucleotides per 100 (98%) and one more nt because sometimes latest nt can move from end of one sequence to beginning next sequence\n",
    "\n",
    "path_to_file_before_MAFFT = \"/home/norbert/mrdn/euglena/kod_i_pliki/surowe_pliki_plus_minus_500/raw_reads_9/merging_fastas\"\n",
    "path_to_file_after_MAFFT = \"/home/norbert/mrdn/euglena/kod_i_pliki/surowe_pliki_plus_minus_500/raw_reads_9/fastas_after_mafft_na_probe\"\n",
    "\n",
    "# path_to_file_before_MAFFT = '/home/norbert/mrdn/euglena/kod_i_pliki/pliki_ktore_nie_przeszly_divided_fastas'\n",
    "# path_to_file_after_MAFFT = '/home/norbert/mrdn/euglena/kod_i_pliki/pliki_ktore_nie_przeszly_fastas_after_mafft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f91688eb-6822-4df4-b7fc-59c484ad9f0c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      " \n",
      " Start running function: cutting_scrap\n",
      "\n",
      " ################################################################################### \n",
      " running file: LON_OG0030309.fasta which is 1 of 9, what means 11.11% of advancement \n",
      " ################################################################################### \n",
      "\n",
      "\n",
      " ################################################################################### \n",
      " running file: HIE_OG0030307.fasta which is 2 of 9, what means 22.22% of advancement \n",
      " ################################################################################### \n",
      "\n",
      "\n",
      " ################################################################################### \n",
      " running file: LON_OG0030307.fasta which is 3 of 9, what means 33.33% of advancement \n",
      " ################################################################################### \n",
      "\n",
      "\n",
      " ################################################################################### \n",
      " running file: GRA_OG0030307.fasta which is 4 of 9, what means 44.44% of advancement \n",
      " ################################################################################### \n",
      "\n",
      "\n",
      " ################################################################################### \n",
      " running file: GRA_OG0030308.fasta which is 5 of 9, what means 55.56% of advancement \n",
      " ################################################################################### \n",
      "\n",
      "\n",
      " ################################################################################### \n",
      " running file: LON_OG0030308.fasta which is 6 of 9, what means 66.67% of advancement \n",
      " ################################################################################### \n",
      "\n",
      "\n",
      " ################################################################################### \n",
      " running file: HIE_OG0030309.fasta which is 7 of 9, what means 77.78% of advancement \n",
      " ################################################################################### \n",
      "\n",
      "\n",
      " ################################################################################### \n",
      " running file: GRA_OG0030309.fasta which is 8 of 9, what means 88.89% of advancement \n",
      " ################################################################################### \n",
      "\n",
      "\n",
      " ################################################################################### \n",
      " running file: HIE_OG0030308.fasta which is 9 of 9, what means 100.0% of advancement \n",
      " ################################################################################### \n",
      "\n",
      "100.0% of files end up succesfully. 0 had error. List of files with errors in broken_files file.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def cutting_scrap(path_to_file_before_MAFFT, path_to_file_after_MAFFT, acceptable_gap_length, extreme_homology):\n",
    "    gff_data_frames = [] #pre-list for tables\n",
    "    gaps_signs = \"-\" * acceptable_gap_length #maximum length of gaps in sequence in one exon's sequence\n",
    "    files_in_progress = 0\n",
    "    broken_files = [] #list of files which caused error\n",
    "    nucleotides = [\"a\", \"t\", \"g\", \"c\", \"A\", \"T\", \"G\", \"C\"]\n",
    "    column_names = ['seqid', 'source', 'exon_type', 'start', 'end', 'length', 'homology', 'strand', 'phase', 'attributes']\n",
    "    print(f\"\\n \\n \\n \\n Start running function: cutting_scrap\")\n",
    "\n",
    "    total_files = len([f for f in os.listdir(path_to_file_after_MAFFT) if f.endswith(\".fasta\")])\n",
    "\n",
    "    #linking two files from input and making alignment\n",
    "    for filename in os.listdir(path_to_file_after_MAFFT):\n",
    "        file = os.path.join(path_to_file_after_MAFFT, filename)\n",
    "        if not os.path.isfile(file):\n",
    "            continue\n",
    "        if file.endswith(\".fasta\"):\n",
    "            try:\n",
    "                files_in_progress += 1\n",
    "                count_files = percentage_of_advancement(path_to_file_before_MAFFT)\n",
    "                print(f\"\\n ################################################################################### \\n running file: {filename} which is {files_in_progress} of {count_files}, what means {round(files_in_progress/count_files*100, 2)}% of advancement \\n ################################################################################### \\n\")\n",
    "                alignment = AlignIO.read(file, \"fasta\")\n",
    "                alignment_id = alignment[0].id\n",
    "                alignment_DIDNT_TOUCHED = AlignIO.read(file, \"fasta\")\n",
    "    \n",
    "    \n",
    "                file_before_MAFFT = os.path.join(path_to_file_before_MAFFT, filename)\n",
    "                if not os.path.isfile(file_before_MAFFT):\n",
    "                    continue\n",
    "        \n",
    "                # extracting_strands_from_alignment function is used to determine which strand is genomic strand or transcript\n",
    "                seq1_DT, seq2_DT = extracting_strands_from_alignment(alignment_DIDNT_TOUCHED)\n",
    "                seq1, seq2 = extracting_strands_from_alignment(alignment)\n",
    "    \n",
    "                # Removing gaps from 5' and 3'. Then pre-exon (candidat to be exon) will be first\n",
    "                seq1, seq2, count_deleted_gaps_left = cleaning_gaps_from_both_edges(seq1_DT, seq2_DT, alignment, filename) #deleting gaps from 5' and 3'. It is important. The next function start counting from possible exon.\n",
    "        \n",
    "                # Creating list with nucleotides's positions of intron or exon\n",
    "                temp_intron, temp_exon, invalid_nucleotides = indices_of_introns_and_exons_fun(seq1_DT, seq2_DT, seq1, seq2, count_deleted_gaps_left, alignment_id)\n",
    "                \n",
    "                # Creating dictionary like: start_exon:end_exon. If distance between two following indices is bigger than two (acceptable gap length) then there is intron between them.\n",
    "                all_exon_range_dict = start_and_end_parameters_of_exons_dict_fun(temp_exon, gaps_signs)\n",
    "        \n",
    "        \n",
    "                # Making alignment and determinig class of exon according to their homology.\n",
    "                # Forming a table with exons\n",
    "                exon_rows_to_concete_table, one_alignment_df = making_exons_gff_table_fun(all_exon_range_dict, min_length_aligned_sequence, extreme_homology, seq1_DT, seq2_DT, alignment_id, column_names, filename[:-6])\n",
    "\n",
    "                if one_alignment_df.empty:\n",
    "                    print(f'File: {filename} is out of exons and introns. Processing failed. Broken_file list appended.')\n",
    "                    broken_files.append(filename)\n",
    "                else:\n",
    "                    gff_data_frames.append(one_alignment_df)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "                broken_files.append(filename)\n",
    "        \n",
    "    \n",
    "    # spliting a data frame to 3 others, with different exon's class\n",
    "    if not len(gff_data_frames) == 0:\n",
    "        all_exons_df = pd.concat(gff_data_frames, ignore_index = True)\n",
    "        TLH_exons_df = all_exons_df[all_exons_df['exon_type'] == 'TLH'].copy()\n",
    "        fine_exons_df = all_exons_df[all_exons_df['exon_type'] == 'fine'].copy()\n",
    "\n",
    "        all_exons_df, TLH_exons_df, fine_exons_df = adding_introns_to_gff_data_frame(all_exons_df, TLH_exons_df, fine_exons_df, column_names)\n",
    "\n",
    "        print(f'{((total_files - len(broken_files)) / total_files) * 100 }% of files end up succesfully. {len(broken_files)} had error. List of files with errors in broken_files file. Invalid nucleotides (different than A T G C) are written with positions in invalid_nucleotides file')\n",
    "\n",
    "        fine_exons_count = fine_exons_df['exon_type'].str.contains('fine', case = False).sum()\n",
    "\n",
    "    else:\n",
    "        print(\"No valid data frames to concatenate.\")\n",
    "        all_exons_df = pd.DataFrame(columns=column_names)\n",
    "        TLH_exons_df = pd.DataFrame(columns=column_names)\n",
    "        fine_exons_df = pd.DataFrame(columns=column_names)\n",
    "        fine_exons_count = 0        \n",
    "        \n",
    "    # Adding introns to tables\n",
    "\n",
    "    # Saving table in tsv gff format\n",
    "    # save_to_gff_file(all_exons_df, 'broken_all_exons_gff.tsv')\n",
    "    # save_to_gff_file(TLH_exons_df, 'broken_TLH_exons_gff.tsv')\n",
    "    # save_to_gff_file(fine_exons_df, 'broken_fine_exons_gff.tsv')\n",
    "    print(len(broken_files))\n",
    "    with open('broken_files', 'w') as file:\n",
    "        file.write('lista plikow ktora nie przeszla programu \\n ')\n",
    "        file.write(str(broken_files))\n",
    "    length_fine_exons_df = len(fine_exons_df)\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        for alignment_id, nt1, nt2, pos in invalid_nucleotides:\n",
    "            file.write(f'{alignment_id}\\t{nt1}\\t{nt2}\\t{pos}\\n')\n",
    "        \n",
    "    return fine_exons_count, length_fine_exons_df\n",
    "\n",
    "                   \n",
    "\n",
    "#############################################################################################################################\n",
    "#######################################                MINOR FUNCTIONS                #######################################       \n",
    "#############################################################################################################################\n",
    "\n",
    "def percentage_of_advancement(directory):\n",
    "    count = 0\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".fasta\"):\n",
    "            count += 1\n",
    "    return(count)\n",
    "    \n",
    "\n",
    "def cleaning_gaps_from_both_edges(sequence_transcipt, sequence_genome, alignment, filename):\n",
    "    sequence_transcipt_left_shorted = sequence_transcipt.lstrip(\"-\")\n",
    "    count_deleted_gaps_left = len(sequence_transcipt) - len(sequence_transcipt_left_shorted) #inform how many gaps were deleted from 5'\n",
    "    \n",
    "    sequence_transcipt_right_and_left_shorted = sequence_transcipt_left_shorted.rstrip(\"-\")\n",
    "    count_deleted_gaps_right = len(sequence_transcipt_left_shorted) - len(sequence_transcipt_right_and_left_shorted) #inform how many gaps were deleted from 3'\n",
    "    \n",
    "    if count_deleted_gaps_right:\n",
    "        sequence_genome_left_and_right_shorted = sequence_genome[count_deleted_gaps_left:-count_deleted_gaps_right]\n",
    "    else:\n",
    "        sequence_genome_left_and_right_shorted = sequence_genome[count_deleted_gaps_left:]\n",
    "\n",
    "    if len(sequence_genome_left_and_right_shorted) != len(sequence_transcipt_right_and_left_shorted):\n",
    "        raise ValueError(f\" VALUE ERROR: Given sequences: {alignment[0].id} - len: {len(alignment[0])} and  {alignment[1].id} - len {len(alignment[1])} from {filename} must have the same length.\")\n",
    "    return sequence_transcipt_right_and_left_shorted, sequence_genome_left_and_right_shorted, count_deleted_gaps_left\n",
    "\n",
    "\n",
    "def indices_of_introns_and_exons_fun(seq1_DT, seq2_DT, seq1, seq2, count_deleted_gaps_left, alignment_id):\n",
    "    #linking pre-exon's nucleotides (nt-nt pairs) and pre-intron's nucleotides (gap-nt pairs) to theirs indices \n",
    "    #NOTE: output is nucleotide or gap from seq1 (transcript sequence). Even if it is gap like this: \n",
    "    #seq1: aaatttggg, seq2: aaa---ggg, output will be 'ttt' instead '---'\n",
    "    #it shows that where were gap, seq2 genome or in seq1 transcript\n",
    "    temp_exon = [] #list containing indices of intron's positions in sequence.\n",
    "    temp_intron = [] #list containing indices of intron's positions in sequence.\n",
    "    invalid_nucleotides = []\n",
    "    index = 0\n",
    "    paired_nucleotides = zip(seq1, seq2)\n",
    "    valid_nucleotides = set('ATGCatgc')\n",
    "    \n",
    "    if len(seq1_DT) != len(seq2_DT):\n",
    "        raise ValueError(\"The two sequences must be of the same length.\")\n",
    "        print(seq1)\n",
    "        print(seq2)\n",
    "        \n",
    "    for nt1, nt2 in paired_nucleotides:\n",
    "        if \"-\" in (nt1, nt2):\n",
    "            temp_intron.append(index + count_deleted_gaps_left)\n",
    "        elif nt1 not in valid_nucleotides or nt2 not in valid_nucleotides:\n",
    "            if alignment_id not in invalid_nucleotides:\n",
    "                invalid_nucleotides.append((alignment_id, nt1, nt2, index + count_deleted_gaps_left))  # Dodanie nukleotydów i pozycji\n",
    "        else:\n",
    "            temp_exon.append(index + count_deleted_gaps_left)\n",
    "        index += 1 #indeks do wskazywania pozycji w sekwencji \n",
    "    return temp_intron, temp_exon, invalid_nucleotides\n",
    "\n",
    "\n",
    "def start_and_end_parameters_of_exons_dict_fun(temp_exon, gaps_signs):\n",
    "    #linking indices into single strand of possible exons\n",
    "    start_exon = temp_exon[1]\n",
    "    all_exon_range_dict = {} #that dictionary contains: key(index of start exon) and value(index of end exon)\n",
    "    \n",
    "    for i in range(len(temp_exon)): \n",
    "        #print(start_exon)\n",
    "        if (temp_exon[i] - temp_exon[i-1]) > 1+len(gaps_signs): #if difference between two indices of exons's positions is bigger than given number (2), that smaller number is index of 3' nucleotide in exon \n",
    "            end_exon = temp_exon[i-1] #temp_exon[i-1] because it is index in list temp_exon. +1 because python starts counting from 0\n",
    "            all_exon_range_dict[start_exon] = end_exon +1  #creating dictionary with all exons, even with theese too short and with too low homology\n",
    "            start_exon = temp_exon[i] +1\n",
    "            #brakuje ostatniego eksonu. Jest to spowodowane tym, ze nie mozna okreslic parametru end_exon, gdyz nie w liscie temp_exon wiekszego numeru niz ten ostatni\n",
    "    all_exon_range_dict[start_exon] = temp_exon[-1] #last pair\n",
    "\n",
    "    if all_exon_range_dict == {}:\n",
    "        raise ValueError(f\"ValueError - all_exon_range_dict seems to be empty. Its length = {len(all_exon_range_dict)}\")\n",
    "    #print(f\" \\n all_exon_range_dict: {all_exon_range_dict}\")\n",
    "    return (all_exon_range_dict)\n",
    "\n",
    "\n",
    "     \n",
    "def start_and_end_parameters_of_introns_dict_fun(all_exon_range_dict):\n",
    "    #linking indices into single strand of possible introns\n",
    "    intron_range_dict = {}\n",
    "    keys_from_all_exon_range_dict = sorted(all_exon_range_dict.keys()) #list that contains exons's start positions\n",
    "    #print(f\"keys_from_all_exon_range_dict: {keys_from_all_exon_range_dict}\\n\")\n",
    "\n",
    "    for i in range(len(keys_from_all_exon_range_dict) - 1):\n",
    "        start_intron = all_exon_range_dict[keys_from_all_exon_range_dict[i]] +1 \n",
    "        end_intron = keys_from_all_exon_range_dict[i + 1] - 1\n",
    "        intron_range_dict[start_intron] = end_intron\n",
    "    if intron_range_dict == {}:\n",
    "        raise ValueError(f\"ValueError - intron_range_dict seems to be empty. Its length = {len(intron_range_dict)}\")\n",
    "    #print(f\" \\n intron_range_dict: {intron_range_dict}\")\n",
    "    return (intron_range_dict)\n",
    "\n",
    "\n",
    "#zmienione all_fine_exons na fine_exons\n",
    "def making_exons_gff_table_fun(all_exon_range_dict, min_length_aligned_sequence, extreme_homology, seq1_DT, seq2_DT, alignment_id, column_names, filename):\n",
    "    exon_rows_to_concete_table = []\n",
    "\n",
    "    aligner = Align.PairwiseAligner()\n",
    "    aligner.mismatch_score = 0 #customized setting towards get pure percentage of homology, not alignment with gap penalty etc. The object of interest is simply that how many nt in query has equivalend in target seq. \n",
    "    aligner.open_gap_score = 0\n",
    "    aligner.extend_gap_score = 0\n",
    "    \n",
    "    for key in all_exon_range_dict:    \n",
    "        query = seq1_DT[key -1 : all_exon_range_dict[key]]\n",
    "        target = seq2_DT[key-1 : all_exon_range_dict[key]]\n",
    "        target_length = len(target)\n",
    "    \n",
    "        if target_length > min_length_aligned_sequence:\n",
    "            score = aligner.score(query, target)\n",
    "            identity_level = (round(((score * 100) / len(target)), 2))\n",
    "            #print(f\"query: {query}, \\n target: {target}, percent of homology = {round(score / len(target), 2) * 100} %, score = {score}, len = {len(target)}\\n\")\n",
    "            if identity_level >= extreme_homology * 100:\n",
    "                exon_class = 'fine'\n",
    "                # print(f\"\\n query: {query} \\n target: {target} \\n with start: {key} and end: {all_exon_range_dict[key]} goes to fine_exons with homology: {identity_level} \\n\")\n",
    "                # if len(target) != len(query):\n",
    "                    # print(f'target: {len(target)} query: {len(query)}')\n",
    "            else:\n",
    "                exon_class = 'TLH'  \n",
    "                # print(f\"\\n query: {query} \\n target: {target} \\n with start: {key} and end: {all_exon_range_dict[key]} goes to TLH_exons with homology: {identity_level}\")\n",
    "                # if len(target) != len(query):\n",
    "                    # print(f'target: {len(target)} query: {len(query)}')\n",
    "        else:\n",
    "            identity_level = 0\n",
    "            exon_class = \"TS\" \n",
    "            # print(f\"\\n query: {query} \\n target: {target} \\n with start: {key} and end: {all_exon_range_dict[key]} goes to TS_exons with homology: {identity_level}\")\n",
    "            # if len(target) != len(query):\n",
    "                    # print(f'target: {len(target)} query: {len(query)}')\n",
    "        source = extending_source_data_frame(alignment_id)\n",
    "        strand = extending_strand_data_frame(alignment_id)\n",
    "        exon_rows_to_concete_table.append((alignment_id, source, exon_class, (key), (all_exon_range_dict[key]), ((all_exon_range_dict[key]) - (key)+1), identity_level, strand, \".\", filename))\n",
    "    df = pd.DataFrame(exon_rows_to_concete_table, columns = column_names)\n",
    "    \n",
    "    return exon_rows_to_concete_table, df\n",
    "\n",
    "    \n",
    "\n",
    "def extending_source_data_frame(alignment_id):\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: extending_source_data_frame, przeszlo\")\n",
    "    keywords = [\"TRINITY\", \"BACKBONE\", \"SCAFFOLD\"]\n",
    "    found = False\n",
    "    for key in keywords:\n",
    "        alignment_id = str(alignment_id).lower()\n",
    "        if alignment_id.find(key.lower()) != -1:\n",
    "            source = key\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        source = None #none means unknown\n",
    "    return source\n",
    "\n",
    "\n",
    "def extending_strand_data_frame(alignment_id): \n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: extending_strand_data_frame, przeszlo\")\n",
    "    if str(alignment_id).find(\"SL+\") != -1:\n",
    "        strand = \"+\"\n",
    "    elif str(alignment_id).find(\"SL-\") != -1:\n",
    "        strand = \"-\"\n",
    "    else:\n",
    "        strand = None\n",
    "    return strand\n",
    "\n",
    "\n",
    "def adding_introns_to_gff_data_frame(all_exons_df, TLH_exons_df, fine_exons_df, column_names):\n",
    "    data_frames_list = [all_exons_df, TLH_exons_df, fine_exons_df]\n",
    "\n",
    "    for i, df in enumerate(data_frames_list):\n",
    "        df.sort_values(by = ['seqid', 'start'], inplace = True)\n",
    "        \n",
    "        df['seqid_introns'] = df['seqid'].shift(-1) #creating new column with seqid of next sequention. In next steps, rows without matching seqid and seqid_introns, would not be consideresd\n",
    "        df['intron_start'] = df['start'].shift(-1) #creating index of start position in introns\n",
    "        \n",
    "        intron_df_temp = df[df['seqid'] == df['seqid_introns']].copy()\n",
    "        intron_df_temp['start'] = df['end'] + 1\n",
    "        intron_df_temp['end'] = df['intron_start'] - 1\n",
    "        intron_df_temp['exon_type'] = 'intron'\n",
    "        intron_df_temp['homology'] = 0\n",
    "        intron_df_temp['length'] = intron_df_temp['end'] - intron_df_temp['start'] + 1\n",
    "        intron_df_temp = intron_df_temp[column_names]\n",
    "\n",
    "        df = df[column_names]        \n",
    "        df = pd.concat([df, intron_df_temp]).sort_values(by = ['seqid', 'start']).reset_index(drop = True)\n",
    "\n",
    "        data_frames_list[i] = df\n",
    "\n",
    "    return data_frames_list[0], data_frames_list[1], data_frames_list[2]\n",
    "\n",
    "\n",
    "def save_to_gff_file(gff_final_data_frame, filename):\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: save_to_gff_file, przeszlo\")\n",
    "    if os.path.isfile(filename):\n",
    "        user_input = input(f\"GFF file  already exists. Do you want overwrite? y/n: \")\n",
    "        if user_input == \"n\":\n",
    "            base_name, ext = os.path.splitext(filename)\n",
    "            i = 1\n",
    "            while os.path.isfile(f\"{base_name}_{i}{ext}\"):\n",
    "                i += 1\n",
    "            filename = f\"{base_name}_{i}{ext}\"\n",
    "            gff_final_data_frame.to_csv(filename, sep = \"\\t\")\n",
    "            \n",
    "        elif user_input == \"y\":\n",
    "            gff_final_data_frame.to_csv(filename, sep = \"\\t\")\n",
    "        else:\n",
    "            print(\"Clarify your answer. Nothing has done.\")\n",
    "            \n",
    "    else:\n",
    "        gff_final_data_frame.to_csv(filename, sep = \"\\t\")\n",
    "\n",
    "\n",
    "def extracting_strands_from_alignment(alignment): #that function describe what strand is transcript strand and genomic strand - in our files, transcript strand has longer ID.\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: extracting_strands_from_alignment, przeszlo\")\n",
    "    seq1 = alignment[0].seq\n",
    "    seq2 = alignment[1].seq\n",
    "    # if len(alignment[0].id) >= len(alignment[1].id):\n",
    "    #     seq1 = alignment[0].seq\n",
    "    #     seq2 = alignment[1].seq\n",
    "    #     #seq1_is_transcript = True #obecnie nie uzywane\n",
    "    # else:\n",
    "    #     seq1 = alignment[1].seq\n",
    "    #     seq2 = alignment[0].seq    \n",
    "        #seq1_is_transcript = False #obecnie nie uzywane\n",
    "    #print(f\"\\n Just run function: extracting_strands_from_alignment\")\n",
    "    return seq1, seq2\n",
    "    #return seq1_is_transcript #obecnie nie uzywane\n",
    "\n",
    "def function_progress(alignment, alignment_DIDNT_TOUCHED, files_in_progress, count_files):\n",
    "    #do tego tematu wrocimy jak zrobimy tabele z odrzuconymi wartosciami\n",
    "    first_50_nt_alignment = str((alignment[1].seq)[:20])\n",
    "    #print(f\" {round((100-(len(alignment[0])\\/len(alignment_DIDNT_TOUCHED[0]))*100), 2)}% % base pair done of {alignment[0].id}. {files_in_progress} of {count_files}, what means {round(files_in_progress/count_files*100, 2)}% of advancement. The time is: {time.strftime('%H:%M:%S', time.localtime())}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cutting_scrap(path_to_file_before_MAFFT, path_to_file_after_MAFFT, 2, 0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0175598-095b-4300-8771-14b24766ed79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f936e-814a-40de-816a-cc9fa1d2c801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
