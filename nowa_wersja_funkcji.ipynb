{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f037dd59-8d78-4bbc-b1f8-f878afe1f342",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/norbert/miniconda3/lib/python3.11/site-packages/Bio/pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from Bio import AlignIO, SeqIO, Seq, pairwise2, Align\n",
    "from Bio.pairwise2 import format_alignment\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bcfc1da-09e6-4c26-9117-eb29a1b363c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nuzupelnij do konca tworzenie listy exonow\\nto samo zrob dla intronow\\n\\nsklekotaj dwie pozostale tabelki o zadane parametry\\n\\ndodaj funkcje liczaca wszystkie pliki w folderze oraz dajaca input ile z tych plikow przeszlo pomyslnie procedury liczenia\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "uzupelnij do konca tworzenie listy exonow\n",
    "to samo zrob dla intronow\n",
    "\n",
    "sklekotaj dwie pozostale tabelki o zadane parametry\n",
    "\n",
    "dodaj funkcje liczaca wszystkie pliki w folderze oraz dajaca input ile z tych plikow przeszlo pomyslnie procedury liczenia\n",
    "\n",
    "'''\n",
    "\n",
    "#dobra\n",
    "#beda trzy tabelki outputowe, wiec musisz napisac trzy funkcje do robienia tych tabelek. Dlatego pamietaj przy argumentach by dawac odpowiednie zrodla dancyh all fine TS_ TLH_ etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daa9c022-9b97-48b7-b7f7-9d01e024710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ASSUMPTIONS\n",
    "#- both file, path_to_file_before_MAFFT and path_to_file_after_MAFFT have to have the same name\n",
    "#- they have to be .fasta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "min_length_aligned_sequence = 30 #Minimal lenght of sequence which could be an exon\n",
    "extreme_homology = 0.97 #percentage of homology of sequence, treshold #I assume two faulty aligned nucleotides per 100 (98%) and one more nt because sometimes latest nt can move from end of one sequence to beginning next sequence\n",
    "\n",
    "#9 plikow\n",
    "path_to_file_before_MAFFT = \"/home/norbert/mrdn/euglena/kod_i_pliki/surowe_pliki_plus_minus_500/raw_reads_9/merging_fastas\"\n",
    "path_to_file_after_MAFFT = \"/home/norbert/mrdn/euglena/kod_i_pliki/surowe_pliki_plus_minus_500/raw_reads_9/fastas_after_mafft_na_probe\"\n",
    "\n",
    "#ten niedzialajacy pierwszy LON309\n",
    "#path_to_file_before_MAFFT = \"/home/norbert/mrdn/euglena/kod_i_pliki/surowe_pliki_plus_minus_500/raw_reads_9/roboczy_przed_fastami/\"\n",
    "#path_to_file_after_MAFFT = \"/home/norbert/mrdn/euglena/kod_i_pliki/surowe_pliki_plus_minus_500/raw_reads_9/roboczy_po_fastach/\"\n",
    "\n",
    "#pozostale 8\n",
    "#ath_to_file_before_MAFFT = \"/home/norbert/mrdn/euglena/kod_i_pliki/surowe_pliki_plus_minus_500/raw_reads_9/inne_przed_fastami/\"\n",
    "#path_to_file_after_MAFFT = \"/home/norbert/mrdn/euglena/kod_i_pliki/surowe_pliki_plus_minus_500/raw_reads_9/inne_po_fastach/\"\n",
    "\n",
    "\n",
    "#DODAJ FRAGMENT MOWIACY ZE JESLI: NA RAZIE SIE WSTRZYMAM, MOZE NIE JEST TO POTRZEBNE\n",
    "    #LICZBA INTRONOW >= LICZBIE EKSONOW TO BREAK\n",
    "    #LICZBA INTRONOW + EKSONOW <= 3 TO BREAK\n",
    "\n",
    "\n",
    "#dodac tabelke tak by mowila ile bylo odczytow na inpucie, ile przeszło pozytywnie a ile zostało przerwanych w trakcie\n",
    "#dodatkowa tabelka ze wszystkimi eksonami które byly za krótkie teżhbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "42d489c2-e090-4244-98db-a453e82cc84b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      " \n",
      " Just run function: cutting_scrap\n",
      "\n",
      " ################################################################################### \n",
      " running file: LON_OG0030309.fasta which is 1 of 9, what means 11.11% of advancement \n",
      " ################################################################################### \n",
      "\n",
      "{0: 112, 695: 920, 1350: 1586, 1904: 2056, 2141: 2222, 3188: 3252, 3546: 3684, 4033: 4126, 4878: 5096, 5866: 6045}\n",
      " fine_exons_range_dict: {695: 920, 1350: 1586, 1904: 2056, 3546: 3684, 4878: 5096, 5866: 6045}, \n",
      " too short exon range dict {0: 112}, \n",
      " too low homology exon range dict {2141: 2222, 3188: 3252, 4033: 4126}\n",
      "alignment: LON_SL+_GGOE01000359.1: and his all fine exon sequence: \n",
      " accgtcaagtgcctgtcaaccgattccgttggcaaccgccttgagcgattgcaacagaccaatgcatcgctggcccgggagatggcagcgctgcgggagcgggcggagtacctggacgtgatggaggagatgaagtacttctcggagcggcccatgacccccatcagccacgctgtgttccacccctttgccccgcacctgacccacaagactgcgttgttgcaggg \n",
      "\n",
      "alignment: LON_SL+_GGOE01000359.1: and his all fine exon sequence: \n",
      " ggccctcttcctcaaggcggagctgcccatccgcctggcccactccgcccatctgctcgaccgcctaccccgtgccctcgggggcctcgccgcgttccagcaggtgcgagagatgttcaccgactcgttccgtgacctgcggtccttccagttctccaagcctgccccggtttgggaggacagccagcggttccacgacatgctctttgttctccagaaccgccacaagcacgtccct \n",
      "\n",
      "alignment: LON_SL+_GGOE01000359.1: and his all fine exon sequence: \n",
      " gaggtgattttccagggctgcaagcaacttcgcgctcagctgcaccgcacgcacggtcccaattgggcggaccaccctgaccagcatctcacccaggacgtgctggacgaatttttcggagggcgtgcagttgttcgctttctggtcggtcagc \n",
      "\n",
      "alignment: LON_SL+_GGOE01000359.1: and his all fine exon sequence: \n",
      " gacattgaactcacctgcaactgtgggacaccgctggtgcaaatcgggcaccacatccagttcatcaccatccgcctgctgcgacatgccatcataggcgtgctgcaggcccatggcgtggagaaggtgaaggaaggcaa \n",
      "\n",
      "alignment: LON_SL+_GGOE01000359.1: and his all fine exon sequence: \n",
      " ccaagttgcgccagctgtggagctatttgcacgacgacgctcaccttggggagcaggagtgggtggcggcgacggaggaggacaagcagtatggggaaggcttcgggctgcccctcgcccgcttgcgtgcttggctctttgggggcgacgtcgtcgtccagagcgtccagggcgtcggcctggacatgtacgcatacatcccaaagttcaagccagagga \n",
      "\n",
      "alignment: LON_SL+_GGOE01000359.1: and his all fine exon sequence: \n",
      " ggtgctttctggacccgcgtttcaaaagtgacctcctattcgtgtcccctgctcgcttgtgcccttccctttgctcctttccttcgttggatttccccggtccctgggtgcggacagctccgttgctgccattcccgcacattctttttaccaccactggaacgaggaaggagtggtgcac \n",
      "\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " accgtcaagtgcctgtcaaccgattccgttggcaaccgccttgagcgattgcaacagaccaatgcatcgctggcccgggagatggcagcgctgcgggagcgggcggagtacctggacgtgatggaggagatgaagtacttctcggagcggcccatgacccccatcagccacgctgtgttccacccctttgccccgcacctgacccacaagactgcgttgttgcaggg\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " ggccctcttcctcaaggcggagctgcccatccgcctggcccactccgcccatctgctcgaccgcctaccccgtgccctcgggggcctcgccgcgttccagcaggtgcgagagatgttcaccgactcgttccgtgacctgcggtccttccagttctccaagcctgccccggtttgggaggacagccagcggttccacgacatgctctttgttctccagaaccgccacaagcacgtccct\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " gaggtgattttccagggctgcaagcaacttcgcgctcagctgcaccgcacgcacggtcccaattgggcggaccaccctgaccagcatctcacccaggacgtgctggacgaatttttcggagggcgtgcagttgttcgctttctggtcggtcagc\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " gacattgaactcacctgcaactgtgggacaccgctggtgcaaatcgggcaccacatccagttcatcaccatccgcctgctgcgacatgccatcataggcgtgctgcaggcccatggcgtggagaaggtgaaggaaggcaa\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " ccaagttgcgccagctgtggagctatttgcacgacgacgctcaccttggggagcaggagtgggtggcggcgacggaggaggacaagcagtatggggaaggcttcgggctgcccctcgcccgcttgcgtgcttggctctttgggggcgacgtcgtcgtccagagcgtccagggcgtcggcctggacatgtacgcatacatcccaaagttcaagccagagga\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " ggtgctttctggacccgcgtttcaaaagtgacctcctattcgtgtcccctgctcgcttgtgcccttccctttgctcctttccttcgttggatttccccggtccctgggtgcggacagctccgttgctgccattcccgcacattctttttaccaccactggaacgaggaaggagtggtgcac\n",
      "\n",
      " \n",
      " \n",
      " WIELKIE PODSUMOWANIE: \n",
      "\n",
      "\n",
      "\n",
      "poczatki dla fine: dict_keys([695, 1350, 1904, 3546, 4878, 5866]), \n",
      " konce: dict_values([920, 1586, 2056, 3684, 5096, 6045])\n",
      "seqid: \n",
      " ['LON_SL+_GGOE01000359.1', 'LON_SL+_GGOE01000359.1', 'LON_SL+_GGOE01000359.1', 'LON_SL+_GGOE01000359.1', 'LON_SL+_GGOE01000359.1', 'LON_SL+_GGOE01000359.1'], \n",
      " source \n",
      " [None, None, None, None, None, None],  \n",
      " typ: ['exon', 'exon', 'exon', 'exon', 'exon', 'exon'],  \n",
      " score_list: [100.0, 100.0, 100.0, 100.0, 100.0, 99.45], \n",
      " strand: ['+', '+', '+', '+', '+', '+'], \n",
      " phase: ['.', '.', '.', '.', '.', '.'], \n",
      " attributes: ['LON_SL+_GGOE01000359.1', 'LON_SL+_GGOE01000359.1', 'LON_SL+_GGOE01000359.1', 'LON_SL+_GGOE01000359.1', 'LON_SL+_GGOE01000359.1', 'LON_SL+_GGOE01000359.1'], \n",
      " start: [695, 1350, 1904, 3546, 4878, 5866], \n",
      " end: [920, 1586, 2056, 3684, 5096, 6045]\n",
      "{113: 694, 921: 1349, 1587: 1903, 2057: 2140, 2223: 3187, 3253: 3545, 3685: 4032, 4127: 4877, 5097: 5865}\n",
      "\n",
      " ################################################################################### \n",
      " running file: HIE_OG0030307.fasta which is 2 of 9, what means 22.22% of advancement \n",
      " ################################################################################### \n",
      "\n",
      "{0: 4, 16: 34, 157: 324, 957: 1062, 1237: 1347, 1657: 1717, 2285: 2437, 2845: 2933, 3167: 3298, 5114: 5176, 5327: 5371, 5809: 5951}\n",
      " fine_exons_range_dict: {157: 324, 957: 1062, 1237: 1347, 2285: 2437, 3167: 3298, 5809: 5951}, \n",
      " too short exon range dict {0: 4, 16: 34}, \n",
      " too low homology exon range dict {1657: 1717, 2845: 2933, 5114: 5176, 5327: 5371}\n",
      "alignment: HIE_SL+_TRINITY_DN44987_c1_g4_i1: and his all fine exon sequence: \n",
      " gggcgctgttctgtgccacacccataataccaatggcggaggaggacgccccggacgagaacacgatgatctactggacgggcttccagggggacgaggaggacgacctcggggagcggtgcgagcagcgcgagcggatgttcttctcccgcgactactgggcccgccc \n",
      "\n",
      "alignment: HIE_SL+_TRINITY_DN44987_c1_g4_i1: and his all fine exon sequence: \n",
      " caacccattccatctcgggcctcggcggaaaaatggcgatttcctcggcgtcaaagtcgtgaagaaccgcttctttgtgccgtacctctatggtggccttgaggaac \n",
      "\n",
      "alignment: HIE_SL+_TRINITY_DN44987_c1_g4_i1: and his all fine exon sequence: \n",
      " gggtccaccacgcccggggcatgacccacgaccacctcgtgccgtatgtgggctgctattgtgaccacagccgagaggaaacactgctatggcagcaccatgtacagggggc \n",
      "\n",
      "alignment: HIE_SL+_TRINITY_DN44987_c1_g4_i1: and his all fine exon sequence: \n",
      " ggctgtacagcgtgctgaagggcctgcagcacctccaccagcacggctgcgtgctcggcaagctgcgcgcctccaaggtggtcctcgaccgggacggtgttccggtactggtggactgggcctgcccgccgaacctgttcaagtccatctacac \n",
      "\n",
      "alignment: HIE_SL+_TRINITY_DN44987_c1_g4_i1: and his all fine exon sequence: \n",
      " aattgagctcctgactggacagcccgccttccccgtctccgaggagacccaactccggttccgggcggcgcaccctgaggaggacgccacagtgggggcgatggaggcccgggacatcaagcaccggtgctct \n",
      "\n",
      "alignment: HIE_SL+_TRINITY_DN44987_c1_g4_i1: and his all fine exon sequence: \n",
      " gaagcggcaggcgctgctcggcctgaagcgggcgttgacggaccgcatggcccgcacgcaggccgccgtggcgatggcgaagagcgcagaggaggacgacgaggaggaggaggcggacgagtcgccgcgcagcccgccgagccc \n",
      "\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " gggcgctgttctgtgccacacccataataccaatggcggaggaggacgccccggacgagaacacgatgatctactggacgggcttccagggggacgaggaggacgacctcggggagcggtgcgagcagcgcgagcggatgttcttctcccgcgactactgggcccgccc\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " caacccattccatctcgggcctcggcggaaaaatggcgatttcctcggcgtcaaagtcgtgaagaaccgcttctttgtgccgtacctctatggtggccttgaggaac\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " gggtccaccacgcccggggcatgacccacgaccacctcgtgccgtatgtgggctgctattgtgaccacagccgagaggaaacactgctatggcagcaccatgtacagggggc\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " ggctgtacagcgtgctgaagggcctgcagcacctccaccagcacggctgcgtgctcggcaagctgcgcgcctccaaggtggtcctcgaccgggacggtgttccggtactggtggactgggcctgcccgccgaacctgttcaagtccatctacac\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " aattgagctcctgactggacagcccgccttccccgtctccgaggagacccaactccggttccgggcggcgcaccctgaggaggacgccacagtgggggcgatggaggcccgggacatcaagcaccggtgctct\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " gaagcggcaggcgctgctcggcctgaagcgggcgttgacggaccgcatggcccgcacgcaggccgccgtggcgatggcgaagagcgcagaggaggacgacgaggaggaggaggcggacgagtcgccgcgcagcccgccgagccc\n",
      "\n",
      " \n",
      " \n",
      " WIELKIE PODSUMOWANIE: \n",
      "\n",
      "\n",
      "\n",
      "poczatki dla fine: dict_keys([157, 957, 1237, 2285, 3167, 5809]), \n",
      " konce: dict_values([324, 1062, 1347, 2437, 3298, 5951])\n",
      "seqid: \n",
      " ['HIE_SL+_TRINITY_DN44987_c1_g4_i1', 'HIE_SL+_TRINITY_DN44987_c1_g4_i1', 'HIE_SL+_TRINITY_DN44987_c1_g4_i1', 'HIE_SL+_TRINITY_DN44987_c1_g4_i1', 'HIE_SL+_TRINITY_DN44987_c1_g4_i1', 'HIE_SL+_TRINITY_DN44987_c1_g4_i1'], \n",
      " source \n",
      " ['TRINITY', 'TRINITY', 'TRINITY', 'TRINITY', 'TRINITY', 'TRINITY'],  \n",
      " typ: ['exon', 'exon', 'exon', 'exon', 'exon', 'exon'],  \n",
      " score_list: [100.0, 100.0, 100.0, 100.0, 100.0, 100.0], \n",
      " strand: ['+', '+', '+', '+', '+', '+'], \n",
      " phase: ['.', '.', '.', '.', '.', '.'], \n",
      " attributes: ['HIE_SL+_TRINITY_DN44987_c1_g4_i1', 'HIE_SL+_TRINITY_DN44987_c1_g4_i1', 'HIE_SL+_TRINITY_DN44987_c1_g4_i1', 'HIE_SL+_TRINITY_DN44987_c1_g4_i1', 'HIE_SL+_TRINITY_DN44987_c1_g4_i1', 'HIE_SL+_TRINITY_DN44987_c1_g4_i1'], \n",
      " start: [157, 957, 1237, 2285, 3167, 5809], \n",
      " end: [324, 1062, 1347, 2437, 3298, 5951]\n",
      "{5: 15, 35: 156, 325: 956, 1063: 1236, 1348: 1656, 1718: 2284, 2438: 2844, 2934: 3166, 3299: 5113, 5177: 5326, 5372: 5808}\n",
      "\n",
      " ################################################################################### \n",
      " running file: LON_OG0030307.fasta which is 3 of 9, what means 33.33% of advancement \n",
      " ################################################################################### \n",
      "\n",
      "{0: 25, 55: 81, 132: 173, 180: 223, 258: 334, 349: 374, 407: 431, 451: 470, 478: 575, 593: 671, 676: 695, 700: 769, 782: 785, 880: 919, 925: 966, 1005: 1023, 1028: 1044, 1081: 1215, 3658: 3724, 4039: 4291, 4444: 4461}\n",
      " fine_exons_range_dict: {1081: 1215, 4039: 4291}, \n",
      " too short exon range dict {0: 25, 55: 81, 349: 374, 407: 431, 451: 470, 676: 695, 782: 785, 1005: 1023, 1028: 1044, 4444: 4461}, \n",
      " too low homology exon range dict {132: 173, 180: 223, 258: 334, 478: 575, 593: 671, 700: 769, 880: 919, 925: 966, 3658: 3724}\n",
      "alignment: LON_SL+_GGOE01000328.1: and his all fine exon sequence: \n",
      " gattgaactgctgacgggacagcccgcattcccggcctcggaggacacccaatttcggttccgagcgaccgggggggtggacgacatcgaccaagatggggggatggaggccagagagatcaagaatcgctgcttt \n",
      "\n",
      "alignment: LON_SL+_GGOE01000328.1: and his all fine exon sequence: \n",
      " cccacagctgttggctgaccccttcttctcggacccggcggtgaagaggcaggcgctgctgggaatgaagcgggcgttgacagaccgcatggcgcgcacccaagctgcagtggcgatggcaaagggtggggaggacgagagtgaggaagacggggaggaagaggagaatggcgatgaggagaaggcgtcgcccacaccgccacccgcccctgcatcccgcactggatgcacaaagcacacctgacctgcccccc \n",
      "\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " gattgaactgctgacgggacagcccgcattcccggcctcggaggacacccaatttcggttccgagcgaccgggggggtggacgacatcgaccaagatggggggatggaggccagagagatcaagaatcgctgcttt\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " cccacagctgttggctgaccccttcttctcggacccggcggtgaagaggcaggcgctgctgggaatgaagcgggcgttgacagaccgcatggcgcgcacccaagctgcagtggcgatggcaaagggtggggaggacgagagtgaggaagacggggaggaagaggagaatggcgatgaggagaaggcgtcgcccacaccgccacccgcccctgcatcccgcactggatgcacaaagcacacctgacctgcccccc\n",
      "\n",
      " \n",
      " \n",
      " WIELKIE PODSUMOWANIE: \n",
      "\n",
      "\n",
      "\n",
      "poczatki dla fine: dict_keys([1081, 4039]), \n",
      " konce: dict_values([1215, 4291])\n",
      "seqid: \n",
      " ['LON_SL+_GGOE01000328.1', 'LON_SL+_GGOE01000328.1'], \n",
      " source \n",
      " [None, None],  \n",
      " typ: ['exon', 'exon'],  \n",
      " score_list: [100.0, 100.0], \n",
      " strand: ['+', '+'], \n",
      " phase: ['.', '.'], \n",
      " attributes: ['LON_SL+_GGOE01000328.1', 'LON_SL+_GGOE01000328.1'], \n",
      " start: [1081, 4039], \n",
      " end: [1215, 4291]\n",
      "{26: 54, 82: 131, 174: 179, 224: 257, 335: 348, 375: 406, 432: 450, 471: 477, 576: 592, 672: 675, 696: 699, 770: 781, 786: 879, 920: 924, 967: 1004, 1024: 1027, 1045: 1080, 1216: 3657, 3725: 4038, 4292: 4443}\n",
      "\n",
      " ################################################################################### \n",
      " running file: GRA_OG0030307.fasta which is 4 of 9, what means 44.44% of advancement \n",
      " ################################################################################### \n",
      "\n",
      "{0: 9, 36: 80, 111: 116, 148: 153, 176: 216, 222: 232, 237: 253, 267: 285, 305: 315, 320: 330, 354: 361, 477: 579, 657: 733, 822: 832, 909: 927, 1074: 1104, 1188: 1212, 1269: 1291, 1437: 1458, 1464: 1502, 1549: 1563, 1750: 1765, 2301: 2315, 2361: 2455, 2559: 2702, 2942: 3012, 3413: 3601}\n",
      " fine_exons_range_dict: {2559: 2702, 3413: 3601}, \n",
      " too short exon range dict {0: 9, 111: 116, 148: 153, 222: 232, 237: 253, 267: 285, 305: 315, 320: 330, 354: 361, 822: 832, 909: 927, 1188: 1212, 1269: 1291, 1437: 1458, 1549: 1563, 1750: 1765, 2301: 2315}, \n",
      " too low homology exon range dict {36: 80, 176: 216, 477: 579, 657: 733, 1074: 1104, 1464: 1502, 2361: 2455, 2942: 3012}\n",
      "alignment: GRA_SL+_TRINITY_DN42854_c0_g2_i2: and his all fine exon sequence: \n",
      " gatcgagctgctgaccggccagccggcgttccccgtctcggaggacacccagcggcagttccgggcggcggaccacgaggaggaggggccaccgggtgtccctgcgggggccgtggaggcgcgggacatcaagaaccgctgcttt \n",
      "\n",
      "alignment: GRA_SL+_TRINITY_DN42854_c0_g2_i2: and his all fine exon sequence: \n",
      " cagctgcaggcggacccgttcttcgcggacgtggccgtgaagcgccaggcgctgctcgggatgaagaaggcgctgacggaccgtatggcccgcacgcaggcggcggtggcgatggcgaagagtgcggaggagagcgagggggacgaggacgaggacgccgaggccccccccagcccgcccagccccccgg \n",
      "\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " gatcgagctgctgaccggccagccggcgttccccgtctcggaggacacccagcggcagttccgggcggcggaccacgaggaggaggggccaccgggtgtccctgcgggggccgtggaggcgcgggacatcaagaaccgctgcttt\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " cagctgcaggcggacccgttcttcgcggacgtggccgtgaagcgccaggcgctgctcgggatgaagaaggcgctgacggaccgtatggcccgcacgcaggcggcggtggcgatggcgaagagtgcggaggagagcgagggggacgaggacgaggacgccgaggccccccccagcccgcccagccccccgg\n",
      "\n",
      " \n",
      " \n",
      " WIELKIE PODSUMOWANIE: \n",
      "\n",
      "\n",
      "\n",
      "poczatki dla fine: dict_keys([2559, 3413]), \n",
      " konce: dict_values([2702, 3601])\n",
      "seqid: \n",
      " ['GRA_SL+_TRINITY_DN42854_c0_g2_i2', 'GRA_SL+_TRINITY_DN42854_c0_g2_i2'], \n",
      " source \n",
      " ['TRINITY', 'TRINITY'],  \n",
      " typ: ['exon', 'exon'],  \n",
      " score_list: [100.0, 97.89], \n",
      " strand: ['+', '+'], \n",
      " phase: ['.', '.'], \n",
      " attributes: ['GRA_SL+_TRINITY_DN42854_c0_g2_i2', 'GRA_SL+_TRINITY_DN42854_c0_g2_i2'], \n",
      " start: [2559, 3413], \n",
      " end: [2702, 3601]\n",
      "{10: 35, 81: 110, 117: 147, 154: 175, 217: 221, 233: 236, 254: 266, 286: 304, 316: 319, 331: 353, 362: 476, 580: 656, 734: 821, 833: 908, 928: 1073, 1105: 1187, 1213: 1268, 1292: 1436, 1459: 1463, 1503: 1548, 1564: 1749, 1766: 2300, 2316: 2360, 2456: 2558, 2703: 2941, 3013: 3412}\n",
      "\n",
      " ################################################################################### \n",
      " running file: GRA_OG0030308.fasta which is 5 of 9, what means 55.56% of advancement \n",
      " ################################################################################### \n",
      "\n",
      "{0: 9, 174: 322, 519: 574, 625: 671, 1152: 1210, 1539: 1579, 2054: 2112, 2394: 2459, 4052: 4090, 4464: 4531, 5808: 5881, 6080: 6142, 6694: 6734, 6876: 6937, 7249: 7352, 8002: 8128, 8815: 8874, 9765: 9935, 10316: 10381, 10481: 10536, 11813: 11940, 13146: 13195, 13857: 13956, 14521: 14616, 15272: 15350, 16456: 16502, 16784: 16852, 17320: 17400, 17597: 17691, 18081: 18400}\n",
      " fine_exons_range_dict: {174: 322, 7249: 7352, 8002: 8128, 9765: 9935, 11813: 11940, 13857: 13956, 18081: 18400}, \n",
      " too short exon range dict {0: 9}, \n",
      " too low homology exon range dict {519: 574, 625: 671, 1152: 1210, 1539: 1579, 2054: 2112, 2394: 2459, 4052: 4090, 4464: 4531, 5808: 5881, 6080: 6142, 6694: 6734, 6876: 6937, 8815: 8874, 10316: 10381, 10481: 10536, 13146: 13195, 14521: 14616, 15272: 15350, 16456: 16502, 16784: 16852, 17320: 17400, 17597: 17691}\n",
      "alignment: GRA_SL+_TRINITY_DN44436_c0_g1_i1: and his all fine exon sequence: \n",
      " gtggtgcccatcggcaaggaccaggatggcgtagccggggatggccgaccgggccaagttccccaactacatcgaggacaccgcccgggcccagcggttcctccgcggtttcaaggacgcggccggcagacccaagtatttacagcaact \n",
      "\n",
      "alignment: GRA_SL+_TRINITY_DN44436_c0_g1_i1: and his all fine exon sequence: \n",
      " ggtgtatctccccgagtcggcccaggggttcaaggcactgatggccagcactgtaaccaacgttttcttctacgctctaaggatcgagcagaacaagaagtcata \n",
      "\n",
      "alignment: GRA_SL+_TRINITY_DN44436_c0_g1_i1: and his all fine exon sequence: \n",
      " tgcagagtacacggcgtcccggaagctgcaggaggaggtggaggcgctgtacgccagcggcgagcgggtgtacgaccgcctgagccagagcatcgcccccgagatctggggccactccgacataaaga \n",
      "\n",
      "alignment: GRA_SL+_TRINITY_DN44436_c0_g1_i1: and his all fine exon sequence: \n",
      " aggggacatcaacgtctgcctgatgggggaccccggcgtcgccaagagccaaatgctgaagtgggtggccacgctggcgccgcgcgcggtgtacacgacgggcaagggcagcagcggcgtcgggctgactgccgcggtgacaaaggaccagtacacgggggaggccctcctc \n",
      "\n",
      "alignment: GRA_SL+_TRINITY_DN44436_c0_g1_i1: and his all fine exon sequence: \n",
      " aggcgggtatcgttacgtctctaaatgcccgggtctcggttctagcggcgagcaacccgctgttcggttcatggaacaagcgaaagactgtgcaggagaacatcaacttgcccacctccttgctctccc \n",
      "\n",
      "alignment: GRA_SL+_TRINITY_DN44436_c0_g1_i1: and his all fine exon sequence: \n",
      " ggaccctggcgaactttgttgccaagaatcacctcatcacacaggactccgacttgcgcgccagccgggaggagcgccacatcgttgacatcaacctactg \n",
      "\n",
      "alignment: GRA_SL+_TRINITY_DN44436_c0_g1_i1: and his all fine exon sequence: \n",
      " ggtggcccacgactactccgaggtgcggttctacacctaaccgcccggggcggcaggtggtgctctctgtgcggacctccccatccccaacccactccgccatcagtcgccactctgccccgacactgcccgtgtctcccgcttcccccccccacccccctcacatggaggtcccacccacatcggccgcttctcagcggcttcccctccccttccaccaccgttcaaacctacaccctggctgccacactgcccggtggaaatcgactctgaatggcttgccttcgaaccctgcggcagccagcagaacaaagcagagat \n",
      "\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " gtggtgcccatcggcaaggaccaggatggcgtagccggggatggccgaccgggccaagttccccaactacatcgaggacaccgcccgggcccagcggttcctccgcggtttcaaggacgcggccggcagacccaagtatttacagcaact\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " ggtgtatctccccgagtcggcccaggggttcaaggcactgatggccagcactgtaaccaacgttttcttctacgctctaaggatcgagcagaacaagaagtcata\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " tgcagagtacacggcgtcccggaagctgcaggaggaggtggaggcgctgtacgccagcggcgagcgggtgtacgaccgcctgagccagagcatcgcccccgagatctggggccactccgacataaaga\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " aggggacatcaacgtctgcctgatgggggaccccggcgtcgccaagagccaaatgctgaagtgggtggccacgctggcgccgcgcgcggtgtacacgacgggcaagggcagcagcggcgtcgggctgactgccgcggtgacaaaggaccagtacacgggggaggccctcctc\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " aggcgggtatcgttacgtctctaaatgcccgggtctcggttctagcggcgagcaacccgctgttcggttcatggaacaagcgaaagactgtgcaggagaacatcaacttgcccacctccttgctctccc\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " ggaccctggcgaactttgttgccaagaatcacctcatcacacaggactccgacttgcgcgccagccgggaggagcgccacatcgttgacatcaacctactg\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " ggtggcccacgactactccgaggtgcggttctacacctaaccgcccggggcggcaggtggtgctctctgtgcggacctccccatccccaacccactccgccatcagtcgccactctgccccgacactgcccgtgtctcccgcttcccccccccacccccctcacatggaggtcccacccacatcggccgcttctcagcggcttcccctccccttccaccaccgttcaaacctacaccctggctgccacactgcccggtggaaatcgactctgaatggcttgccttcgaaccctgcggcagccagcagaacaaagcagagat\n",
      "\n",
      " \n",
      " \n",
      " WIELKIE PODSUMOWANIE: \n",
      "\n",
      "\n",
      "\n",
      "poczatki dla fine: dict_keys([174, 7249, 8002, 9765, 11813, 13857, 18081]), \n",
      " konce: dict_values([322, 7352, 8128, 9935, 11940, 13956, 18400])\n",
      "seqid: \n",
      " ['GRA_SL+_TRINITY_DN44436_c0_g1_i1', 'GRA_SL+_TRINITY_DN44436_c0_g1_i1', 'GRA_SL+_TRINITY_DN44436_c0_g1_i1', 'GRA_SL+_TRINITY_DN44436_c0_g1_i1', 'GRA_SL+_TRINITY_DN44436_c0_g1_i1', 'GRA_SL+_TRINITY_DN44436_c0_g1_i1', 'GRA_SL+_TRINITY_DN44436_c0_g1_i1'], \n",
      " source \n",
      " ['TRINITY', 'TRINITY', 'TRINITY', 'TRINITY', 'TRINITY', 'TRINITY', 'TRINITY'],  \n",
      " typ: ['exon', 'exon', 'exon', 'exon', 'exon', 'exon', 'exon'],  \n",
      " score_list: [100.0, 99.05, 98.44, 100.0, 100.0, 99.01, 99.38], \n",
      " strand: ['+', '+', '+', '+', '+', '+', '+'], \n",
      " phase: ['.', '.', '.', '.', '.', '.', '.'], \n",
      " attributes: ['GRA_SL+_TRINITY_DN44436_c0_g1_i1', 'GRA_SL+_TRINITY_DN44436_c0_g1_i1', 'GRA_SL+_TRINITY_DN44436_c0_g1_i1', 'GRA_SL+_TRINITY_DN44436_c0_g1_i1', 'GRA_SL+_TRINITY_DN44436_c0_g1_i1', 'GRA_SL+_TRINITY_DN44436_c0_g1_i1', 'GRA_SL+_TRINITY_DN44436_c0_g1_i1'], \n",
      " start: [174, 7249, 8002, 9765, 11813, 13857, 18081], \n",
      " end: [322, 7352, 8128, 9935, 11940, 13956, 18400]\n",
      "{10: 173, 323: 518, 575: 624, 672: 1151, 1211: 1538, 1580: 2053, 2113: 2393, 2460: 4051, 4091: 4463, 4532: 5807, 5882: 6079, 6143: 6693, 6735: 6875, 6938: 7248, 7353: 8001, 8129: 8814, 8875: 9764, 9936: 10315, 10382: 10480, 10537: 11812, 11941: 13145, 13196: 13856, 13957: 14520, 14617: 15271, 15351: 16455, 16503: 16783, 16853: 17319, 17401: 17596, 17692: 18080}\n",
      "\n",
      " ################################################################################### \n",
      " running file: LON_OG0030308.fasta which is 6 of 9, what means 66.67% of advancement \n",
      " ################################################################################### \n",
      "\n",
      " VALUE ERROR: Given sequences: LON_SL_GGOE01000331.1 - len: 2414 and  LON_scaffold_2181 - len 21 from LON_OG0030308.fasta must have the same length.\n",
      "{0: 9, 174: 322, 519: 574, 625: 671, 1152: 1210, 1539: 1579, 2054: 2112, 2394: 2459, 4052: 4090, 4464: 4531, 5808: 5881, 6080: 6142, 6694: 6734, 6876: 6937, 7249: 7352, 8002: 8128, 8815: 8874, 9765: 9935, 10316: 10381, 10481: 10536, 11813: 11940, 13146: 13195, 13857: 13956, 14521: 14616, 15272: 15350, 16456: 16502, 16784: 16852, 17320: 17400, 17597: 17691, 18081: 18400}\n",
      " fine_exons_range_dict: {174: 322, 7249: 7352, 8002: 8128, 9765: 9935, 11813: 11940, 13857: 13956, 18081: 18400}, \n",
      " too short exon range dict {0: 9}, \n",
      " too low homology exon range dict {519: 574, 625: 671, 1152: 1210, 1539: 1579, 2054: 2112, 2394: 2459, 4052: 4090, 4464: 4531, 5808: 5881, 6080: 6142, 6694: 6734, 6876: 6937, 8815: 8874, 10316: 10381, 10481: 10536, 13146: 13195, 14521: 14616, 15272: 15350, 16456: 16502, 16784: 16852, 17320: 17400, 17597: 17691}\n",
      "alignment: LON_SL_GGOE01000331.1: and his all fine exon sequence: \n",
      " gtggtgcccatcggcaaggaccaggatggcgtagccggggatggccgaccgggccaagttccccaactacatcgaggacaccgcccgggcccagcggttcctccgcggtttcaaggacgcggccggcagacccaagtatttacagcaact \n",
      "\n",
      "alignment: LON_SL_GGOE01000331.1: and his all fine exon sequence: \n",
      " ggtgtatctccccgagtcggcccaggggttcaaggcactgatggccagcactgtaaccaacgttttcttctacgctctaaggatcgagcagaacaagaagtcata \n",
      "\n",
      "alignment: LON_SL_GGOE01000331.1: and his all fine exon sequence: \n",
      " tgcagagtacacggcgtcccggaagctgcaggaggaggtggaggcgctgtacgccagcggcgagcgggtgtacgaccgcctgagccagagcatcgcccccgagatctggggccactccgacataaaga \n",
      "\n",
      "alignment: LON_SL_GGOE01000331.1: and his all fine exon sequence: \n",
      " aggggacatcaacgtctgcctgatgggggaccccggcgtcgccaagagccaaatgctgaagtgggtggccacgctggcgccgcgcgcggtgtacacgacgggcaagggcagcagcggcgtcgggctgactgccgcggtgacaaaggaccagtacacgggggaggccctcctc \n",
      "\n",
      "alignment: LON_SL_GGOE01000331.1: and his all fine exon sequence: \n",
      " aggcgggtatcgttacgtctctaaatgcccgggtctcggttctagcggcgagcaacccgctgttcggttcatggaacaagcgaaagactgtgcaggagaacatcaacttgcccacctccttgctctccc \n",
      "\n",
      "alignment: LON_SL_GGOE01000331.1: and his all fine exon sequence: \n",
      " ggaccctggcgaactttgttgccaagaatcacctcatcacacaggactccgacttgcgcgccagccgggaggagcgccacatcgttgacatcaacctactg \n",
      "\n",
      "alignment: LON_SL_GGOE01000331.1: and his all fine exon sequence: \n",
      " ggtggcccacgactactccgaggtgcggttctacacctaaccgcccggggcggcaggtggtgctctctgtgcggacctccccatccccaacccactccgccatcagtcgccactctgccccgacactgcccgtgtctcccgcttcccccccccacccccctcacatggaggtcccacccacatcggccgcttctcagcggcttcccctccccttccaccaccgttcaaacctacaccctggctgccacactgcccggtggaaatcgactctgaatggcttgccttcgaaccctgcggcagccagcagaacaaagcagagat \n",
      "\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " gtggtgcccatcggcaaggaccaggatggcgtagccggggatggccgaccgggccaagttccccaactacatcgaggacaccgcccgggcccagcggttcctccgcggtttcaaggacgcggccggcagacccaagtatttacagcaact\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " ggtgtatctccccgagtcggcccaggggttcaaggcactgatggccagcactgtaaccaacgttttcttctacgctctaaggatcgagcagaacaagaagtcata\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " tgcagagtacacggcgtcccggaagctgcaggaggaggtggaggcgctgtacgccagcggcgagcgggtgtacgaccgcctgagccagagcatcgcccccgagatctggggccactccgacataaaga\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " aggggacatcaacgtctgcctgatgggggaccccggcgtcgccaagagccaaatgctgaagtgggtggccacgctggcgccgcgcgcggtgtacacgacgggcaagggcagcagcggcgtcgggctgactgccgcggtgacaaaggaccagtacacgggggaggccctcctc\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " aggcgggtatcgttacgtctctaaatgcccgggtctcggttctagcggcgagcaacccgctgttcggttcatggaacaagcgaaagactgtgcaggagaacatcaacttgcccacctccttgctctccc\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " ggaccctggcgaactttgttgccaagaatcacctcatcacacaggactccgacttgcgcgccagccgggaggagcgccacatcgttgacatcaacctactg\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " ggtggcccacgactactccgaggtgcggttctacacctaaccgcccggggcggcaggtggtgctctctgtgcggacctccccatccccaacccactccgccatcagtcgccactctgccccgacactgcccgtgtctcccgcttcccccccccacccccctcacatggaggtcccacccacatcggccgcttctcagcggcttcccctccccttccaccaccgttcaaacctacaccctggctgccacactgcccggtggaaatcgactctgaatggcttgccttcgaaccctgcggcagccagcagaacaaagcagagat\n",
      "\n",
      " \n",
      " \n",
      " WIELKIE PODSUMOWANIE: \n",
      "\n",
      "\n",
      "\n",
      "poczatki dla fine: dict_keys([174, 7249, 8002, 9765, 11813, 13857, 18081]), \n",
      " konce: dict_values([322, 7352, 8128, 9935, 11940, 13956, 18400])\n",
      "seqid: \n",
      " ['LON_SL_GGOE01000331.1', 'LON_SL_GGOE01000331.1', 'LON_SL_GGOE01000331.1', 'LON_SL_GGOE01000331.1', 'LON_SL_GGOE01000331.1', 'LON_SL_GGOE01000331.1', 'LON_SL_GGOE01000331.1'], \n",
      " source \n",
      " [None, None, None, None, None, None, None],  \n",
      " typ: ['exon', 'exon', 'exon', 'exon', 'exon', 'exon', 'exon'],  \n",
      " score_list: [100.0, 99.05, 98.44, 100.0, 100.0, 99.01, 99.38], \n",
      " strand: [None, None, None, None, None, None, None], \n",
      " phase: ['.', '.', '.', '.', '.', '.', '.'], \n",
      " attributes: ['LON_SL_GGOE01000331.1', 'LON_SL_GGOE01000331.1', 'LON_SL_GGOE01000331.1', 'LON_SL_GGOE01000331.1', 'LON_SL_GGOE01000331.1', 'LON_SL_GGOE01000331.1', 'LON_SL_GGOE01000331.1'], \n",
      " start: [174, 7249, 8002, 9765, 11813, 13857, 18081], \n",
      " end: [322, 7352, 8128, 9935, 11940, 13956, 18400]\n",
      "{10: 173, 323: 518, 575: 624, 672: 1151, 1211: 1538, 1580: 2053, 2113: 2393, 2460: 4051, 4091: 4463, 4532: 5807, 5882: 6079, 6143: 6693, 6735: 6875, 6938: 7248, 7353: 8001, 8129: 8814, 8875: 9764, 9936: 10315, 10382: 10480, 10537: 11812, 11941: 13145, 13196: 13856, 13957: 14520, 14617: 15271, 15351: 16455, 16503: 16783, 16853: 17319, 17401: 17596, 17692: 18080}\n",
      "\n",
      " ################################################################################### \n",
      " running file: HIE_OG0030309.fasta which is 7 of 9, what means 77.78% of advancement \n",
      " ################################################################################### \n",
      "\n",
      "{0: 27, 47: 58, 72: 241, 509: 525, 532: 620, 1449: 1508, 1746: 1922, 2267: 2419, 3045: 3127, 3442: 3506, 5728: 5866, 6414: 6504, 8109: 8204, 8433: 8507, 8809: 8854, 9607: 9816}\n",
      " fine_exons_range_dict: {72: 241, 1746: 1922, 2267: 2419, 5728: 5866, 8109: 8204, 9607: 9816}, \n",
      " too short exon range dict {0: 27, 47: 58, 509: 525}, \n",
      " too low homology exon range dict {532: 620, 1449: 1508, 3045: 3127, 3442: 3506, 6414: 6504, 8433: 8507, 8809: 8854}\n",
      "alignment: HIE_SL_TRINITY_DN48344_c2_g1_i8: and his all fine exon sequence: \n",
      " tgggcatcgatgcggcaagcccgtaggtttgtgctgccgggctggcgtttcatcgccctgaggtttttctccactgatcccattgcggagcggcttgaccggctgcagagcaacaacgcattgttggcgcaggagatggcggcggtgcgggagcgggcggagtacttggac \n",
      "\n",
      "alignment: HIE_SL_TRINITY_DN48344_c2_g1_i8: and his all fine exon sequence: \n",
      " accggctgccccggggcctgggcggcctggcagcgttccagcaggtacgggagatgttcacggactcgttccgcgagctgcgggcgttccgcttctccaagcccgcccccagctgggaggacggccagcgcttccacgacatgctctccgtcatccagagccgccacacccaggtccc \n",
      "\n",
      "alignment: HIE_SL_TRINITY_DN48344_c2_g1_i8: and his all fine exon sequence: \n",
      " cgaggtcatcttccagggctgccagcagctccgcgttcagctgcagcgcacgcacgggccccactgggcggaccacccggaccggcacctgacgcaggacgtgctggaggatttcttcggggcccgggtcgtcgtccgcttcctgatcggccag \n",
      "\n",
      "alignment: HIE_SL_TRINITY_DN48344_c2_g1_i8: and his all fine exon sequence: \n",
      " gacatcgagctcacctgcaagtgtggcacgcccttcatccagatcgggtaccacatccagttcatcaccgtgcgcctgctgcgccacgccatcatgggcgtgctgaaggcccacggcgtggagaaggtcagggccggcca \n",
      "\n",
      "alignment: HIE_SL_TRINITY_DN48344_c2_g1_i8: and his all fine exon sequence: \n",
      " ccaagctgcgctacctgtggagctacctgcacgatgatgctcgcctcggggagcgggagtgggtggcggcatcccccgaggaccggaagtacggaga \n",
      "\n",
      "alignment: HIE_SL_TRINITY_DN48344_c2_g1_i8: and his all fine exon sequence: \n",
      " ggtgctctccaggcccccgttcccgaattgatgtgcgcttttgggcgcattcgcaccaagcattcacttgtacaatggtcatcccagatctatgtcctctcagcccaagatatggagtcacgtgtgcacaatctcccatctgctaatgcggttgggcgcaagtgtcttctatgtccctctttcacttgcaaatttcaacagtataaccagg \n",
      "\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " tgggcatcgatgcggcaagcccgtaggtttgtgctgccgggctggcgtttcatcgccctgaggtttttctccactgatcccattgcggagcggcttgaccggctgcagagcaacaacgcattgttggcgcaggagatggcggcggtgcgggagcgggcggagtacttggac\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " accggctgccccggggcctgggcggcctggcagcgttccagcaggtacgggagatgttcacggactcgttccgcgagctgcgggcgttccgcttctccaagcccgcccccagctgggaggacggccagcgcttccacgacatgctctccgtcatccagagccgccacacccaggtccc\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " cgaggtcatcttccagggctgccagcagctccgcgttcagctgcagcgcacgcacgggccccactgggcggaccacccggaccggcacctgacgcaggacgtgctggaggatttcttcggggcccgggtcgtcgtccgcttcctgatcggccag\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " gacatcgagctcacctgcaagtgtggcacgcccttcatccagatcgggtaccacatccagttcatcaccgtgcgcctgctgcgccacgccatcatgggcgtgctgaaggcccacggcgtggagaaggtcagggccggcca\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " ccaagctgcgctacctgtggagctacctgcacgatgatgctcgcctcggggagcgggagtgggtggcggcatcccccgaggaccggaagtacggaga\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " ggtgctctccaggcccccgttcccgaattgatgtgcgcttttgggcgcattcgcaccaagcattcacttgtacaatggtcatcccagatctatgtcctctcagcccaagatatggagtcacgtgtgcacaatctcccatctgctaatgcggttgggcgcaagtgtcttctatgtccctctttcacttgcaaatttcaacagtataaccagg\n",
      "\n",
      " \n",
      " \n",
      " WIELKIE PODSUMOWANIE: \n",
      "\n",
      "\n",
      "\n",
      "poczatki dla fine: dict_keys([72, 1746, 2267, 5728, 8109, 9607]), \n",
      " konce: dict_values([241, 1922, 2419, 5866, 8204, 9816])\n",
      "seqid: \n",
      " ['HIE_SL_TRINITY_DN48344_c2_g1_i8', 'HIE_SL_TRINITY_DN48344_c2_g1_i8', 'HIE_SL_TRINITY_DN48344_c2_g1_i8', 'HIE_SL_TRINITY_DN48344_c2_g1_i8', 'HIE_SL_TRINITY_DN48344_c2_g1_i8', 'HIE_SL_TRINITY_DN48344_c2_g1_i8'], \n",
      " source \n",
      " ['TRINITY', 'TRINITY', 'TRINITY', 'TRINITY', 'TRINITY', 'TRINITY'],  \n",
      " typ: ['exon', 'exon', 'exon', 'exon', 'exon', 'exon'],  \n",
      " score_list: [84.21, 100.0, 100.0, 100.0, 100.0, 99.05], \n",
      " strand: [None, None, None, None, None, None], \n",
      " phase: ['.', '.', '.', '.', '.', '.'], \n",
      " attributes: ['HIE_SL_TRINITY_DN48344_c2_g1_i8', 'HIE_SL_TRINITY_DN48344_c2_g1_i8', 'HIE_SL_TRINITY_DN48344_c2_g1_i8', 'HIE_SL_TRINITY_DN48344_c2_g1_i8', 'HIE_SL_TRINITY_DN48344_c2_g1_i8', 'HIE_SL_TRINITY_DN48344_c2_g1_i8'], \n",
      " start: [72, 1746, 2267, 5728, 8109, 9607], \n",
      " end: [241, 1922, 2419, 5866, 8204, 9816]\n",
      "{28: 46, 59: 71, 242: 508, 526: 531, 621: 1448, 1509: 1745, 1923: 2266, 2420: 3044, 3128: 3441, 3507: 5727, 5867: 6413, 6505: 8108, 8205: 8432, 8508: 8808, 8855: 9606}\n",
      "\n",
      " ################################################################################### \n",
      " running file: GRA_OG0030309.fasta which is 8 of 9, what means 88.89% of advancement \n",
      " ################################################################################### \n",
      "\n",
      "{6: 116, 132: 148, 158: 192, 206: 292, 426: 481, 1782: 1832, 4274: 4324, 4961: 5091, 5440: 5592, 5950: 6032, 7703: 7768, 8910: 9072, 9260: 9277, 9682: 9691, 9742: 9796, 9930: 9952, 10001: 10071, 10184: 10240, 10502: 10507, 10724: 10752, 10866: 10897, 11014: 11032, 11387: 11407, 11445: 11472, 11509: 11518}\n",
      " fine_exons_range_dict: {4961: 5091, 5440: 5592, 8910: 9072}, \n",
      " too short exon range dict {132: 148, 9260: 9277, 9682: 9691, 9930: 9952, 10502: 10507, 10724: 10752, 11014: 11032, 11387: 11407, 11445: 11472, 11509: 11518}, \n",
      " too low homology exon range dict {6: 116, 158: 192, 206: 292, 426: 481, 1782: 1832, 4274: 4324, 5950: 6032, 7703: 7768, 9742: 9796, 10001: 10071, 10184: 10240, 10866: 10897}\n",
      "alignment: GRA_SL+_TRINITY_DN42647_c0_g1_i3: and his all fine exon sequence: \n",
      " tgcgggagatgttcacggactccttccgggagctgcgcgcgttccagttctccaagccggccccgagctgggaggacggccagcggttccacgacatgctctccgtcatccagagccgccacaagcgggtcc \n",
      "\n",
      "alignment: GRA_SL+_TRINITY_DN42647_c0_g1_i3: and his all fine exon sequence: \n",
      " ctgagatcgtcttccagggctgccagcagctccgcacgcagctccagcgcacccacggggccaactgggccgaccaccccgaccgccaactcgcccaagatgtgttggaggacttcttcggggcccgtgtcgtggtgcgcttcctgatcggcca \n",
      "\n",
      "alignment: GRA_SL+_TRINITY_DN42647_c0_g1_i3: and his all fine exon sequence: \n",
      " gacattgagctcacatgcaagtgcgggacaccattcatccaaatcgggtaccacatccagttcatcaccgtccgcctcttgcgccactccatcatggggacgctgaaggcccacggcgtggagaaggtgaaggccgggaaagcccctccagtgacgatcacggt \n",
      "\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " tgcgggagatgttcacggactccttccgggagctgcgcgcgttccagttctccaagccggccccgagctgggaggacggccagcggttccacgacatgctctccgtcatccagagccgccacaagcgggtcc\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " ctgagatcgtcttccagggctgccagcagctccgcacgcagctccagcgcacccacggggccaactgggccgaccaccccgaccgccaactcgcccaagatgtgttggaggacttcttcggggcccgtgtcgtggtgcgcttcctgatcggcca\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " gacattgagctcacatgcaagtgcgggacaccattcatccaaatcgggtaccacatccagttcatcaccgtccgcctcttgcgccactccatcatggggacgctgaaggcccacggcgtggagaaggtgaaggccgggaaagcccctccagtgacgatcacggt\n",
      "\n",
      " \n",
      " \n",
      " WIELKIE PODSUMOWANIE: \n",
      "\n",
      "\n",
      "\n",
      "poczatki dla fine: dict_keys([4961, 5440, 8910]), \n",
      " konce: dict_values([5091, 5592, 9072])\n",
      "seqid: \n",
      " ['GRA_SL+_TRINITY_DN42647_c0_g1_i3', 'GRA_SL+_TRINITY_DN42647_c0_g1_i3', 'GRA_SL+_TRINITY_DN42647_c0_g1_i3'], \n",
      " source \n",
      " ['TRINITY', 'TRINITY', 'TRINITY'],  \n",
      " typ: ['exon', 'exon', 'exon'],  \n",
      " score_list: [99.24, 99.35, 93.29], \n",
      " strand: ['+', '+', '+'], \n",
      " phase: ['.', '.', '.'], \n",
      " attributes: ['GRA_SL+_TRINITY_DN42647_c0_g1_i3', 'GRA_SL+_TRINITY_DN42647_c0_g1_i3', 'GRA_SL+_TRINITY_DN42647_c0_g1_i3'], \n",
      " start: [4961, 5440, 8910], \n",
      " end: [5091, 5592, 9072]\n",
      "{117: 131, 149: 157, 193: 205, 293: 425, 482: 1781, 1833: 4273, 4325: 4960, 5092: 5439, 5593: 5949, 6033: 7702, 7769: 8909, 9073: 9259, 9278: 9681, 9692: 9741, 9797: 9929, 9953: 10000, 10072: 10183, 10241: 10501, 10508: 10723, 10753: 10865, 10898: 11013, 11033: 11386, 11408: 11444, 11473: 11508}\n",
      "\n",
      " ################################################################################### \n",
      " running file: HIE_OG0030308.fasta which is 9 of 9, what means 100.0% of advancement \n",
      " ################################################################################### \n",
      "\n",
      "{0: 10, 46: 191, 257: 313, 367: 413, 1620: 1698, 1731: 1750, 1841: 1849, 1951: 1981, 2020: 2059, 2107: 2150, 2249: 2251, 2263: 2300, 2655: 2666, 2682: 2692, 2802: 2837, 2875: 2886, 3056: 3067, 3250: 3294, 3380: 3388, 3562: 3595, 3731: 3734, 3796: 3842, 3914: 3928, 3998: 4027, 4060: 4105, 4117: 4125, 4137: 4147, 4198: 4212, 4260: 4285, 4511: 4526, 4532: 4542, 4779: 4788, 4969: 4985, 5090: 5103, 5149: 5186, 5213: 5239, 5277: 5281, 5323: 5349, 5359: 5396, 5429: 5438, 5504: 5524, 5591: 5615, 5666: 5692, 5753: 5765, 5772: 5778, 5899: 5906, 5929: 5937, 5976: 5992, 6049: 6065, 6093: 6108, 6141: 6196, 6208: 6237, 6340: 6370, 6402: 6435, 6590: 6616, 6621: 6656, 6689: 6708, 6713: 6731, 6806: 6821, 6900: 6949, 6986: 7000, 7014: 7023, 7242: 7273, 7281: 7292, 7314: 7321, 7361: 7380, 7513: 7534, 7820: 7849, 8004: 8046, 8228: 8261, 8502: 8510, 8581: 8598, 8669: 8687, 9110: 9117, 9171: 9188, 9536: 9546, 9867: 9902, 10017: 10026, 10080: 10095, 10180: 10188, 10194: 10202, 10214: 10256, 10293: 10314, 10366: 10419, 10484: 10513, 10552: 10559, 10611: 10625, 10639: 10656, 10832: 10858, 10867: 10888, 11033: 11048, 11121: 11171, 11180: 11190, 11280: 11292, 11419: 11435, 11506: 11532, 11674: 11692, 11834: 11843, 12021: 12043, 12248: 12254}\n",
      " fine_exons_range_dict: {46: 191}, \n",
      " too short exon range dict {0: 10, 1731: 1750, 1841: 1849, 2249: 2251, 2655: 2666, 2682: 2692, 2875: 2886, 3056: 3067, 3380: 3388, 3731: 3734, 3914: 3928, 4117: 4125, 4137: 4147, 4198: 4212, 4260: 4285, 4511: 4526, 4532: 4542, 4779: 4788, 4969: 4985, 5090: 5103, 5213: 5239, 5277: 5281, 5323: 5349, 5429: 5438, 5504: 5524, 5591: 5615, 5666: 5692, 5753: 5765, 5772: 5778, 5899: 5906, 5929: 5937, 5976: 5992, 6049: 6065, 6093: 6108, 6590: 6616, 6689: 6708, 6713: 6731, 6806: 6821, 6986: 7000, 7014: 7023, 7281: 7292, 7314: 7321, 7361: 7380, 7513: 7534, 8502: 8510, 8581: 8598, 8669: 8687, 9110: 9117, 9171: 9188, 9536: 9546, 10017: 10026, 10080: 10095, 10180: 10188, 10194: 10202, 10293: 10314, 10552: 10559, 10611: 10625, 10639: 10656, 10832: 10858, 10867: 10888, 11033: 11048, 11180: 11190, 11280: 11292, 11419: 11435, 11506: 11532, 11674: 11692, 11834: 11843, 12021: 12043, 12248: 12254}, \n",
      " too low homology exon range dict {257: 313, 367: 413, 1620: 1698, 1951: 1981, 2020: 2059, 2107: 2150, 2263: 2300, 2802: 2837, 3250: 3294, 3562: 3595, 3796: 3842, 3998: 4027, 4060: 4105, 5149: 5186, 5359: 5396, 6141: 6196, 6208: 6237, 6340: 6370, 6402: 6435, 6621: 6656, 6900: 6949, 7242: 7273, 7820: 7849, 8004: 8046, 8228: 8261, 9867: 9902, 10214: 10256, 10366: 10419, 10484: 10513, 11121: 11171}\n",
      "alignment: HIE_SL+_TRINITY_DN47597_c0_g1_i1: and his all fine exon sequence: \n",
      " cgaacttcatgcgaagaagcataatgtgctaacaaggatggccgaacgaggcaagttcccaaattacatcgaggacacagcacgcgcacaacgtttcattaggagcttcaaagatgcttctggaattccgaaatatctgaggcaatt \n",
      "\n",
      "\n",
      " \n",
      " teraz analizujemy takie sekwencje: \n",
      " cgaacttcatgcgaagaagcataatgtgctaacaaggatggccgaacgaggcaagttcccaaattacatcgaggacacagcacgcgcacaacgtttcattaggagcttcaaagatgcttctggaattccgaaatatctgaggcaatt\n",
      "\n",
      " \n",
      " \n",
      " WIELKIE PODSUMOWANIE: \n",
      "\n",
      "\n",
      "\n",
      "poczatki dla fine: dict_keys([46]), \n",
      " konce: dict_values([191])\n",
      "seqid: \n",
      " ['HIE_SL+_TRINITY_DN47597_c0_g1_i1'], \n",
      " source \n",
      " ['TRINITY'],  \n",
      " typ: ['exon'],  \n",
      " score_list: [99.32], \n",
      " strand: ['+'], \n",
      " phase: ['.'], \n",
      " attributes: ['HIE_SL+_TRINITY_DN47597_c0_g1_i1'], \n",
      " start: [46], \n",
      " end: [191]\n",
      "{11: 45, 192: 256, 314: 366, 414: 1619, 1699: 1730, 1751: 1840, 1850: 1950, 1982: 2019, 2060: 2106, 2151: 2248, 2252: 2262, 2301: 2654, 2667: 2681, 2693: 2801, 2838: 2874, 2887: 3055, 3068: 3249, 3295: 3379, 3389: 3561, 3596: 3730, 3735: 3795, 3843: 3913, 3929: 3997, 4028: 4059, 4106: 4116, 4126: 4136, 4148: 4197, 4213: 4259, 4286: 4510, 4527: 4531, 4543: 4778, 4789: 4968, 4986: 5089, 5104: 5148, 5187: 5212, 5240: 5276, 5282: 5322, 5350: 5358, 5397: 5428, 5439: 5503, 5525: 5590, 5616: 5665, 5693: 5752, 5766: 5771, 5779: 5898, 5907: 5928, 5938: 5975, 5993: 6048, 6066: 6092, 6109: 6140, 6197: 6207, 6238: 6339, 6371: 6401, 6436: 6589, 6617: 6620, 6657: 6688, 6709: 6712, 6732: 6805, 6822: 6899, 6950: 6985, 7001: 7013, 7024: 7241, 7274: 7280, 7293: 7313, 7322: 7360, 7381: 7512, 7535: 7819, 7850: 8003, 8047: 8227, 8262: 8501, 8511: 8580, 8599: 8668, 8688: 9109, 9118: 9170, 9189: 9535, 9547: 9866, 9903: 10016, 10027: 10079, 10096: 10179, 10189: 10193, 10203: 10213, 10257: 10292, 10315: 10365, 10420: 10483, 10514: 10551, 10560: 10610, 10626: 10638, 10657: 10831, 10859: 10866, 10889: 11032, 11049: 11120, 11172: 11179, 11191: 11279, 11293: 11418, 11436: 11505, 11533: 11673, 11693: 11833, 11844: 12020, 12044: 12247}\n"
     ]
    }
   ],
   "source": [
    "def cutting_scrap(path_to_file_before_MAFFT, path_to_file_after_MAFFT, acceptable_gap_length):\n",
    "    data_frames = []\n",
    "    gff_data_frames = []\n",
    "    gaps_signs = \"-\" * acceptable_gap_length #maximum length of gaps in sequence in one exon's sequence\n",
    "    files_in_progress = 0\n",
    "    print(f\"\\n \\n \\n \\n Just run function: cutting_scrap\")\n",
    "\n",
    "    for filename in os.listdir(path_to_file_after_MAFFT):\n",
    "        #print(f\"\\n Just tried make 1st turn - find all filenames, \\n there is filename: {str(filename)}\")\n",
    "        file = os.path.join(path_to_file_after_MAFFT, filename)\n",
    "        if not os.path.isfile(file):\n",
    "            continue\n",
    "        if file.endswith(\".fasta\"):\n",
    "            files_in_progress += 1\n",
    "            count_files = percentage_of_advancement(path_to_file_before_MAFFT)\n",
    "            print(f\"\\n ################################################################################### \\n running file: {filename} which is {files_in_progress} of {count_files}, what means {round(files_in_progress/count_files*100, 2)}% of advancement \\n ################################################################################### \\n\")\n",
    "            alignment = AlignIO.read(file, \"fasta\")\n",
    "            alignment_id = alignment[0].id\n",
    "            alignment_DIDNT_TOUCHED = AlignIO.read(file, \"fasta\")\n",
    "            #print(f\"NA POCZATKU alignment: {alignment[0].id} ma dlugosc {len(alignment[0])}\")\n",
    "\n",
    "        count_of_errors = 0\n",
    "\n",
    "\n",
    "        #Pawel mowil o mozliwosci stworzenia slownika w ktorym bylo przypisanie \n",
    "        #sekwencja_z_pliku_przed_mafftem do sekwencja_z_pliku_po_maffcie. \n",
    "        #Ja nie zrobilem slownika, a wydaje mi sie jeszcze krotsza droga:\n",
    "        file_before_MAFFT = os.path.join(path_to_file_before_MAFFT, filename)\n",
    "        if not os.path.isfile(file_before_MAFFT):\n",
    "            continue\n",
    "\n",
    "        \n",
    "        seq1_DT, seq2_DT = extracting_strands_from_alignment(alignment_DIDNT_TOUCHED) #type is Bio.Seq.Seq but works like a string\n",
    "        #dzialamy teraz nie na iterowanym alignmencie tylko na sekwencji ktora zachowuje sie jak string\n",
    "        \n",
    "        nucleotides = [\"a\", \"t\", \"g\", \"c\", \"A\", \"T\", \"G\", \"C\"]\n",
    "        index = 0\n",
    "\n",
    "        #EMPTY LIST TSV\n",
    "        id = []\n",
    "        exon_or_intron_number = []\n",
    "        is_it_intron = []\n",
    "        class_of_exon = []\n",
    "        percent_of_homology_for_exons = []\n",
    "        length = []\n",
    "        first_nt_position = []\n",
    "        last_nt_position = []\n",
    "        first_10_nt = []\n",
    "        last_10_nt = []\n",
    "        path = []\n",
    "        sequence = []\n",
    "\n",
    "        #EMPTY LIST GFF\n",
    "        seqid = [] #name of the chromosome or scaffold; chromosome names can be given with or without the 'chr' prefix. Important note: the seq ID must be one used within Ensembl, i.e. a standard chromosome name or an Ensembl identifier such as a scaffold ID, without any additional content such as species or assembly. See the example GFF output below.\n",
    "        source = [] #tutaj pominiemy ten krok - damy kropke #Describes the algorithm or the procedure that generated this feature. Typically Genescane or Genebank, respectively. #scaffold backbone trinity  i te numerki jakieś\n",
    "        typ = [] #Describes what the feature is (mRNA, domain, exon, etc.). #scaffold backbone trinity  i te numerki jakieś #intron lub exon\n",
    "        start = []\n",
    "        end = []\n",
    "        score_list = [] #Typically E-values for sequence similarity and P-values for predictions. Homology there\n",
    "        strand = []\n",
    "        phase = [] #Indicates where the feature begins with reference to the reading frame. The phase is one of the integers 0, 1, or 2, indicating the number of bases that should be removed from the beginning of this feature to reach the first base of the next codon.. Propably 0\n",
    "        attributes = [] #A semicolon-separated list of tag-value pairs, providing additional information about each feature. Some of these tags are predefined, e.g. ID, Name, Alias, Parent . You can see the full list [here](https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md).\n",
    "\n",
    "        try:\n",
    "            seq1, seq2, count_of_errors = cleaning_gaps_from_both_edges(seq1_DT, seq2_DT, count_of_errors, alignment, filename) #deleting gaps from 5' and 3'. It is important. The next function start counting from possible exon.\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            count_of_errors += 1\n",
    "\n",
    "\n",
    "        #linking pre-exon's nucleotides (nt-nt pairs) and pre-intron's nucleotides (gap-nt pairs) to theirs indices \n",
    "        temp_intron, temp_exon, count_of_errors = indices_of_introns_and_exons_fun(seq1_DT, seq2_DT, seq1, seq2, count_of_errors)\n",
    "        # print(f\"temp_introns indices: {temp_intron}\")\n",
    "        # print(f\"temp_exon indices: {temp_exon}\")\n",
    "\n",
    "\n",
    "        #linking indices into single strand of possible exons\n",
    "        all_exon_range_dict, count_of_errors = start_and_end_parameters_of_exons_dict_fun(temp_exon, gaps_signs, count_of_errors)\n",
    "        # for key in all_exon_range_dict: #shows exon's sequences according to exon range dict\n",
    "        #     print(f\"Dlugosc eksonu: {all_exon_range_dict[key] - key} ekson: {seq1[key:all_exon_range_dict[key]]}\")\n",
    "        print(all_exon_range_dict)\n",
    "        #linking indices into single strand of possible introns\n",
    "        intron_range_dict, count_of_errors  = start_and_end_parameters_of_introns_dict_fun(all_exon_range_dict, count_of_errors)\n",
    "        # for key in intron_range_dict: #shows introns's sequences according to exon range dict\n",
    "        #     print(f\"Koniec intronu {intron_range_dict[key]} - poczatek intronu{key} = dlugosc intronu {intron_range_dict[key] - key} lista intronow: {seq1[key:intron_range_dict[key]]} \\n sekwencja genomowa odpowiadajaca pozycjami wobec intronu:  {seq2[key:intron_range_dict[key]]} \\n\")\n",
    "\n",
    "\n",
    "        #sorting exons to propoer categories\n",
    "        fine_exons, TLH_exons, TS_exons, TLH_percentage_of_identity, fine_percentage_of_identity, fine_exons_range_dict, TS_exon_range_dict, TLH_exon_range_dict = making_and_sorting_alignments_fun(all_exon_range_dict, min_length_aligned_sequence, seq1, seq2)\n",
    "        #print(f\"\\n all fine exons = {fine_exons}, \\n too low homology = {TLH_exons}, \\n too short exons = {TS_exons}\")\n",
    "        print(f\" fine_exons_range_dict: {fine_exons_range_dict}, \\n too short exon range dict {TS_exon_range_dict}, \\n too low homology exon range dict {TLH_exon_range_dict}\")\n",
    "\n",
    "\n",
    "        for element in range(len(fine_exons)):\n",
    "            print(f\"alignment: {alignment[0].id}: and his all fine exon sequence: \\n {fine_exons[element]} \\n\")\n",
    "            #printing sequence of fine exons\n",
    "\n",
    "        #gff table for fine exons\n",
    "        seqid, source, typ, score_list, start, end, strand, phase, attributes = making_exon_gff_table(fine_exons, seqid, source, typ, score_list, start, end, strand, phase, attributes, alignment_id, fine_percentage_of_identity, fine_exons_range_dict)\n",
    "\n",
    "        #for introns we made other functions with similiar output. Then two tables would be merged and sorted by increasingvalues\n",
    "\n",
    "        ####################################################################################################################\n",
    "        ####################################################################################################################\n",
    "        ####################################################################################################################\n",
    "        ####################################################################################################################\n",
    "        ####################################################################################################################\n",
    "\n",
    "        \n",
    "        print('\\n \\n \\n WIELKIE PODSUMOWANIE: \\n\\n\\n')\n",
    "        print(f\"poczatki dla fine: {fine_exons_range_dict.keys()}, \\n konce: {fine_exons_range_dict.values()}\")\n",
    "        print(f\"seqid: \\n {seqid}, \\n source \\n {source},  \\n typ: {typ},  \\n score_list: {score_list}, \\n strand: {strand}, \\n phase: {phase}, \\n attributes: {attributes}, \\n start: {start}, \\n end: {end}\")\n",
    "\n",
    "        print(intron_range_dict)\n",
    "            \n",
    "\n",
    "#############################################################################################################################\n",
    "#######################################                     FUNKCJE                   #######################################       \n",
    "#############################################################################################################################\n",
    "\n",
    "def percentage_of_advancement(directory):\n",
    "    count = 0\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".fasta\"):\n",
    "            count += 1\n",
    "    return(count)\n",
    "    \n",
    "\n",
    "def cleaning_gaps_from_both_edges(sequence_transcipt, sequence_genome, count_of_errors, alignment, filename):\n",
    "    sequence_transcipt_left_shorted = sequence_transcipt.lstrip(\"-\")\n",
    "    count_deleted_gaps_left = len(sequence_transcipt) - len(sequence_transcipt_left_shorted) #inform how many gaps were deleted from 5'\n",
    "    \n",
    "    sequence_transcipt_right_and_left_shorted = sequence_transcipt_left_shorted.rstrip(\"-\")\n",
    "    count_deleted_gaps_right = len(sequence_transcipt_left_shorted) - len(sequence_transcipt_right_and_left_shorted) #inform how many gaps were deleted from 3'\n",
    "    \n",
    "    sequence_genome_left_and_right_shorted = sequence_genome[count_deleted_gaps_left:-count_deleted_gaps_right]\n",
    "    if len(sequence_genome_left_and_right_shorted) != len(sequence_transcipt_right_and_left_shorted):\n",
    "        raise ValueError(f\" VALUE ERROR: Given sequences: {alignment[0].id} - len: {len(alignment[0])} and  {alignment[1].id} - len {len(alignment[0].id)} from {filename} must have the same length.\")\n",
    "    return sequence_transcipt_right_and_left_shorted, sequence_genome_left_and_right_shorted, count_of_errors\n",
    "\n",
    "\n",
    "def indices_of_introns_and_exons_fun(seq1_DT, seq2_DT, seq1, seq2, count_of_errors):\n",
    "    #linking pre-exon's nucleotides (nt-nt pairs) and pre-intron's nucleotides (gap-nt pairs) to theirs indices \n",
    "    #NOTE: output is nucleotide or gap from seq1 (transcript sequence). Even if it is gap like this: \n",
    "    #seq1: aaatttggg, seq2: aaa---ggg, output will be 'ttt' instead '---'\n",
    "    #it shows that where were gap, seq2 genome or in seq1 transcript\n",
    "    temp_exon = [] #list containing indices of intron's positions in sequence.\n",
    "    temp_intron = [] #list containing indices of intron's positions in sequence.\n",
    "    index = 0\n",
    "    paired_nucleotides = zip(seq1, seq2)\n",
    "    \n",
    "    if len(seq1_DT) != len(seq2_DT):\n",
    "        raise ValueError(\"The two sequences must be of the same length.\")\n",
    "        #return count_of_errors\n",
    "    for nt1, nt2 in paired_nucleotides:\n",
    "        if \"-\" in (nt1, nt2):\n",
    "            temp_intron.append(index)\n",
    "        else:\n",
    "            temp_exon.append(index)\n",
    "        index += 1 #indeks do wskazywania pozycji w sekwencji \n",
    "    return temp_intron, temp_exon, count_of_errors\n",
    "\n",
    "\n",
    "def start_and_end_parameters_of_exons_dict_fun(temp_exon, gaps_signs, count_of_errors):\n",
    "    #linking indices into single strand of possible exons\n",
    "    start_exon = temp_exon[0]\n",
    "    all_exon_range_dict = {} #that dictionary contains: key(index of start exon) and value(index of end exon)\n",
    "    \n",
    "    for i in range(len(temp_exon)): \n",
    "        #print(start_exon)\n",
    "        if (temp_exon[i] - temp_exon[i-1]) > 1+len(gaps_signs): #if difference between two indices of exons's positions is bigger than given number (2), that smaller number is index of 3' nucleotide in exon \n",
    "            end_exon = temp_exon[i-1] #temp_exon[i-1] because it is index in list temp_exon. +1 because python starts counting from 0\n",
    "            all_exon_range_dict[start_exon] = end_exon #creating dictionary with all exons, even with theese too short and with too low homology\n",
    "            start_exon = temp_exon[i] + 1\n",
    "            #brakuje ostatniego eksonu. Jest to spowodowane tym, ze nie mozna okreslic parametru end_exon, gdyz nie w liscie temp_exon wiekszego numeru niz ten ostatni\n",
    "    all_exon_range_dict[start_exon] = temp_exon[-1]\n",
    "\n",
    "    if all_exon_range_dict == {}:\n",
    "        raise ValueError(f\"ValueError - all_exon_range_dict seems to be empty. Its length = {len(all_exon_range_dict)}\")\n",
    "    #print(f\" \\n all_exon_range_dict: {all_exon_range_dict}\")\n",
    "    return (all_exon_range_dict, count_of_errors)\n",
    "\n",
    "\n",
    "     \n",
    "def start_and_end_parameters_of_introns_dict_fun(all_exon_range_dict, count_of_errors):\n",
    "    #linking indices into single strand of possible introns\n",
    "    intron_range_dict = {}\n",
    "    keys_from_all_exon_range_dict = sorted(all_exon_range_dict.keys()) #list that contains exons's start positions\n",
    "    #print(f\"keys_from_all_exon_range_dict: {keys_from_all_exon_range_dict}\\n\")\n",
    "\n",
    "    for i in range(len(keys_from_all_exon_range_dict) - 1):\n",
    "        start_intron = all_exon_range_dict[keys_from_all_exon_range_dict[i]] +1 \n",
    "        end_intron = keys_from_all_exon_range_dict[i + 1] - 1\n",
    "        intron_range_dict[start_intron] = end_intron\n",
    "    if intron_range_dict == {}:\n",
    "        raise ValueError(f\"ValueError - intron_range_dict seems to be empty. Its length = {len(intron_range_dict)}\")\n",
    "    #print(f\" \\n intron_range_dict: {intron_range_dict}\")\n",
    "    return (intron_range_dict, count_of_errors)\n",
    "\n",
    "\n",
    "#zmienione all_fine_exons na fine_exons\n",
    "def making_and_sorting_alignments_fun(all_exon_range_dict, min_length_aligned_sequence, seq1, seq2):\n",
    "    fine_exons_range_dict = {}\n",
    "    TS_exon_range_dict = {} #too\n",
    "    TLH_exon_range_dict = {}\n",
    "    TS_exons = []\n",
    "    TLH_exons = []\n",
    "    fine_exons = []\n",
    "    fine_percentage_of_identity = []\n",
    "    TLH_percentage_of_identity = []\n",
    "    for key in all_exon_range_dict:\n",
    "        query = seq1[key-1:all_exon_range_dict[key]+1]\n",
    "        target = seq2[key-1:all_exon_range_dict[key]+1]\n",
    "        if len(target) > min_length_aligned_sequence:\n",
    "            aligner = Align.PairwiseAligner()\n",
    "            aligner.mismatch_score = 0 #customized setting towards get pure percentage of homology, not alignment with gap penalty etc. The object of interest is simply that how many nt in query has equivalend in target seq. \n",
    "            aligner.open_gap_score = 0\n",
    "            aligner.extend_gap_score = 0\n",
    "            score = aligner.score(query, target)\n",
    "            identity_level = (round(((score * 100) / len(target)), 2))\n",
    "            #print(f\"query: {query}, \\n target: {target}, percent of homology = {round(score / len(target), 2) * 100} %, score = {score}, len = {len(target)}\\n\")\n",
    "            if score >= extreme_homology * 100:\n",
    "                fine_exons.append(query)\n",
    "                fine_exons_range_dict[key] = all_exon_range_dict[key]\n",
    "                fine_percentage_of_identity.append(identity_level)\n",
    "                #print(f\"query: {query} goes to fine_exons\")\n",
    "            else:\n",
    "                TLH_percentage_of_identity.append(identity_level)\n",
    "                TLH_exons.append(query)\n",
    "                TLH_exon_range_dict[key] = all_exon_range_dict[key]       \n",
    "                #print(f\"query: {query} goes to TLH_exons\")\n",
    "        else:\n",
    "            TS_exons.append(query)\n",
    "            TS_exon_range_dict[key] = all_exon_range_dict[key]       \n",
    "            #print(f\"query: {query} goes to TS_exons\")\n",
    "    return fine_exons, TLH_exons, TS_exons, TLH_percentage_of_identity, fine_percentage_of_identity, fine_exons_range_dict, TS_exon_range_dict, TLH_exon_range_dict\n",
    "\n",
    "def extending_source_data_frame(alignment_id, source):\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: extending_source_data_frame, przeszlo\")\n",
    "    keywords = [\"TRINITY\", \"BACKBONE\", \"SCAFFOLD\"]\n",
    "    found = False\n",
    "    for key in keywords:\n",
    "        alignment_id = str(alignment_id).lower()\n",
    "        if alignment_id.find(key.lower()) != -1:\n",
    "            source.append(key)\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        source.append(None) #none means unknown\n",
    "    return source\n",
    "\n",
    "\n",
    "def extending_strand_data_frame(alignment_id, strand): \n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: extending_strand_data_frame, przeszlo\")\n",
    "    if str(alignment_id).find(\"SL+\") != -1:\n",
    "        strand.append(\"+\")\n",
    "    elif str(alignment_id).find(\"SL-\") != -1:\n",
    "        strand.append(\"-\")\n",
    "    else:\n",
    "        strand.append(None)\n",
    "    return strand\n",
    "\n",
    "\n",
    "def appending_list_by_start_exon_position(dict_with_exon_range, start, element):\n",
    "    #for all types of exons in table\n",
    "    sorted_keys_start_exon = sorted(dict_with_exon_range.keys())\n",
    "    start.append(sorted_keys_start_exon[element])\n",
    "    return start\n",
    "\n",
    "def appending_list_by_end_exon_position(dict_with_exon_range, end, element):\n",
    "    #for all types of exons in table\n",
    "    sorted_values_end_exon = sorted(dict_with_exon_range.values())\n",
    "    end.append(sorted_values_end_exon[element])\n",
    "    return end\n",
    "\n",
    "def making_exon_gff_table(fine_exons, seqid, source, typ, score_list, start, end, strand, phase, attributes, alignment_id, fine_percentage_of_identity, fine_exons_range_dict):\n",
    "    for element in range(len(fine_exons)):\n",
    "        print(\"\\n \\n teraz analizujemy takie sekwencje: \\n\", fine_exons[element])\n",
    "        seqid.append(alignment_id) #what organism\n",
    "        source = extending_source_data_frame(alignment_id, source) #backbone, trinity or scaffold. Usually trinity\n",
    "        typ.append(\"exon\") #exon or trinity\n",
    "        score_list.append(fine_percentage_of_identity[element]) #doesn't matter for Ania\n",
    "        start = appending_list_by_start_exon_position(fine_exons_range_dict, start, element)\n",
    "        end = appending_list_by_end_exon_position(fine_exons_range_dict, end, element)\n",
    "        strand = extending_strand_data_frame(alignment_id, strand) # + or - from sequence's name \n",
    "        phase.append(\".\") #????\n",
    "        attributes.append(alignment_id) #other\n",
    "    return seqid, source, typ, score_list, start, end, strand, phase, attributes\n",
    "\n",
    "\n",
    "\n",
    "#zmodyfikuj ta funkcje by byla fajna\n",
    "def making__intron_gff_table(intron_range_dict, seqid, source, typ, score_list, start, end, strand, phase, attributes, alignment_id):\n",
    "    for element in range(len(fine_exons)):\n",
    "        print(\"\\n \\n teraz analizujemy takie sekwencje: \\n\", fine_exons[element])\n",
    "        seqid.append(alignment_id) #what organism\n",
    "        source = extending_source_data_frame(alignment_id, source) #backbone, trinity or scaffold. Usually trinity\n",
    "        typ.append(\"exon\") #exon or trinity\n",
    "        score_list.append(fine_percentage_of_identity[element]) #doesn't matter for Ania\n",
    "        start = appending_list_by_start_exon_position(fine_exons_range_dict, start, element)\n",
    "        end = appending_list_by_end_exon_position(fine_exons_range_dict, end, element)\n",
    "        strand = extending_strand_data_frame(alignment_id, strand) # + or - from sequence's name \n",
    "        phase.append(\".\") #????\n",
    "        attributes.append(alignment_id) #other\n",
    "    return seqid, source, typ, score_list, start, end, strand, phase, attributes\n",
    "\n",
    "\n",
    "#nowe\n",
    "#################################\n",
    "#stare\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extracting_strands_from_alignment(alignment): #that function describe what strand is transcript strand and genomic strand - in our files, transcript strand has longer ID.\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: extracting_strands_from_alignment, przeszlo\")\n",
    "    if len(alignment[0].id) >= len(alignment[1].id):\n",
    "        seq1 = alignment[0].seq\n",
    "        seq2 = alignment[1].seq\n",
    "        #seq1_is_transcript = True #obecnie nie uzywane\n",
    "    else:\n",
    "        seq1 = alignment[1].seq\n",
    "        seq2 = alignment[0].seq    \n",
    "        #seq1_is_transcript = False #obecnie nie uzywane\n",
    "    #print(f\"\\n Just run function: extracting_strands_from_alignment\")\n",
    "    return seq1, seq2\n",
    "    #return seq1_is_transcript #obecnie nie uzywane\n",
    "    \n",
    "\n",
    "def finding_gap_index(sequence, acceptable_gap_length, alignment): #finding index of first gap in each sequence\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: finding_gap_index, przeszlo\")\n",
    "    gap_index = sequence.find(\"-\" * acceptable_gap_length) #-1 is output when anything was found\n",
    "    #print(type(gap_index))\n",
    "    return gap_index\n",
    "\n",
    "\n",
    "def reversed_finding_gap_index(sequence, acceptable_gap_length, alignment): #finding index of first gap in each sequence for counting the end of intron\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: reversed_finding_gap_index, przeszlo\")\n",
    "    return sequence.rfind(\"-\" * acceptable_gap_length) + acceptable_gap_length #now we have to add length of gap to index, because python starts counting from 0 nt, and we just change direction of counting.\n",
    "\n",
    "def setting_distance_between_first_nt_and_gap(is_it_exon, first_gap_position_seq1, first_gap_position_seq2, last_gap_position_seq1, last_gap_position_seq2, seq1, alignment): #it means lenght of exon\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: setting_distance_between_first_nt_and_gap, przeszlo\")\n",
    "    if is_it_exon == True:\n",
    "        if first_gap_position_seq2 < 0: #when in second strand there are no gaps - it means, second strand is correctly prepared. <0 because find() function gives -1 result when not find\n",
    "            distance_between_first_nt_and_gap = first_gap_position_seq1 #more important is seq2 because it is genomic sequence, ~reference sequence\n",
    "        elif first_gap_position_seq1 < 0:\n",
    "            distance_between_first_nt_and_gap = first_gap_position_seq2\n",
    "        else:\n",
    "            distance_between_first_nt_and_gap = min(first_gap_position_seq1, first_gap_position_seq2)\n",
    "        return distance_between_first_nt_and_gap\n",
    "\n",
    "    elif is_it_exon == False:\n",
    "        if last_gap_position_seq2 < 0: #it means, if there is no gaps in second sequention, gap position is gap position in seq1. Additional, rfind() function return \"-1\" if it won't find any gaps in seq\n",
    "            distance_between_first_nt_and_gap = (len(seq1) - last_gap_position_seq1) -1\n",
    "            #print(\"pierwszy\", distance_between_first_nt_and_gap) #checkpoint\n",
    "        else:\n",
    "            distance_between_first_nt_and_gap = (len(seq1) - max(last_gap_position_seq1, last_gap_position_seq2))\n",
    "        return distance_between_first_nt_and_gap\n",
    "            #print(\"drugi\", distance_between_first_nt_and_gap) #checkpoint\n",
    "\n",
    "        \n",
    "def describing_class_of_exon(percent_of_homology, last_nt_exon_index, min_length_aligned_sequence, class_of_exon): #class of exon depends of their homology\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: describing_class_of_exon, przeszlo\")\n",
    "    if percent_of_homology >= 0.9 and last_nt_exon_index >= min_length_aligned_sequence:\n",
    "        class_of_exon.append(\"1\")\n",
    "    elif percent_of_homology < 0.9 and percent_of_homology > 0 and last_nt_exon_index < min_length_aligned_sequence:\n",
    "        class_of_exon.append(\"2\")\n",
    "    else:\n",
    "        class_of_exon.append(\"3\")\n",
    "\n",
    "\n",
    "def exons_indices(is_it_exon, max_genomic_seq, seq2, last_nt_exon_index, first_nt_position, last_nt_position):\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: exons_indices, przeszlo\")\n",
    "    if is_it_exon:\n",
    "        first_nt_index_of_exon = max_genomic_seq.find(str(seq2[:last_nt_exon_index]).replace(\"-\", \"\"))\n",
    "        if first_nt_index_of_exon == -1:\n",
    "            first_nt_position.append(0)\n",
    "            last_nt_position.append(0)\n",
    "            #print(f\"That exon: {seq2[:last_nt_exon_index].replace('-', '')} comes from genomic sequence and cannot be found in file_before_MAFFT.\")\n",
    "        else:\n",
    "            first_nt_position.append(first_nt_index_of_exon + 1)\n",
    "            last_nt_position.append(first_nt_index_of_exon + len(seq2[:last_nt_exon_index]))\n",
    "            \n",
    "\n",
    "def introns_indices(is_it_exon, max_genomic_seq, seq2, last_nt_intron_index, first_nt_position, last_nt_position):\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: introns_indices, przeszlo\")\n",
    "    if is_it_exon: \n",
    "        first_nt_index_of_intron = max_genomic_seq.find(str(seq2[:last_nt_intron_index]))\n",
    "        if first_nt_index_of_intron == -1:\n",
    "            first_nt_position.append(0)\n",
    "            last_nt_position.append(0)\n",
    "        else:\n",
    "            first_nt_position.append(first_nt_index_of_intron + 1)\n",
    "            last_nt_position.append(first_nt_index_of_intron + len(seq2[:last_nt_intron_index]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extending_start_and_end_data_frame_exons(seq2, seq2_DT, start, end, distance_between_first_nt_and_gap, last_nt_exon_index): #index of exons's start\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: extending_start_and_end_data_frame, przeszlo\")\n",
    "    #print(f\"Alignment:, funkcja: extending_start_and_end_data_frame, przeszlo\")\n",
    "    start.append(seq2_DT.find(seq2[:distance_between_first_nt_and_gap])+1) #+1 because python starts counting from 0, not 1\n",
    "    end.append(seq2_DT.find(seq2[:distance_between_first_nt_and_gap]) + last_nt_exon_index)\n",
    "\n",
    "def extending_start_and_end_data_frame_introns(seq2, seq2_DT, start, end, distance_between_first_nt_and_gap, last_nt_intron_index): #index of intron's start\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: extending_start_and_end_data_frame, przeszlo\")\n",
    "    #print(f\"Alignment:, funkcja: extending_start_and_end_data_frame, przeszlo\")\n",
    "    start.append(seq2_DT.find(seq2[:distance_between_first_nt_and_gap])+1)\n",
    "    end.append(seq2_DT.find(seq2[:distance_between_first_nt_and_gap]) + last_nt_intron_index)\n",
    "\n",
    "def save_to_file(final_data_frame, filename):\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: save_to_file, przeszlo\")\n",
    "    if os.path.isfile(filename):\n",
    "        user_input = input(\"TSV file already exists. Do you want overwrite? y/n: \")\n",
    "        if user_input == \"n\":\n",
    "            base_name, ext = os.path.splitext(filename)\n",
    "            i = 1\n",
    "            while os.path.isfile(f\"{base_name}_{i}{ext}\"):\n",
    "                i += 1\n",
    "            filename = f\"{base_name}_{i}{ext}\"\n",
    "            final_data_frame.to_csv(filename, sep = \"\\t\")\n",
    "            \n",
    "        elif user_input == \"y\":\n",
    "            final_data_frame.to_csv(filename, sep = \"\\t\")\n",
    "        else:\n",
    "            print(\"Clarify your answer. Nothing has done.\")\n",
    "            \n",
    "    else:\n",
    "        final_data_frame.to_csv(filename, sep = \"\\t\")\n",
    "\n",
    "def save_to_gff_file(gff_final_data_frame, filename):\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: save_to_gff_file, przeszlo\")\n",
    "    if os.path.isfile(filename):\n",
    "        user_input = input(\"GFF file already exists. Do you want overwrite? y/n: \")\n",
    "        if user_input == \"n\":\n",
    "            base_name, ext = os.path.splitext(filename)\n",
    "            i = 1\n",
    "            while os.path.isfile(f\"{base_name}_{i}{ext}\"):\n",
    "                i += 1\n",
    "            filename = f\"{base_name}_{i}{ext}\"\n",
    "            gff_final_data_frame.to_csv(filename, sep = \"\\t\")\n",
    "            \n",
    "        elif user_input == \"y\":\n",
    "            gff_final_data_frame.to_csv(filename, sep = \"\\t\")\n",
    "        else:\n",
    "            print(\"Clarify your answer. Nothing has done.\")\n",
    "            \n",
    "    else:\n",
    "        gff_final_data_frame.to_csv(filename, sep = \"\\t\")\n",
    "\n",
    "def check_alignment_eligibility(alignment, alignment_DIDNT_TOUCHED, acceptable_gap_length, files_in_progress, count_files, count_of_errors, filename, extreme_homology, min_length_aligned_sequence):\n",
    "    while True:\n",
    "        is_it_exon = True\n",
    "        first_nucleotides_pair = alignment[:, 0]#first nucleotides in both strands as variable\n",
    "        if \"-\" in first_nucleotides_pair: #deleting mismatches at the start\n",
    "            if len(alignment[0]) < 100: #zakoczenie przycinania - nie wiem czy to jest to o co mi chodzilo\n",
    "                break\n",
    "            else:\n",
    "                alignment = alignment[:, 1:]\n",
    "                #print(f\"alignment after cut:\\n{alignment}\") #chekpoint\n",
    "                if len(alignment[0]) % 10 == 0:\n",
    "                    # print(f\"{round((100-(len(alignment[0])/len(alignment_DIDNT_TOUCHED[0]))*100), 2)}% base pair done of {alignment[0].id}. {files_in_progress} of {count_files}, what means {round(files_in_progress/count_files*100, 2)}% of advancement. The time is: {time.strftime('%H:%M:%S', time.localtime())}\")\n",
    "                    # print(f\"First 50 nt of reference sequence is: {(alignment[1].seq)[:50]}. \\n\")\n",
    "                    function_progress(alignment, alignment_DIDNT_TOUCHED, files_in_progress, count_files)\n",
    "            continue\n",
    "    \n",
    "                \n",
    "        seq1, seq2 = extracting_strands_from_alignment(alignment)\n",
    "        seq1_DT, seq2_DT = extracting_strands_from_alignment(alignment_DIDNT_TOUCHED) #sequences without cutted gaps from the ends. They will be used to count index where intron starts\n",
    "        \n",
    "    \n",
    "        first_gap_position_seq1 = finding_gap_index(seq1, acceptable_gap_length, alignment)\n",
    "        first_gap_position_seq2 = finding_gap_index(seq2, acceptable_gap_length, alignment)\n",
    "        last_gap_position_seq1 = reversed_finding_gap_index(seq1, acceptable_gap_length, alignment)\n",
    "        last_gap_position_seq2 = reversed_finding_gap_index(seq2, acceptable_gap_length, alignment)\n",
    "        #print(f\"first gap position in first sequence:{first_gap_position_seq1}, in 2nd sequence: {first_gap_position_seq2}\") #checkpoint\n",
    "    \n",
    "        distance_between_first_nt_and_gap = setting_distance_between_first_nt_and_gap(is_it_exon, first_gap_position_seq1, first_gap_position_seq2, last_gap_position_seq1, last_gap_position_seq2, seq1, alignment)\n",
    "        \n",
    "        local_alignment = pairwise2.align.localxx(seq1[:distance_between_first_nt_and_gap], seq2[:distance_between_first_nt_and_gap], one_alignment_only = True) #\"xx\"  means no gap penalty while opening gaps or longering them and no penalties for mismatch. Just pure score of alignment to count homology\n",
    "        print(\"local_alignment:\", local_alignment) #checkpoint\n",
    "    \n",
    "        try:  #instruction what to do when seq is too short\n",
    "            local_homology_percentage = (local_alignment[0].score / distance_between_first_nt_and_gap) \n",
    "        except IndexError:\n",
    "            if len(seq1) < 100:\n",
    "                count_of_errors += 1\n",
    "                print(f\"That file {filename} made {count_of_errors}th error\")\n",
    "                #print(f\"\\n \\n {alignment[0].id} sequence is too short, sequence has not exons or other fault. \\n\")\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        \n",
    "        local_homology_percentage = (local_alignment[0].score / distance_between_first_nt_and_gap)\n",
    "        print(local_homology_percentage)\n",
    "\n",
    "    \n",
    "        if local_homology_percentage <= extreme_homology:\n",
    "            print(f\" Too low score of homology. \\n alignment: {alignment[:, :50]} \\n score: {local_alignment[0].score}, distance: {distance_between_first_nt_and_gap}, percent: {round(local_homology_percentage, 2)*100}%, \\n {local_alignment} \\n \\n \\n \\n \\n\")\n",
    "            alignment = alignment[:, distance_between_first_nt_and_gap:]\n",
    "            function_progress(alignment, alignment_DIDNT_TOUCHED, files_in_progress, count_files)            \n",
    "        else:\n",
    "            if distance_between_first_nt_and_gap <= min_length_aligned_sequence:\n",
    "                #print(f\"\\n alignment is too short. \\n distance between sequences {distance_between_first_nt_and_gap}, \\n alignment: {alignment}, \\n score: {local_alignment[0].score}, distance: {distance_between_first_nt_and_gap}, procentowo: {round(local_homology_percentage, 2)*100}%, \\n {local_alignment}\")\n",
    "                alignment = alignment[:, distance_between_first_nt_and_gap:]\n",
    "                function_progress(alignment, alignment_DIDNT_TOUCHED, files_in_progress, count_files)\n",
    "            else:\n",
    "                #print(f\"Sequence {alignment[0].id} cut properly from LEFT side, \\n distance between sequences: {distance_between_first_nt_and_gap}\"), alignment: {alignment[:, :50]},\n",
    "                break\n",
    "                \n",
    "        if len(seq1) < 100 or len(seq2) < 100:\n",
    "            break\n",
    "\n",
    "        return seq1, seq2, seq1_DT, seq2_DT, first_gap_position_seq1, first_gap_position_seq2, last_gap_position_seq1, last_gap_position_seq2, distance_between_first_nt_and_gap, local_alignment, local_homology_percentage\n",
    "\n",
    "    ######################### TERAZ OD PRAWEJ DO LEWEJ #######################\n",
    "    \n",
    "    while True:\n",
    "            \n",
    "        is_it_exon = False\n",
    "        last_nucleotides_pair = alignment[:, -1]#last nucleotides in both strands as variable\n",
    "\n",
    "    \n",
    "        if \"-\" in last_nucleotides_pair: #deleting mismatches at the end\n",
    "            if len(alignment[0]) < 100 or len(alignment[1]) < 100:\n",
    "                #print(\"alignment has no matching nucleotides\")\n",
    "                break \n",
    "            else:\n",
    "                alignment = alignment[:, :-1]\n",
    "                function_progress(alignment, alignment_DIDNT_TOUCHED, files_in_progress, count_files)\n",
    "                #print(f\"alignment check:\\n{alignment[:, -50:-1]}\") checkpoint\n",
    "            continue\n",
    "        else:\n",
    "            if len(alignment[0]) < 100 or len(alignment[1]) < 0:\n",
    "                #print(\"counting from right side: alignment is too short\")\n",
    "                break\n",
    "        \n",
    "    \n",
    "        seq1, seq2 = extracting_strands_from_alignment(alignment)\n",
    "        seq1_DT, seq2_DT = extracting_strands_from_alignment(alignment_DIDNT_TOUCHED) #sequences without cutted gaps from the ends. They will be used to count index where intron starts\n",
    "        \n",
    "\n",
    "        first_gap_position_seq1 = finding_gap_index(seq1, acceptable_gap_length, alignment)\n",
    "        first_gap_position_seq2 = finding_gap_index(seq2, acceptable_gap_length, alignment)\n",
    "        last_gap_position_seq1 = reversed_finding_gap_index(seq1, acceptable_gap_length, alignment)\n",
    "        last_gap_position_seq2 = reversed_finding_gap_index(seq2, acceptable_gap_length, alignment)\n",
    "        #print(\"seq1:\", last_gap_position_seq1, \"seq2:\", last_gap_position_seq2) #checkpoint\n",
    "        \n",
    "        distance_between_first_nt_and_gap = setting_distance_between_first_nt_and_gap(is_it_exon, first_gap_position_seq1, first_gap_position_seq2, last_gap_position_seq1, last_gap_position_seq2, seq1, alignment)\n",
    "        #print(\"drugi\", distance_between_first_nt_and_gap)\n",
    "                \n",
    "        local_alignment = pairwise2.align.localxx(seq1[-distance_between_first_nt_and_gap:], seq2[-distance_between_first_nt_and_gap:], one_alignment_only = True) #\"xx\" means no gap penalty while opening gaps or longering them and no penalties for mismatch. Just pure score of alignment\n",
    "        #print(\"local_alignment:\", local_alignment) #checkpoint\n",
    "                \n",
    "    \n",
    "        try:#instruction what to do when seq is too short or gaps were not found\n",
    "            if distance_between_first_nt_and_gap != 0:\n",
    "                local_homology_percentage = (local_alignment[0].score / distance_between_first_nt_and_gap) \n",
    "            else:\n",
    "                break\n",
    "        except IndexError:\n",
    "            if len(seq1) < 100:\n",
    "                count_of_errors += 1\n",
    "                print(f\"That file {filename} made {count_of_errors}th error\")\n",
    "                #print(f\"\\n \\n {alignment[0].id} sequence is too short or sequence has not exons. \\n\")\n",
    "                break\n",
    "            continue\n",
    "    \n",
    "        \n",
    "        if local_homology_percentage <= extreme_homology:\n",
    "            #print(f\"\\n Too low homology! \\n alignment: {alignment[:, -50:]} \\n score: {local_alignment[0].score}, \\distance: {distance_between_first_nt_and_gap}, %%%: {round(local_homology_percentage, 2)*100}%, \\n {local_alignment}\")\n",
    "            alignment = alignment[:, :-distance_between_first_nt_and_gap] \n",
    "            function_progress(alignment, alignment_DIDNT_TOUCHED, files_in_progress, count_files)\n",
    "            \n",
    "        else:\n",
    "            if distance_between_first_nt_and_gap <= min_length_aligned_sequence:\n",
    "                #print(f\"\\n Alignment is too short!. \\n distance: {distance_between_first_nt_and_gap}, \\n alignment: {alignment}, \\n score: {local_alignment[0].score}, distance: {distance_between_first_nt_and_gap}, %%%: {round(local_homology_percentage, 2)*100}%, \\n {local_alignment}\")\n",
    "                alignment = alignment[:, :-distance_between_first_nt_and_gap]\n",
    "                function_progress(alignment, alignment_DIDNT_TOUCHED, files_in_progress, count_files)\n",
    "            else:\n",
    "                #print(f\"Sequence: {alignment[0].id} cut properly from RIGHT side, alignment: \\n {alignment[:, -50:]}\")\n",
    "                break\n",
    "                \n",
    "        return seq1, seq2, seq1_DT, seq2_DT, first_gap_position_seq1, first_gap_position_seq2, last_gap_position_seq1, last_gap_position_seq2, distance_between_first_nt_and_gap, local_alignment, local_homology_percentage\n",
    "\n",
    "        if len(seq1) < 100 or len(seq2) < 100:\n",
    "            break\n",
    "\n",
    "def function_progress(alignment, alignment_DIDNT_TOUCHED, files_in_progress, count_files):\n",
    "    #do tego tematu wrocimy jak zrobimy tabele z odrzuconymi wartosciami\n",
    "    first_50_nt_alignment = str((alignment[1].seq)[:20])\n",
    "    #print(f\" {round((100-(len(alignment[0])\\/len(alignment_DIDNT_TOUCHED[0]))*100), 2)}% % base pair done of {alignment[0].id}. {files_in_progress} of {count_files}, what means {round(files_in_progress/count_files*100, 2)}% of advancement. The time is: {time.strftime('%H:%M:%S', time.localtime())}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cutting_scrap(path_to_file_before_MAFFT, path_to_file_after_MAFFT, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b53f45b-6863-4da2-b267-c258749a5060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstan rzeczy: cos sie dubluje, niektore fragmenty tekstu. Mozliwe ze to klopot z iteracja po iteracji. Napisz najpierw slownik\\n\\ncel 0: zmien frament na poczatku zeby nie iterowal po iterowanym tekscie. prostsza metoda, nie slownik \\nCHYBA GIT\\n\\ncel: usprawnij to tak by outputem byl 148 wierszowa tabela\\n    Dwa klopoty:\\n        1. wszystkie dane sa od tego samego organizmu --> klopot z nazwa w seq id --> trzeba zmienic sposob laczenia list do slownika UPDATE. to nawet nie jest przyczyna trudności, tu chodzi o kwestie iteracji\\n        2. niektore dane maja za niska homologie - mozliwe ze sa powiazania z powyzszym punktem\\n        3. a inne nie respectuja minimalnej dlugosci eksonu\\n\\ncek na potem: usunac introny z ostatniej pozycji w tabeli - powiazanie z powyzszymi punktami\\n\\ncel na potem: zrob tak by wyswietlal sie postep na biezaco, nie tylko gdy pierwszym znakiem bedzie gap\\n    Ta funkcja jest zle skonstruowana. Musimy przyjac za punkt odniesienia stopien zaawansowania w odniesieniu do funkcji Didnt Touched\\n    Wroce do tego potem jak bede pisał tabele z odrzuconymi danymi \\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "stan rzeczy: cos sie dubluje, niektore fragmenty tekstu. Mozliwe ze to klopot z iteracja po iteracji. Napisz najpierw slownik\n",
    "\n",
    "cel 0: zmien frament na poczatku zeby nie iterowal po iterowanym tekscie. prostsza metoda, nie slownik \n",
    "CHYBA GIT\n",
    "\n",
    "cel: usprawnij to tak by outputem byl 148 wierszowa tabela\n",
    "    Dwa klopoty:\n",
    "        1. wszystkie dane sa od tego samego organizmu --> klopot z nazwa w seq id --> trzeba zmienic sposob laczenia list do slownika UPDATE. to nawet nie jest przyczyna trudności, tu chodzi o kwestie iteracji\n",
    "        2. niektore dane maja za niska homologie - mozliwe ze sa powiazania z powyzszym punktem\n",
    "        3. a inne nie respectuja minimalnej dlugosci eksonu\n",
    "\n",
    "cek na potem: usunac introny z ostatniej pozycji w tabeli - powiazanie z powyzszymi punktami\n",
    "\n",
    "cel na potem: zrob tak by wyswietlal sie postep na biezaco, nie tylko gdy pierwszym znakiem bedzie gap\n",
    "    Ta funkcja jest zle skonstruowana. Musimy przyjac za punkt odniesienia stopien zaawansowania w odniesieniu do funkcji Didnt Touched\n",
    "    Wroce do tego potem jak bede pisał tabele z odrzuconymi danymi \n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41155fc2-97d3-4d6d-818d-c3aade37bc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
