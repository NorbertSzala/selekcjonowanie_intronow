{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f037dd59-8d78-4bbc-b1f8-f878afe1f342",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Bio import AlignIO, SeqIO, Seq, pairwise2, Align\n",
    "from Bio.pairwise2 import format_alignment\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daa9c022-9b97-48b7-b7f7-9d01e024710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ASSUMPTIONS\n",
    "#- both file, path_to_file_before_MAFFT and path_to_file_after_MAFFT have to have the same name\n",
    "#- they have to be .fasta\n",
    "\n",
    "\n",
    "min_length_aligned_sequence = 30 #Minimal lenght of sequence which could be an exon\n",
    "extreme_homology = 0.97 #percentage of homology of sequence, treshold #I assume two faulty aligned nucleotides per 100 (98%) and one more nt because sometimes latest nt can move from end of one sequence to beginning next sequence\n",
    "\n",
    "#9 plikow\n",
    "#path_to_file_before_MAFFT = \"/home/norbert/mrdn/euglena/kod_i_pliki/surowe_pliki_plus_minus_500/raw_reads_9/merging_fastas\"\n",
    "#path_to_file_after_MAFFT = \"/home/norbert/mrdn/euglena/kod_i_pliki/surowe_pliki_plus_minus_500/raw_reads_9/fastas_after_mafft_na_probe\"\n",
    "\n",
    "#ten niedzialajacy pierwszy LON309\n",
    "path_to_file_before_MAFFT = \"/home/norbert/mrdn/euglena/kod_i_pliki/surowe_pliki_plus_minus_500/raw_reads_9/roboczy_przed_fastami/\"\n",
    "path_to_file_after_MAFFT = \"/home/norbert/mrdn/euglena/kod_i_pliki/surowe_pliki_plus_minus_500/raw_reads_9/roboczy_po_fastach/\"\n",
    "\n",
    "#pozostale 8\n",
    "#ath_to_file_before_MAFFT = \"/home/norbert/mrdn/euglena/kod_i_pliki/surowe_pliki_plus_minus_500/raw_reads_9/inne_przed_fastami/\"\n",
    "#path_to_file_after_MAFFT = \"/home/norbert/mrdn/euglena/kod_i_pliki/surowe_pliki_plus_minus_500/raw_reads_9/inne_po_fastach/\"\n",
    "\n",
    "\n",
    "#DODAJ FRAGMENT MOWIACY ZE JESLI: NA RAZIE SIE WSTRZYMAM, MOZE NIE JEST TO POTRZEBNE\n",
    "    #LICZBA INTRONOW >= LICZBIE EKSONOW TO BREAK\n",
    "    #LICZBA INTRONOW + EKSONOW <= 3 TO BREAK\n",
    "\n",
    "\n",
    "#dodac tabelke tak by mowila ile bylo odczytow na inpucie, ile przeszło pozytywnie a ile zostało przerwanych w trakcie\n",
    "#dodatkowa tabelka ze wszystkimi eksonami które byly za krótkie teżhbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "42d489c2-e090-4244-98db-a453e82cc84b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      " \n",
      " Just run function: cutting_scrap\n",
      "\n",
      " ################################################################################### \n",
      " running file: GRA_OG0030307.fasta which is 1 of 1, what means 100.0% of advancement \n",
      " ################################################################################### \n",
      "\n",
      "temp_introns indices: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 195, 217, 218, 219, 220, 233, 234, 235, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 316, 317, 318, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 677, 678, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1469, 1470, 1471, 1472, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319, 3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329, 3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355, 3356, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411]\n",
      "temp_exon indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 110, 111, 112, 113, 114, 115, 116, 147, 148, 149, 150, 151, 152, 153, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 353, 354, 355, 356, 357, 358, 359, 360, 361, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541, 3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552, 3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596, 3597, 3598, 3599, 3600, 3601]\n",
      " \n",
      " exon_range_dict: {0: 9, 35: 80, 110: 116, 147: 153, 175: 216, 221: 232, 236: 253, 266: 285, 304: 315, 319: 330, 353: 361, 476: 579, 656: 676, 679: 733, 821: 832, 908: 927, 1073: 1104, 1187: 1212, 1268: 1291, 1436: 1468, 1473: 1502, 1548: 1563, 1749: 1765, 2300: 2315, 2360: 2455, 2558: 2702, 2941: 3012}\n",
      "lista eksonow: gatctattt\n",
      "lista eksonow: ttttcgccgtgccacgtgcccgcctgcgatggcggaggaggacgc\n",
      "lista eksonow: ccggat\n",
      "lista eksonow: acaaca\n",
      "lista eksonow: gatgatctactggacgggct-tccagggggacgacgaggac\n",
      "lista eksonow: acctcggcgag\n",
      "lista eksonow: gctgcgaccagcgggag\n",
      "lista eksonow: ggatgttcttctcccgcga\n",
      "lista eksonow: tactgggcccg\n",
      "lista eksonow: cccaacccgtt\n",
      "lista eksonow: catctcgg\n",
      "lista eksonow: cctcggcggaagaacggcgactttcttggggtcaaggtcgtcaagaatcgcttctttgtgccgtacctgtacggcggcctggaggagagggtgcaccacgcgc\n",
      "lista eksonow: aagcgtgacgcatgaccacc\n",
      "lista eksonow: cgtaccgtacgtcggctgctaccacgaccacggccgggacgagacgctgctgtg\n",
      "lista eksonow: cagcaccacgt\n",
      "lista eksonow: cagggggcgtcgctggaga\n",
      "lista eksonow: gctgctgcggaagcgacggcggggcttcgac\n",
      "lista eksonow: aggtgcaggtgcgggtctggctgtt\n",
      "lista eksonow: agcgtgctgagggggctgcagca\n",
      "lista eksonow: ctccaccagcatggctgcgtgctgggcaagct\n",
      "lista eksonow: cctccaaggtggtcctcgaccgggacggc\n",
      "lista eksonow: tcccggtgctggtgg\n",
      "lista eksonow: ctgggcctgcccgccc\n",
      "lista eksonow: gcctcttcaaggcca\n",
      "lista eksonow: ctacacgtacaagcacgtgaacgtcatggcgccggaggtccaaagcggccgggacttcgggccctccgccgacgtctggagcttcgggctgctcc\n",
      "lista eksonow: gatcgagctgctgaccggccagccggcgttccccgtctcggaggacacccagcggcagttccgggcggcggaccacgaggaggaggggccaccgggtgtccctgcgggggccgtggaggcgcgggacatcaagaaccgctgctt\n",
      "lista eksonow: atgtcgccggacgccctctcgctggccctccgctgcctcgccaagtccccggcccggcggcccaccgtggc\n",
      "[0, 35, 110, 147, 175, 221, 236, 266, 304, 319, 353, 476, 656, 679, 821, 908, 1073, 1187, 1268, 1436, 1473, 1548, 1749, 2300, 2360, 2558, 2941]\n",
      "{10: 34, 81: 109, 117: 146, 154: 174, 217: 220, 233: 235, 254: 265, 286: 303, 316: 318, 331: 352, 362: 475, 580: 655, 677: 678, 734: 820, 833: 907, 928: 1072, 1105: 1186, 1213: 1267, 1292: 1435, 1469: 1472, 1503: 1547, 1564: 1748, 1766: 2299, 2316: 2359, 2456: 2557, 2703: 2940}\n",
      "<class 'dict'>\n",
      "lista intronow: ------------------------\n",
      "lista intronow: ----------------------------\n",
      "lista intronow: -----------------------------\n",
      "lista intronow: --------------------\n",
      "lista intronow: ---\n",
      "lista intronow: --\n",
      "lista intronow: -----------\n",
      "lista intronow: -----------------\n",
      "lista intronow: --\n",
      "lista intronow: ---------------------\n",
      "lista intronow: -----------------------------------------------------------------------------------------------------------------\n",
      "lista intronow: ---------------------------------------------------------------------------\n",
      "lista intronow: -\n",
      "lista intronow: --------------------------------------------------------------------------------------\n",
      "lista intronow: --------------------------------------------------------------------------\n",
      "lista intronow: ------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "lista intronow: ---------------------------------------------------------------------------------\n",
      "lista intronow: ------------------------------------------------------\n",
      "lista intronow: -----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "lista intronow: cgg\n",
      "lista intronow: --------------------------------------------\n",
      "lista intronow: ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "lista intronow: -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "lista intronow: -------------------------------------------\n",
      "lista intronow: -----------------------------------------------------------------------------------------------------\n",
      "lista intronow: ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def cutting_scrap(path_to_file_before_MAFFT, path_to_file_after_MAFFT, acceptable_gap_length):\n",
    "    data_frames = []\n",
    "    gff_data_frames = []\n",
    "    gaps_signs = \"-\" * acceptable_gap_length #maximum length of gaps in sequence in one exon's sequence\n",
    "    files_in_progress = 0\n",
    "    print(f\"\\n \\n \\n \\n Just run function: cutting_scrap\")\n",
    "\n",
    "    for filename in os.listdir(path_to_file_after_MAFFT):\n",
    "        #print(f\"\\n Just tried make 1st turn - find all filenames, \\n there is filename: {str(filename)}\")\n",
    "        file = os.path.join(path_to_file_after_MAFFT, filename)\n",
    "        if not os.path.isfile(file):\n",
    "            continue\n",
    "        if file.endswith(\".fasta\"):\n",
    "            files_in_progress += 1\n",
    "            count_files = percentage_of_advancement(path_to_file_before_MAFFT)\n",
    "            print(f\"\\n ################################################################################### \\n running file: {filename} which is {files_in_progress} of {count_files}, what means {round(files_in_progress/count_files*100, 2)}% of advancement \\n ################################################################################### \\n\")\n",
    "            alignment = AlignIO.read(file, \"fasta\")\n",
    "            alignment_id = alignment[0].id\n",
    "            alignment_DIDNT_TOUCHED = AlignIO.read(file, \"fasta\")\n",
    "            #print(f\"NA POCZATKU alignment: {alignment[0].id} ma dlugosc {len(alignment[0])}\")\n",
    "\n",
    "        count_of_errors = 0\n",
    "\n",
    "\n",
    "        #Pawel mowil o mozliwosci stworzenia slownika w ktorym bylo przypisanie \n",
    "        #sekwencja_z_pliku_przed_mafftem do sekwencja_z_pliku_po_maffcie. \n",
    "        #Ja nie zrobilem slownika, a wydaje mi sie jeszcze krotsza droga:\n",
    "        file_before_MAFFT = os.path.join(path_to_file_before_MAFFT, filename)\n",
    "        if not os.path.isfile(file_before_MAFFT):\n",
    "            continue\n",
    "\n",
    "        \n",
    "        seq1_DT, seq2_DT = extracting_strands_from_alignment(alignment_DIDNT_TOUCHED) #type is Bio.Seq.Seq but works like a string\n",
    "        #dzialamy teraz nie na iterowanym alignmencie tylko na sekwencji ktora zachowuje sie jak string\n",
    "        \n",
    "        nucleotides = [\"a\", \"t\", \"g\", \"c\", \"A\", \"T\", \"G\", \"C\"]\n",
    "        index = 0\n",
    "\n",
    "        #EMPTY LIST TSV\n",
    "        id = []\n",
    "        exon_or_intron_number = []\n",
    "        is_it_intron = []\n",
    "        class_of_exon = []\n",
    "        percent_of_homology_for_exons = []\n",
    "        length = []\n",
    "        first_nt_position = []\n",
    "        last_nt_position = []\n",
    "        first_10_nt = []\n",
    "        last_10_nt = []\n",
    "        path = []\n",
    "        sequence = []\n",
    "\n",
    "        #EMPTY LIST GFF\n",
    "        seqid = [] #name of the chromosome or scaffold; chromosome names can be given with or without the 'chr' prefix. Important note: the seq ID must be one used within Ensembl, i.e. a standard chromosome name or an Ensembl identifier such as a scaffold ID, without any additional content such as species or assembly. See the example GFF output below.\n",
    "        source = [] #tutaj pominiemy ten krok - damy kropke #Describes the algorithm or the procedure that generated this feature. Typically Genescane or Genebank, respectively. #scaffold backbone trinity  i te numerki jakieś\n",
    "        typ = [] #Describes what the feature is (mRNA, domain, exon, etc.). #scaffold backbone trinity  i te numerki jakieś #intron lub exon\n",
    "        start = []\n",
    "        end = []\n",
    "        score = [] #Typically E-values for sequence similarity and P-values for predictions. Homology there\n",
    "        strand = []\n",
    "        phase = [] #Indicates where the feature begins with reference to the reading frame. The phase is one of the integers 0, 1, or 2, indicating the number of bases that should be removed from the beginning of this feature to reach the first base of the next codon.. Propably 0\n",
    "        attributes = [] #A semicolon-separated list of tag-value pairs, providing additional information about each feature. Some of these tags are predefined, e.g. ID, Name, Alias, Parent . You can see the full list [here](https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md).\n",
    "\n",
    "\n",
    "        seq1 = cleaning_gaps_from_both_edges(seq1_DT) #deleting gaps from 5' and 3'. It is important. The next function start counting from possible exon.\n",
    "        seq2 = cleaning_gaps_from_both_edges(seq2_DT)\n",
    "\n",
    "        # seq1 = seq1[:300] #ustalenia na potrzeby przejrzystosci dzialania na malych liczbach\n",
    "        # seq2 = seq2[:300]\n",
    "\n",
    "        #pomysl matematyczny\n",
    "        #iterujemy jednoczesnie po dwoch sekwencjach obiektach str. One zatracily swoj charakter .seq na wczesniejszej obrobce.\n",
    "        #mamy teraz dwie listy zawierajace pozycje wszystkich gapow i nie-gapow. Teraz:\n",
    "            #jesli roznica miedzy lista[i] a lista[i+1] > 2\n",
    "\n",
    "        #linking pre-exon's nucleotides (nt-nt pairs) and pre-intron's nucleotides (gap-nt pairs) to theirs indices \n",
    "        temp_intron, temp_exon, count_of_errors = indices_of_introns_and_exons_fun(seq1_DT, seq2_DT, seq1, seq2, count_of_errors)\n",
    "        print(f\"temp_introns indices: {temp_intron}\")\n",
    "        print(f\"temp_exon indices: {temp_exon}\")\n",
    "\n",
    "\n",
    "        #linking indices into single strand of possible exons\n",
    "        exon_range_dict = start_and_end_parameters_of_exons_dict_fun(temp_exon, gaps_signs)\n",
    "        for key in exon_range_dict: #shows exon's sequences according to exon range dict\n",
    "            print(f\"lista eksonow: {seq1[key:exon_range_dict[key]]}\")\n",
    "\n",
    "        #linking indices into single strand of possible introns\n",
    "        intron_range_dict  = start_and_end_parameters_of_introns_dict_fun(exon_range_dict)\n",
    "        for key in intron_range_dict: #shows introns's sequences according to exon range dict\n",
    "            print(f\"lista intronow: {seq1[key:intron_range_dict[key]]}\")\n",
    "\n",
    "\n",
    "    #ALE BINGO PINKNE\n",
    "#NASTEPNIE ZROB ALIGNMENT tych eksonow\n",
    "#POROZDZIELAJ WYNIKI DO LIST W ZALEZNOSCI OD STOPNIA HOMOLOGII i dlugosci - czyli w skrocie, wszystko to co odrzucamy\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "        \n",
    "\n",
    "        #####################################################################################################################\n",
    "        #########################################              NIEWYPAL              ########################################\n",
    "        ##########        NIE MOZEMY SKRACAC SEKWENCJI W TRAKCIE ITEROWANIA BO TO ZLE. SPROBUJ TERAZ INACZEJ       ##########\n",
    "        #####################################################################################################################\n",
    "        # all_exons = []\n",
    "        # good_exons = []\n",
    "        # introns = []\n",
    "        # def alignment_for_exon_fun(potencial_exon, seq1, seq2, min_length_aligned_sequence, extreme_homology, alignment_id):\n",
    "        #     print(\"\\n alignment_for_exon_fun\")\n",
    "        #     length_of_exon = len(potencial_exon)\n",
    "        #     if length_of_exon > 0:\n",
    "        #         aligner = Align.PairwiseAligner() #function making alignment\n",
    "        #         score = aligner.score(seq1[:length_of_exon], seq2[:length_of_exon])\n",
    "        #         target = seq1[:length_of_exon]\n",
    "        #         print(f\"target: {target}\")\n",
    "        #         query = seq2[:length_of_exon]\n",
    "        #         print(f\"query: {query}\")\n",
    "        #         alignment_seqs = aligner.align(target, query)\n",
    "        #         print(f\"Alignment : \\n{alignment_seqs[0]}\")\n",
    "        #         if score == 0:\n",
    "        #             print(f\"Alignment_for_exon_fun deleted. Score of alignment = {score}\")\n",
    "        #             return score, length_of_exon\n",
    "        #         print(f\"Alignment score of {alignment_id} = {score}\")\n",
    "        #         return score, length_of_exon\n",
    "        #     else: \n",
    "        #         print(f\"Alignment_for_exon_fun broken. Length of exon = {length_of_exon} \")\n",
    "        #         score = 0 #dont do alignment if sequencion is too short \n",
    "        #         return score, length_of_exon\n",
    "            \n",
    "        # def detecting_first_exon_fun(all_exons, good_exons, introns, seq1, seq2, gaps_signs, min_length_aligned_sequence, extreme_homology, acceptable_gap_length, alignment_id):\n",
    "        #     print(\"\\n Poczatek detecting_first_exon\")\n",
    "        #     #two_first_positions = seq1[:acceptable_gap_length]\n",
    "        #     # print(\"two first positions: \",two_first_positions)\n",
    "        #     while seq1:\n",
    "        #         print(\"first while\")\n",
    "        #         if seq1[:acceptable_gap_length] == gaps_signs:\n",
    "        #             print(\"there were gaps\")\n",
    "        #             seq1 = seq1[1:]\n",
    "        #             seq2 = seq2[1:]\n",
    "        #             print(seq1[:50])\n",
    "        #             # two_first_positions = seq1[:acceptable_gap_length]\n",
    "\n",
    "        #         else:\n",
    "        #             potencial_exon = \"\"\n",
    "        #             while seq1 and seq1[0] != \"-\":\n",
    "        #                 print(\"second while\")\n",
    "        #                 potencial_exon += seq1[0]\n",
    "        #                 #\n",
    "        #                 seq1 = seq1[1:]\n",
    "        #                 seq2 = seq2[1:] #tu by sie przydala stara sekwencja\n",
    "        #                 print(potencial_exon)\n",
    "                        \n",
    "        #             alignment_score, length_of_exon = alignment_for_exon_fun(potencial_exon, seq1, seq2, min_length_aligned_sequence, extreme_homology, alignment_id)\n",
    "        #             #W tym momencie pojawia sie blad. Importuje do funkcji wyzej zmienna: dlugosc potencial_exon. Na jej podstawie\n",
    "        #             #wykonuje alignment od poczatku sekwencji do n-tego nukleotydu. \n",
    "        #             #niestety ta sekwencja jest ucieta i nie zawiera juz tego egzonu.\n",
    "        #             #proponuje rozwiazanie o usuwani eksonu dopiero po tym jak zrobimy dla niego alignment. Do tego przyda sie stara\n",
    "        #             #funkcja - do obliczania dystansu miedzy przerwa. \n",
    "        #             #albo zrobimy inaczej. \n",
    "        #             print(potencial_exon)\n",
    "    \n",
    "\n",
    "        #             if length_of_exon < min_length_aligned_sequence and alignment_score < extreme_homology:\n",
    "        #                 all_exons.append(potencial_exon)\n",
    "        #                 print(\"one list was appended\")\n",
    "                        \n",
    "        #             else:\n",
    "        #                 print(\"both lists were appended\")\n",
    "        #                 all_exons.append(potencial_exon)\n",
    "        #                 good_exons.append(potencial_exon)\n",
    "        #             two_first_positions = seq1[:acceptable_gap_length]\n",
    "        #         print(potencial_exon)\n",
    "\n",
    "        #     print(\"Koniec detecting_first_exon \\n\")\n",
    "        #     return all_exons, good_exons\n",
    "\n",
    "             \n",
    "            \n",
    "        # list1, list2 = detecting_first_exon_fun(all_exons, good_exons, introns, seq1, seq2, gaps_signs, min_length_aligned_sequence, extreme_homology, acceptable_gap_length, alignment_id)\n",
    "        # print(list1, list2)\n",
    "\n",
    "#########################################################################################################################\n",
    "        # for nts in range(len(seq2[:100])):\n",
    "        #     first_nucleotides_pair = seq1[0]\n",
    "        #     if \"-\" in first_nucleotides_pair:\n",
    "        #         seq1 = seq1[1:]\n",
    "        #         seq2 = seq2[1:]\n",
    "        #     else:\n",
    "        #         potencial_exons_temporary += str(first_nucleotides_pair)\n",
    "        #         print(potencial_exons_temporary)\n",
    "        #         #tutaj zrob ten alignment z sekwencja druga\n",
    "        #         #local_alignment = pairwise2.align.localxx(seq1[:distance_between_first_nt_and_gap], seq2[:distance_between_first_nt_and_gap], one_alignment_only = True)\n",
    "        #         seq1 = seq1[1:]\n",
    "        #         seq2 = seq2[1:]\n",
    "\n",
    "        \n",
    "        # print(potencial_exons_at_edges_temporary)\n",
    "        # print(len(potencial_exons_at_edges_temporary))\n",
    "                #if \"-\" in \n",
    "            #teraz scal wszystkie pozycje tej listy w jeden ciag stringow ALE W OBIEKCIE SEQ\n",
    "            #potem warunek jesli bedzie wiecej niz 30 nt jeden po drugim w gornym strandzie to badaj alignment\n",
    "            #jesli alignment ma odpowiednia homologie to wysylaj tego egzona do nowej listy.\n",
    "            #przerwij funkcje\n",
    "            #napisz to samo od tylu            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "                \n",
    "        # print(craps__gaps_and_nucleotides_at_edges_temporary)\n",
    "        # print(potencial_exons_at_edges_temporary)\n",
    "        # print(f\"dlugosc listy craps__gaps_and_nucleotides_at_edges_temporary by sprawdzic czy suma rowna sie dlugosci alignmentu: {len(craps__gaps_and_nucleotides_at_edges_temporary)}\")\n",
    "        # print(f\"dlugosc listy potencial_exons_at_edges_temporary by sprawdzic czy suma rowna sie dlugosci alignmentu: {len(potencial_exons_at_edges_temporary)}\")\n",
    "\n",
    "'''\n",
    "        tymczasowy_krotki_alignment_roboczy = alignment[:, :50]\n",
    "        print(f\"Sekwencja referencyjna ma dlugosc: {len(tymczasowy_krotki_alignment_roboczy[1])}\")\n",
    "        craps__gaps_and_potencial_exons_temporary = []\n",
    "        potencial_exons_at_edges_temporary = []\n",
    "        for nts in range(len(tymczasowy_krotki_alignment_roboczy[1])):\n",
    "            first_nucleotides_pair = tymczasowy_krotki_alignment_roboczy[:, 0]\n",
    "            craps__gaps_and_potencial_exons_temporary.append(first_nucleotides_pair)\n",
    "            restored_alignment = \"\".join(craps__gaps_and_potencial_exons_temporary)\n",
    "            print(restored_alignment)\n",
    "'''\n",
    "\n",
    "        \n",
    "#############################################################################################################################\n",
    "#######################################                       STARE                   #######################################       \n",
    "#############################################################################################################################\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "#############################################################################################################################\n",
    "#######################################                       NOWE                    #######################################       \n",
    "#############################################################################################################################\n",
    "\n",
    "'''\n",
    "#PLAN JEST TAKI:\n",
    "\n",
    "ROBIMY TABELKE Z D\n",
    "niech program iteruje po alignmencie\n",
    "USUWAMY WSZELKIE GAPY NA POCZATKU\n",
    "jesli znajdize \"-\" to niech zapisuje do slownika intronow na wzor: organism_id : sequence, pozycja_w_odniesieniu_do_sekwencji_wyjsciowej (po tym poskladamy nasze introny i eksony w jedna tabelke)\n",
    "jesli znajdzie dwa sparowane nukleotydy to dodaje je do tymczasowej listy\n",
    "dodawanie zakonczy sie gdy pojawia sie dwa gapy \n",
    "\n",
    "1. ucinamy gapy na początku i końcu, tak by alignment zaczynał i kończył się od eksonu\n",
    "    i tak musimy stworzyc tutaj listy tymczasowe gdzie bedziemy dorzucali pary a nastepnie sprawdzali czy sa one eksonem czy jeszcze nie\n",
    "2. iterujemy po parze nukleotydów i dodajemy je do listy tymczasowej list_temp\n",
    "    - jeśli na dwóch ostatnich pozycjach są gapy (nasz warunek z zewnątrz) oraz na poprzednich pozycjach są nukleotydy to usuwamy te dwa ostatnie gapy, tak by pozostał czysty ekson. Dalej sprawdzamy mu długość a następnie robimy alignment. Potem w zależności od jakości wysyłamy go do listy z dobrą jakością lub złą jakością (o tym rozmawialiśmy na ostatnim spotkaniu)\n",
    "    - jeśli na dwóch ostatnich pozycjach są gapy, ale brakuje nukleotydów to dodajemy ten dwuelementowy \"mini - intron\" do listy tymczasowej list_temp_intron tak długo aż pojawi się nukleotyd. Ten intron wysyłamy do głownej listy z intronami.\n",
    "3. Po zakończeniu, składamy dobre eksony z intronami. Złożę je chronologicznie, bo do poprzednich list dodam ich index, by wiedzieć który to z kolei nukleotyd\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################################################################\n",
    "#######################################                     FUNKCJE                   #######################################       \n",
    "#############################################################################################################################\n",
    "\n",
    "def indices_of_introns_and_exons_fun(seq1_DT, seq2_DT, seq1, seq2, count_of_errors):\n",
    "    #linking pre-exon's nucleotides (nt-nt pairs) and pre-intron's nucleotides (gap-nt pairs) to theirs indices \n",
    "    temp_exon = [] #list containing indices of intron's positions in sequence.\n",
    "    temp_intron = [] #list containing indices of intron's positions in sequence.\n",
    "    index = -1\n",
    "    paired_nucleotides = zip(seq1, seq2)\n",
    "    \n",
    "    if len(seq1_DT) != len(seq2_DT):\n",
    "        raise ValueError(\"The two sequences must be of the same length.\")\n",
    "        count_of_errors += 1\n",
    "        #return count_of_errors\n",
    "    for nt1, nt2 in paired_nucleotides:\n",
    "        index += 1 #indeks do wskazywania pozycji w sekwencji \n",
    "        #print(index)\n",
    "        if \"-\" in (nt1, nt2):\n",
    "            temp_intron.append(index)\n",
    "        else:\n",
    "            temp_exon.append(index)\n",
    "    return temp_intron, temp_exon, count_of_errors\n",
    "\n",
    "\n",
    "def start_and_end_parameters_of_exons_dict_fun(temp_exon, gaps_signs):\n",
    "    #linking indices into single strand of possible exons\n",
    "    start_exon = temp_exon[0]\n",
    "    exon_range_dict = {} #that dictionary contains: key(index of start exon) and value(index of end exon)\n",
    "    for i in range(len(temp_exon)):\n",
    "        #print(start_exon)\n",
    "        if (temp_exon[i] - temp_exon[i-1]) > len(gaps_signs): #if difference between two indices of exons's positions is bigger than given number (2), that smaller number is index of 3' nucleotide in exon \n",
    "            end_exon = temp_exon[i-1] #temp_exon[i-1] because it is index in list temp_exon. +1 because python starts counting from 0\n",
    "            exon_range_dict[start_exon] = end_exon #creating dictionary with \n",
    "            start_exon = temp_exon[i]\n",
    "    print(f\" \\n exon_range_dict: {exon_range_dict}\")\n",
    "    return (exon_range_dict)\n",
    "     \n",
    "def start_and_end_parameters_of_introns_dict_fun(exon_range_dict):\n",
    "    #linking indices into single strand of possible introns\n",
    "    intron_range_dict = {}\n",
    "    keys_from_exon_range_dict = sorted(exon_range_dict.keys()) #list that contains exons's start positions\n",
    "    print(keys_from_exon_range_dict)\n",
    "\n",
    "    for i in range(len(keys_from_exon_range_dict) - 1):\n",
    "        start_intron = exon_range_dict[keys_from_exon_range_dict[i]] +1 \n",
    "        end_intron = keys_from_exon_range_dict[i + 1] - 1\n",
    "        intron_range_dict[start_intron] = end_intron\n",
    "    print(intron_range_dict)\n",
    "    return (intron_range_dict)\n",
    "\n",
    "\n",
    "#nowe\n",
    "#################################\n",
    "#stare\n",
    "\n",
    "def percentage_of_advancement(directory):\n",
    "    count = 0\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".fasta\"):\n",
    "            count += 1\n",
    "    return(count)\n",
    "\n",
    "\n",
    "def extracting_strands_from_alignment(alignment): #that function describe what strand is transcript strand and genomic strand - in our files, transcript strand has longer ID.\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: extracting_strands_from_alignment, przeszlo\")\n",
    "    if len(alignment[0].id) >= len(alignment[1].id):\n",
    "        seq1 = alignment[0].seq\n",
    "        seq2 = alignment[1].seq\n",
    "        #seq1_is_transcript = True #obecnie nie uzywane\n",
    "    else:\n",
    "        seq1 = alignment[1].seq\n",
    "        seq2 = alignment[0].seq    \n",
    "        #seq1_is_transcript = False #obecnie nie uzywane\n",
    "    #print(f\"\\n Just run function: extracting_strands_from_alignment\")\n",
    "    return seq1, seq2\n",
    "    #return seq1_is_transcript #obecnie nie uzywane\n",
    "    \n",
    "\n",
    "def finding_gap_index(sequence, acceptable_gap_length, alignment): #finding index of first gap in each sequence\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: finding_gap_index, przeszlo\")\n",
    "    gap_index = sequence.find(\"-\" * acceptable_gap_length) #-1 is output when anything was found\n",
    "    #print(type(gap_index))\n",
    "    return gap_index\n",
    "\n",
    "\n",
    "def reversed_finding_gap_index(sequence, acceptable_gap_length, alignment): #finding index of first gap in each sequence for counting the end of intron\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: reversed_finding_gap_index, przeszlo\")\n",
    "    return sequence.rfind(\"-\" * acceptable_gap_length) + acceptable_gap_length #now we have to add length of gap to index, because python starts counting from 0 nt, and we just change direction of counting.\n",
    "\n",
    "def setting_distance_between_first_nt_and_gap(is_it_exon, first_gap_position_seq1, first_gap_position_seq2, last_gap_position_seq1, last_gap_position_seq2, seq1, alignment): #it means lenght of exon\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: setting_distance_between_first_nt_and_gap, przeszlo\")\n",
    "    if is_it_exon == True:\n",
    "        if first_gap_position_seq2 < 0: #when in second strand there are no gaps - it means, second strand is correctly prepared. <0 because find() function gives -1 result when not find\n",
    "            distance_between_first_nt_and_gap = first_gap_position_seq1 #more important is seq2 because it is genomic sequence, ~reference sequence\n",
    "        elif first_gap_position_seq1 < 0:\n",
    "            distance_between_first_nt_and_gap = first_gap_position_seq2\n",
    "        else:\n",
    "            distance_between_first_nt_and_gap = min(first_gap_position_seq1, first_gap_position_seq2)\n",
    "        return distance_between_first_nt_and_gap\n",
    "\n",
    "    elif is_it_exon == False:\n",
    "        if last_gap_position_seq2 < 0: #it means, if there is no gaps in second sequention, gap position is gap position in seq1. Additional, rfind() function return \"-1\" if it won't find any gaps in seq\n",
    "            distance_between_first_nt_and_gap = (len(seq1) - last_gap_position_seq1) -1\n",
    "            #print(\"pierwszy\", distance_between_first_nt_and_gap) #checkpoint\n",
    "        else:\n",
    "            distance_between_first_nt_and_gap = (len(seq1) - max(last_gap_position_seq1, last_gap_position_seq2))\n",
    "        return distance_between_first_nt_and_gap\n",
    "            #print(\"drugi\", distance_between_first_nt_and_gap) #checkpoint\n",
    "\n",
    "        \n",
    "def describing_class_of_exon(percent_of_homology, last_nt_exon_index, min_length_aligned_sequence, class_of_exon): #class of exon depends of their homology\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: describing_class_of_exon, przeszlo\")\n",
    "    if percent_of_homology >= 0.9 and last_nt_exon_index >= min_length_aligned_sequence:\n",
    "        class_of_exon.append(\"1\")\n",
    "    elif percent_of_homology < 0.9 and percent_of_homology > 0 and last_nt_exon_index < min_length_aligned_sequence:\n",
    "        class_of_exon.append(\"2\")\n",
    "    else:\n",
    "        class_of_exon.append(\"3\")\n",
    "\n",
    "\n",
    "def exons_indices(is_it_exon, max_genomic_seq, seq2, last_nt_exon_index, first_nt_position, last_nt_position):\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: exons_indices, przeszlo\")\n",
    "    if is_it_exon:\n",
    "        first_nt_index_of_exon = max_genomic_seq.find(str(seq2[:last_nt_exon_index]).replace(\"-\", \"\"))\n",
    "        if first_nt_index_of_exon == -1:\n",
    "            first_nt_position.append(0)\n",
    "            last_nt_position.append(0)\n",
    "            #print(f\"That exon: {seq2[:last_nt_exon_index].replace('-', '')} comes from genomic sequence and cannot be found in file_before_MAFFT.\")\n",
    "        else:\n",
    "            first_nt_position.append(first_nt_index_of_exon + 1)\n",
    "            last_nt_position.append(first_nt_index_of_exon + len(seq2[:last_nt_exon_index]))\n",
    "            \n",
    "\n",
    "def introns_indices(is_it_exon, max_genomic_seq, seq2, last_nt_intron_index, first_nt_position, last_nt_position):\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: introns_indices, przeszlo\")\n",
    "    if is_it_exon: \n",
    "        first_nt_index_of_intron = max_genomic_seq.find(str(seq2[:last_nt_intron_index]))\n",
    "        if first_nt_index_of_intron == -1:\n",
    "            first_nt_position.append(0)\n",
    "            last_nt_position.append(0)\n",
    "        else:\n",
    "            first_nt_position.append(first_nt_index_of_intron + 1)\n",
    "            last_nt_position.append(first_nt_index_of_intron + len(seq2[:last_nt_intron_index]))\n",
    "\n",
    "\n",
    "def extending_source_data_frame(alignment, source):\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: extending_source_data_frame, przeszlo\")\n",
    "    keywords = [\"TRINITY\", \"BACKBONE\", \"SCAFFOLD\"]\n",
    "    found = False\n",
    "    for key in keywords:\n",
    "        if str(alignment[0]).lower().find(key.lower()) != -1:\n",
    "            source.append(key)\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        source.append(None) #none means unknown\n",
    "\n",
    "\n",
    "def extending_strand_data_frame(alignment, strand): \n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: extending_strand_data_frame, przeszlo\")\n",
    "    if str(alignment[0]).find(\"SL+\") != -1:\n",
    "        strand.append(\"+\")\n",
    "    elif str(alignment[0]).find(\"SL-\") != -1:\n",
    "        strand.append(\"-\")\n",
    "    else:\n",
    "        strand.append(None)\n",
    "\n",
    "\n",
    "def extending_start_and_end_data_frame_exons(seq2, seq2_DT, start, end, distance_between_first_nt_and_gap, last_nt_exon_index): #index of exons's start\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: extending_start_and_end_data_frame, przeszlo\")\n",
    "    #print(f\"Alignment:, funkcja: extending_start_and_end_data_frame, przeszlo\")\n",
    "    start.append(seq2_DT.find(seq2[:distance_between_first_nt_and_gap])+1) #+1 because python starts counting from 0, not 1\n",
    "    end.append(seq2_DT.find(seq2[:distance_between_first_nt_and_gap]) + last_nt_exon_index)\n",
    "\n",
    "def extending_start_and_end_data_frame_introns(seq2, seq2_DT, start, end, distance_between_first_nt_and_gap, last_nt_intron_index): #index of intron's start\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: extending_start_and_end_data_frame, przeszlo\")\n",
    "    #print(f\"Alignment:, funkcja: extending_start_and_end_data_frame, przeszlo\")\n",
    "    start.append(seq2_DT.find(seq2[:distance_between_first_nt_and_gap])+1)\n",
    "    end.append(seq2_DT.find(seq2[:distance_between_first_nt_and_gap]) + last_nt_intron_index)\n",
    "\n",
    "def save_to_file(final_data_frame, filename):\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: save_to_file, przeszlo\")\n",
    "    if os.path.isfile(filename):\n",
    "        user_input = input(\"TSV file already exists. Do you want overwrite? y/n: \")\n",
    "        if user_input == \"n\":\n",
    "            base_name, ext = os.path.splitext(filename)\n",
    "            i = 1\n",
    "            while os.path.isfile(f\"{base_name}_{i}{ext}\"):\n",
    "                i += 1\n",
    "            filename = f\"{base_name}_{i}{ext}\"\n",
    "            final_data_frame.to_csv(filename, sep = \"\\t\")\n",
    "            \n",
    "        elif user_input == \"y\":\n",
    "            final_data_frame.to_csv(filename, sep = \"\\t\")\n",
    "        else:\n",
    "            print(\"Clarify your answer. Nothing has done.\")\n",
    "            \n",
    "    else:\n",
    "        final_data_frame.to_csv(filename, sep = \"\\t\")\n",
    "\n",
    "def save_to_gff_file(gff_final_data_frame, filename):\n",
    "    #print(f\"Alignment: {alignment[0].id}, funkcja: save_to_gff_file, przeszlo\")\n",
    "    if os.path.isfile(filename):\n",
    "        user_input = input(\"GFF file already exists. Do you want overwrite? y/n: \")\n",
    "        if user_input == \"n\":\n",
    "            base_name, ext = os.path.splitext(filename)\n",
    "            i = 1\n",
    "            while os.path.isfile(f\"{base_name}_{i}{ext}\"):\n",
    "                i += 1\n",
    "            filename = f\"{base_name}_{i}{ext}\"\n",
    "            gff_final_data_frame.to_csv(filename, sep = \"\\t\")\n",
    "            \n",
    "        elif user_input == \"y\":\n",
    "            gff_final_data_frame.to_csv(filename, sep = \"\\t\")\n",
    "        else:\n",
    "            print(\"Clarify your answer. Nothing has done.\")\n",
    "            \n",
    "    else:\n",
    "        gff_final_data_frame.to_csv(filename, sep = \"\\t\")\n",
    "\n",
    "def check_alignment_eligibility(alignment, alignment_DIDNT_TOUCHED, acceptable_gap_length, files_in_progress, count_files, count_of_errors, filename, extreme_homology, min_length_aligned_sequence):\n",
    "    while True:\n",
    "        is_it_exon = True\n",
    "        first_nucleotides_pair = alignment[:, 0]#first nucleotides in both strands as variable\n",
    "        if \"-\" in first_nucleotides_pair: #deleting mismatches at the start\n",
    "            if len(alignment[0]) < 100: #zakoczenie przycinania - nie wiem czy to jest to o co mi chodzilo\n",
    "                break\n",
    "            else:\n",
    "                alignment = alignment[:, 1:]\n",
    "                #print(f\"alignment after cut:\\n{alignment}\") #chekpoint\n",
    "                if len(alignment[0]) % 10 == 0:\n",
    "                    # print(f\"{round((100-(len(alignment[0])/len(alignment_DIDNT_TOUCHED[0]))*100), 2)}% base pair done of {alignment[0].id}. {files_in_progress} of {count_files}, what means {round(files_in_progress/count_files*100, 2)}% of advancement. The time is: {time.strftime('%H:%M:%S', time.localtime())}\")\n",
    "                    # print(f\"First 50 nt of reference sequence is: {(alignment[1].seq)[:50]}. \\n\")\n",
    "                    function_progress(alignment, alignment_DIDNT_TOUCHED, files_in_progress, count_files)\n",
    "            continue\n",
    "    \n",
    "                \n",
    "        seq1, seq2 = extracting_strands_from_alignment(alignment)\n",
    "        seq1_DT, seq2_DT = extracting_strands_from_alignment(alignment_DIDNT_TOUCHED) #sequences without cutted gaps from the ends. They will be used to count index where intron starts\n",
    "        \n",
    "    \n",
    "        first_gap_position_seq1 = finding_gap_index(seq1, acceptable_gap_length, alignment)\n",
    "        first_gap_position_seq2 = finding_gap_index(seq2, acceptable_gap_length, alignment)\n",
    "        last_gap_position_seq1 = reversed_finding_gap_index(seq1, acceptable_gap_length, alignment)\n",
    "        last_gap_position_seq2 = reversed_finding_gap_index(seq2, acceptable_gap_length, alignment)\n",
    "        #print(f\"first gap position in first sequence:{first_gap_position_seq1}, in 2nd sequence: {first_gap_position_seq2}\") #checkpoint\n",
    "    \n",
    "        distance_between_first_nt_and_gap = setting_distance_between_first_nt_and_gap(is_it_exon, first_gap_position_seq1, first_gap_position_seq2, last_gap_position_seq1, last_gap_position_seq2, seq1, alignment)\n",
    "        \n",
    "        local_alignment = pairwise2.align.localxx(seq1[:distance_between_first_nt_and_gap], seq2[:distance_between_first_nt_and_gap], one_alignment_only = True) #\"xx\"  means no gap penalty while opening gaps or longering them and no penalties for mismatch. Just pure score of alignment to count homology\n",
    "        print(\"local_alignment:\", local_alignment) #checkpoint\n",
    "    \n",
    "        try:  #instruction what to do when seq is too short\n",
    "            local_homology_percentage = (local_alignment[0].score / distance_between_first_nt_and_gap) \n",
    "        except IndexError:\n",
    "            if len(seq1) < 100:\n",
    "                count_of_errors += 1\n",
    "                print(f\"That file {filename} made {count_of_errors}th error\")\n",
    "                #print(f\"\\n \\n {alignment[0].id} sequence is too short, sequence has not exons or other fault. \\n\")\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        \n",
    "        local_homology_percentage = (local_alignment[0].score / distance_between_first_nt_and_gap)\n",
    "        print(local_homology_percentage)\n",
    "\n",
    "    \n",
    "        if local_homology_percentage <= extreme_homology:\n",
    "            print(f\" Too low score of homology. \\n alignment: {alignment[:, :50]} \\n score: {local_alignment[0].score}, distance: {distance_between_first_nt_and_gap}, percent: {round(local_homology_percentage, 2)*100}%, \\n {local_alignment} \\n \\n \\n \\n \\n\")\n",
    "            alignment = alignment[:, distance_between_first_nt_and_gap:]\n",
    "            function_progress(alignment, alignment_DIDNT_TOUCHED, files_in_progress, count_files)            \n",
    "        else:\n",
    "            if distance_between_first_nt_and_gap <= min_length_aligned_sequence:\n",
    "                #print(f\"\\n alignment is too short. \\n distance between sequences {distance_between_first_nt_and_gap}, \\n alignment: {alignment}, \\n score: {local_alignment[0].score}, distance: {distance_between_first_nt_and_gap}, procentowo: {round(local_homology_percentage, 2)*100}%, \\n {local_alignment}\")\n",
    "                alignment = alignment[:, distance_between_first_nt_and_gap:]\n",
    "                function_progress(alignment, alignment_DIDNT_TOUCHED, files_in_progress, count_files)\n",
    "            else:\n",
    "                #print(f\"Sequence {alignment[0].id} cut properly from LEFT side, \\n distance between sequences: {distance_between_first_nt_and_gap}\"), alignment: {alignment[:, :50]},\n",
    "                break\n",
    "                \n",
    "        if len(seq1) < 100 or len(seq2) < 100:\n",
    "            break\n",
    "\n",
    "        return seq1, seq2, seq1_DT, seq2_DT, first_gap_position_seq1, first_gap_position_seq2, last_gap_position_seq1, last_gap_position_seq2, distance_between_first_nt_and_gap, local_alignment, local_homology_percentage\n",
    "\n",
    "    ######################### TERAZ OD PRAWEJ DO LEWEJ #######################\n",
    "    \n",
    "    while True:\n",
    "            \n",
    "        is_it_exon = False\n",
    "        last_nucleotides_pair = alignment[:, -1]#last nucleotides in both strands as variable\n",
    "\n",
    "    \n",
    "        if \"-\" in last_nucleotides_pair: #deleting mismatches at the end\n",
    "            if len(alignment[0]) < 100 or len(alignment[1]) < 100:\n",
    "                #print(\"alignment has no matching nucleotides\")\n",
    "                break \n",
    "            else:\n",
    "                alignment = alignment[:, :-1]\n",
    "                function_progress(alignment, alignment_DIDNT_TOUCHED, files_in_progress, count_files)\n",
    "                #print(f\"alignment check:\\n{alignment[:, -50:-1]}\") checkpoint\n",
    "            continue\n",
    "        else:\n",
    "            if len(alignment[0]) < 100 or len(alignment[1]) < 0:\n",
    "                #print(\"counting from right side: alignment is too short\")\n",
    "                break\n",
    "        \n",
    "    \n",
    "        seq1, seq2 = extracting_strands_from_alignment(alignment)\n",
    "        seq1_DT, seq2_DT = extracting_strands_from_alignment(alignment_DIDNT_TOUCHED) #sequences without cutted gaps from the ends. They will be used to count index where intron starts\n",
    "        \n",
    "\n",
    "        first_gap_position_seq1 = finding_gap_index(seq1, acceptable_gap_length, alignment)\n",
    "        first_gap_position_seq2 = finding_gap_index(seq2, acceptable_gap_length, alignment)\n",
    "        last_gap_position_seq1 = reversed_finding_gap_index(seq1, acceptable_gap_length, alignment)\n",
    "        last_gap_position_seq2 = reversed_finding_gap_index(seq2, acceptable_gap_length, alignment)\n",
    "        #print(\"seq1:\", last_gap_position_seq1, \"seq2:\", last_gap_position_seq2) #checkpoint\n",
    "        \n",
    "        distance_between_first_nt_and_gap = setting_distance_between_first_nt_and_gap(is_it_exon, first_gap_position_seq1, first_gap_position_seq2, last_gap_position_seq1, last_gap_position_seq2, seq1, alignment)\n",
    "        #print(\"drugi\", distance_between_first_nt_and_gap)\n",
    "                \n",
    "        local_alignment = pairwise2.align.localxx(seq1[-distance_between_first_nt_and_gap:], seq2[-distance_between_first_nt_and_gap:], one_alignment_only = True) #\"xx\" means no gap penalty while opening gaps or longering them and no penalties for mismatch. Just pure score of alignment\n",
    "        #print(\"local_alignment:\", local_alignment) #checkpoint\n",
    "                \n",
    "    \n",
    "        try:#instruction what to do when seq is too short or gaps were not found\n",
    "            if distance_between_first_nt_and_gap != 0:\n",
    "                local_homology_percentage = (local_alignment[0].score / distance_between_first_nt_and_gap) \n",
    "            else:\n",
    "                break\n",
    "        except IndexError:\n",
    "            if len(seq1) < 100:\n",
    "                count_of_errors += 1\n",
    "                print(f\"That file {filename} made {count_of_errors}th error\")\n",
    "                #print(f\"\\n \\n {alignment[0].id} sequence is too short or sequence has not exons. \\n\")\n",
    "                break\n",
    "            continue\n",
    "    \n",
    "        \n",
    "        if local_homology_percentage <= extreme_homology:\n",
    "            #print(f\"\\n Too low homology! \\n alignment: {alignment[:, -50:]} \\n score: {local_alignment[0].score}, \\distance: {distance_between_first_nt_and_gap}, %%%: {round(local_homology_percentage, 2)*100}%, \\n {local_alignment}\")\n",
    "            alignment = alignment[:, :-distance_between_first_nt_and_gap] \n",
    "            function_progress(alignment, alignment_DIDNT_TOUCHED, files_in_progress, count_files)\n",
    "            \n",
    "        else:\n",
    "            if distance_between_first_nt_and_gap <= min_length_aligned_sequence:\n",
    "                #print(f\"\\n Alignment is too short!. \\n distance: {distance_between_first_nt_and_gap}, \\n alignment: {alignment}, \\n score: {local_alignment[0].score}, distance: {distance_between_first_nt_and_gap}, %%%: {round(local_homology_percentage, 2)*100}%, \\n {local_alignment}\")\n",
    "                alignment = alignment[:, :-distance_between_first_nt_and_gap]\n",
    "                function_progress(alignment, alignment_DIDNT_TOUCHED, files_in_progress, count_files)\n",
    "            else:\n",
    "                #print(f\"Sequence: {alignment[0].id} cut properly from RIGHT side, alignment: \\n {alignment[:, -50:]}\")\n",
    "                break\n",
    "                \n",
    "        return seq1, seq2, seq1_DT, seq2_DT, first_gap_position_seq1, first_gap_position_seq2, last_gap_position_seq1, last_gap_position_seq2, distance_between_first_nt_and_gap, local_alignment, local_homology_percentage\n",
    "\n",
    "        if len(seq1) < 100 or len(seq2) < 100:\n",
    "            break\n",
    "\n",
    "def function_progress(alignment, alignment_DIDNT_TOUCHED, files_in_progress, count_files):\n",
    "    #do tego tematu wrocimy jak zrobimy tabele z odrzuconymi wartosciami\n",
    "    first_50_nt_alignment = str((alignment[1].seq)[:20])\n",
    "    #print(f\" {round((100-(len(alignment[0])\\/len(alignment_DIDNT_TOUCHED[0]))*100), 2)}% % base pair done of {alignment[0].id}. {files_in_progress} of {count_files}, what means {round(files_in_progress/count_files*100, 2)}% of advancement. The time is: {time.strftime('%H:%M:%S', time.localtime())}\")\n",
    "\n",
    "def cleaning_gaps_from_both_edges(sequence):\n",
    "    return sequence.lstrip(\"-\").rstrip(\"-\")\n",
    "if __name__ == '__main__':\n",
    "    cutting_scrap(path_to_file_before_MAFFT, path_to_file_after_MAFFT, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "057597e7-562d-44ec-ba83-b3280856af6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstan rzeczy: cos sie dubluje, niektore fragmenty tekstu. Mozliwe ze to klopot z iteracja po iteracji. Napisz najpierw slownik\\n\\ncel 0: zmien frament na poczatku zeby nie iterowal po iterowanym tekscie. prostsza metoda, nie slownik \\nCHYBA GIT\\n\\ncel: usprawnij to tak by outputem byl 148 wierszowa tabela\\n    Dwa klopoty:\\n        1. wszystkie dane sa od tego samego organizmu --> klopot z nazwa w seq id --> trzeba zmienic sposob laczenia list do slownika UPDATE. to nawet nie jest przyczyna trudności, tu chodzi o kwestie iteracji\\n        2. niektore dane maja za niska homologie - mozliwe ze sa powiazania z powyzszym punktem\\n        3. a inne nie respectuja minimalnej dlugosci eksonu\\n\\ncek na potem: usunac introny z ostatniej pozycji w tabeli - powiazanie z powyzszymi punktami\\n\\ncel na potem: zrob tak by wyswietlal sie postep na biezaco, nie tylko gdy pierwszym znakiem bedzie gap\\n    Ta funkcja jest zle skonstruowana. Musimy przyjac za punkt odniesienia stopien zaawansowania w odniesieniu do funkcji Didnt Touched\\n    Wroce do tego potem jak bede pisał tabele z odrzuconymi danymi \\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "stan rzeczy: cos sie dubluje, niektore fragmenty tekstu. Mozliwe ze to klopot z iteracja po iteracji. Napisz najpierw slownik\n",
    "\n",
    "cel 0: zmien frament na poczatku zeby nie iterowal po iterowanym tekscie. prostsza metoda, nie slownik \n",
    "CHYBA GIT\n",
    "\n",
    "cel: usprawnij to tak by outputem byl 148 wierszowa tabela\n",
    "    Dwa klopoty:\n",
    "        1. wszystkie dane sa od tego samego organizmu --> klopot z nazwa w seq id --> trzeba zmienic sposob laczenia list do slownika UPDATE. to nawet nie jest przyczyna trudności, tu chodzi o kwestie iteracji\n",
    "        2. niektore dane maja za niska homologie - mozliwe ze sa powiazania z powyzszym punktem\n",
    "        3. a inne nie respectuja minimalnej dlugosci eksonu\n",
    "\n",
    "cek na potem: usunac introny z ostatniej pozycji w tabeli - powiazanie z powyzszymi punktami\n",
    "\n",
    "cel na potem: zrob tak by wyswietlal sie postep na biezaco, nie tylko gdy pierwszym znakiem bedzie gap\n",
    "    Ta funkcja jest zle skonstruowana. Musimy przyjac za punkt odniesienia stopien zaawansowania w odniesieniu do funkcji Didnt Touched\n",
    "    Wroce do tego potem jak bede pisał tabele z odrzuconymi danymi \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727fcc7-0e27-4c02-a448-f402ca2b6fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
